{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VoxCommunis data processing pipeline\n",
    "\n",
    "This is a script of running MFA on recordings from Common Voice corpus. \n",
    "\n",
    "To run this pipeline, you need to download:\n",
    "\n",
    "1. Python modules: epitran, praatio, re, pandas, numpy, subprocess, shutil, os\n",
    "2. The data of XPF corpus\n",
    "\n",
    "The pipeline takes these steps to process data:\n",
    "\n",
    "1. [Step 0: Setups](#step-0-setups)\n",
    "2. [Step 1: Remap speakers](#step-1-remap-the-validated-speakers)\n",
    "3. [Step 2: Create TextGrid and .wav files](#step-2-create-textgrid-files-and-wav-files-based-on-the-mp3-recordings-from-common-voice)\n",
    "4. [Step 3: Prepare the lexicon](#step-3-prepare-the-lexicon)\n",
    "5. [Step 4: G2P grapheme-to-phoneme](#step-4-g2p-grapheme-to-phoneme-epitran-or-xpf)\n",
    "6. [Step 5: Validation](#step-5-train-the-acoustic-model)\n",
    "7. [Step 6: Run MFA](#step-6-train-the-acoustic-model-and-forced-align)\n",
    "8. [Finale](#finale)\n",
    "\n",
    "This script was created by Miao Zhang (miao.zhang@uzh.ch), 22.12.2023\n",
    "\n",
    "This script was modified by Miao Zhang, 07.02.2024 (Revalidation added)\n",
    "\n",
    "Modified on 16.02.2024: added automatic log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Setups\n",
    "Import packages and setup file directories (for both the scripts and data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os, subprocess, shutil, re, csv, sys, importlib, multiprocessing, zipfile\n",
    "import pandas as pd\n",
    "# Turn Copy-On-Write on\n",
    "pd.options.mode.copy_on_write = True\n",
    "import numpy as np\n",
    "\n",
    "# For creating textgrids\n",
    "from praatio import textgrid\n",
    "\n",
    "# For move files concurrently\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Import Path\n",
    "from pathlib import Path\n",
    "\n",
    "# Import Lock to zip output textgrids\n",
    "from threading import Lock\n",
    "\n",
    "# Import functions from cv_processing.py\n",
    "import vxc_processing as vxcproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload vxcproc in case if there are any changes to the code\n",
    "importlib.reload(vxcproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the paths and directories of data and scripts to use.\n",
    "\n",
    "_IMPORTANT_: the folder of the corpus data you downloaded from Common Voice should be named as: {lang_code}_v{version_number}.\n",
    "- For example: the folder for the 16th version of Divhehi should be named: dv_v16.\n",
    "- Another example: the folder for the 15th version of Upper Sorbian should be: hsb_v15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### Directories ################################################\n",
    "\n",
    "# This is the directory where your data downloaded from Common Voice should be saved. This is the root directory where data from each language should be saved in individual folders.\n",
    "# NO (BACK)SLASH at the end!!!\n",
    "commonVoice_dir = '/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice' \n",
    "# For Eleanor\n",
    "#commonVoice_dir = '/Users/eleanor/Documents/CommonVoice' \n",
    "\n",
    "# To use XPF as the G2P engine to process lexicon, you will need to download the XPF data from: https://github.com/CohenPr-XPF/XPF/tree/master/Data and save them on your computer.\n",
    "# Specify the directory where your XPF data is saved.\n",
    "# NO (BACK)SLASH at the end!!!\n",
    "xpf_dir = '/Users/miaozhang/Research/CorpusPhon/CorpusData/G2P/XPF' \n",
    "# For Eleanor\n",
    "#xpf_dir = '/Users/eleanorchodroff/Documents/CorpusData/G2P/XPF'\n",
    "\n",
    "######################### Language name/code and Common Voice version ##############################\n",
    "\n",
    "# Language-related variable names\n",
    "# the Common Voice code of the language (unfortunately, Common Voice mixes the use of iso 639-3 and iso 639-1 codes (they use bcp47 code). This code is also used in XPF).\n",
    "# The code should match the code used in the name of the folder you downloaded from Common Voice.\n",
    "lang_code = 'ca' \n",
    "\n",
    "# The version of the data in Common Voice\n",
    "# Only numbers!!!\n",
    "cv_mod_version = '16' # which version of common voice corpus that the model is trained on?\n",
    "\n",
    "if_mod_same = 0 # Is the model trained on the same version as the data to be aligned now?\n",
    "if if_mod_same == 0: # If not, then...\n",
    "    cv_align_version = '17' # which version of common voice corpus is forced-aligned?\n",
    "else:\n",
    "    cv_align_version = cv_mod_version # set the aligned data to the same version as the model\n",
    "\n",
    "######################### G2P ######################################################################\n",
    "\n",
    "# Specify the G2P engine. Only these keywords are acceptable: \n",
    "# 'xpf' for XPF\n",
    "# 'epi' for Epitran\n",
    "# 'chr' for Charsiu\n",
    "# 'mfa' for MFA\n",
    "# 'vxc' for self-difined lexicon\n",
    "g2p = 'vxc'\n",
    "\n",
    "######################### What writing system is the language using? ###############################\n",
    "\n",
    "# Specify if the language is Chinese/Japanese/Korean\n",
    "if_cjk = 0\n",
    "\n",
    "######################### Using existing model? ###############################\n",
    "\n",
    "if_self_mod = 1\n",
    "\n",
    "######################### Using existing lexicon? ###############################\n",
    "\n",
    "if_self_lex = 1\n",
    "\n",
    "###################### G2P settings (XPF or Epitran) ################################################\n",
    "\n",
    "# This is where VxcCommunis_tracking.csv is (NO (BACK)SLASH at the end!!!):\n",
    "cv_tracking_file = 'VoxCommunis_Info.csv'\n",
    "\n",
    "# Get the G2P processing code for the language\n",
    "if g2p == 'xpf' or 'chr':\n",
    "    with open(cv_tracking_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        lang_row = [row for row in reader if row['code_cv'] == lang_code][0]\n",
    "    # If you are using XPF, get the name of the language in XPF corpus\n",
    "    if g2p == 'xpf':\n",
    "        lang_name = lang_row['name_xpf'].replace(' ', '')\n",
    "        code_xpf = lang_row['code_xpf']\n",
    "    # If you are using Charsiu, get the processing code for the language in Charsiu.\n",
    "    elif g2p == 'chr':\n",
    "        code_chr = lang_row['code_chr']\n",
    "\n",
    "if g2p == 'epi':\n",
    "    # If you are using epitran, ...\n",
    "    # Please refer to VoxCommunics_info.csv to get the processing code of the language in epitran\n",
    "    # !!!Do this manually, since depending on the type of the orthography, the epitran code can differ!!!\n",
    "    epi_code = 'ben-Beng'\n",
    "\n",
    "\n",
    "# Specify if the subversion of a corpus is used. The default is 0\n",
    "if_subversion = 0 \n",
    "# If if_subversion == 1, what suffix you would use?:\n",
    "# Ignore this part, if you don't have a subversion of the corpus you are using.\n",
    "subversion = '_' + 'sub3'\n",
    "\n",
    "################################################################################################### \n",
    "\n",
    "# The folder for the language\n",
    "language_dir = os.path.join(commonVoice_dir, lang_code + '_v' + cv_align_version)\n",
    "\n",
    "# The file that contains the duration of each clip:\n",
    "clip_info_path = os.path.join(language_dir, 'clip_durations.tsv')\n",
    "\n",
    "# MFA paths\n",
    "# The folder of the OOV word files (NO (BACK)SLASH at the end!!!):\n",
    "mfa_oov_path = '/Users/miaozhang/Documents/MFA/validated'\n",
    "# This is where the acoustic model will be saved after MFA training is done (NO (BACK)SLASH at the end!!!):\n",
    "mfa_mod_folder = '/Users/miaozhang/Documents/MFA/pretrained_models/acoustic'\n",
    "\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "\n",
    "# This is where files that will be uploaded to the OSF repo will be saved after the processing is finished (NO (BACK)SLASH at the end!!!):\n",
    "osf_path = '/Users/miaozhang/Research/CorpusPhon/CorpusData/VoxCommunis_OSF'\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "\n",
    "# Get the naming schema.\n",
    "naming_schema = pd.read_csv('vxc_naming_schema.csv', usecols = ['Python_code'])['Python_code'].tolist()\n",
    "naming_schema = [eval(name) for name in naming_schema]\n",
    "\n",
    "# Get the names\n",
    "acs_mod_name = naming_schema[0]\n",
    "textgrid_folder_name = naming_schema[1]\n",
    "word_file_name = naming_schema[2]\n",
    "dict_file_name = naming_schema[3]\n",
    "spkr_file_name = naming_schema[4]\n",
    "\n",
    "# Get the paths\n",
    "textgrid_folder_path = os.path.join(language_dir, textgrid_folder_name)\n",
    "word_file_path = os.path.join(language_dir, word_file_name)\n",
    "dict_file_path = os.path.join(language_dir, dict_file_name)\n",
    "spkr_file_path = os.path.join(language_dir, spkr_file_name)\n",
    "del naming_schema\n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "# For step 3: prepare the lexicon and pronunciation dictionary\n",
    "validated_log = os.path.join(language_dir, 'validated.tsv')\n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "# For step 4: G2P\n",
    "if g2p == 'xpf':\n",
    "    xpf_translater_path = 'xpf_translate04.py'\n",
    "    rule_file_path = os.path.join(xpf_dir, code_xpf + '_' + lang_name, code_xpf + '.rules')\n",
    "    verify_file_path = os.path.join(xpf_dir, code_xpf + '_' + lang_name, code_xpf + '.verify.csv')\n",
    "elif g2p == 'epi':\n",
    "    epitran_translater_path = 'epi_run.py'\n",
    "elif g2p == 'chr':\n",
    "    from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "    chr_model = T5ForConditionalGeneration.from_pretrained('charsiu/g2p_multilingual_byT5_tiny_16_layers_100')\n",
    "    chr_tok = AutoTokenizer.from_pretrained('google/byt5-small')\n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "# For step 6: running MFA\n",
    "# Validate the corpus\n",
    "validated_recs_path = os.path.join(language_dir, 'validated')\n",
    "if if_subversion == 0:\n",
    "    acs_mod_path = os.path.join(mfa_mod_folder, acs_mod_name)\n",
    "else:\n",
    "    acs_mod_name = re.sub('.zip', subversion + '.zip', acs_mod_name)\n",
    "    acs_mod_path = os.path.join(mfa_mod_folder, acs_mod_name)\n",
    "output_path = os.path.join(language_dir, 'output')\n",
    "\n",
    "if if_self_mod == 1:\n",
    "    # Specify the path of the model\n",
    "    acs_mod_path = '/Users/miaozhang/Documents/MFA/pretrained_models/acoustic/ca_vxc_acoustic16.zip'\n",
    "if if_self_lex == 1:\n",
    "    # Specify the path of the lexicon\n",
    "    dict_file_path = os.path.join(language_dir, 'ca_lexicon-IPA.txt')   \n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "# Finale:\n",
    "txtgrds_path = os.path.join(osf_path, 'textgrids', textgrid_folder_name)\n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "print(\"Processing the folder:\\t\", language_dir)\n",
    "print(\"The acoustic model to be trained/used:\\t\", acs_mod_path)\n",
    "print(\"The lexicon to be generated/used:\\t\", dict_file_path)\n",
    "print(\"The speaker file to be generated:\\t\", spkr_file_path)\n",
    "print(\"The textgrid files to be generated:\\t\", txtgrds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Remap the validated speakers\n",
    "Get speaker IDs to put on TextGrids for speaker adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = vxcproc.remap_spkr(language_dir, spkr_file_path, lang_code)\n",
    "print(f'There are {len(valid)} validated recordings in total for ca.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Create TextGrid files for the validated recordings and save the them in a separate folder.\n",
    "\n",
    "All validated clips that are longer than 1s will be moved to a subfolder called 'validated'.\n",
    "\n",
    "The validated clips but are shorter than 1s will be moved to the 'other' folder.\n",
    "\n",
    "The invalidated clips will stay in the 'clips' folder. When the moving is done the 'clips' folder will be renamed to 'invalidated'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the folder for validated clips\n",
    "os.makedirs(validated_recs_path, exist_ok=True)\n",
    "\n",
    "# Setup file chunks to batch processing clip moving and textgrid creating \n",
    "n_clips = len(valid)\n",
    "n_workers = 10\n",
    "chunksize = round(n_clips / n_workers)\n",
    "\n",
    "# Move the clips and create textgrid files:\n",
    "with ThreadPoolExecutor(n_workers) as exe:\n",
    "    for i in range(0, len(valid), chunksize):\n",
    "        chunk_data = valid.loc[i:(i+chunksize),]\n",
    "        _ = exe.submit(vxcproc.move_and_create_tg, chunk_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Prepare the lexicon\n",
    "Generate the wordlist from Common Voice transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "if lang_code == 'ja':\n",
    "    words = vxcproc.process_words(spkr_file_path, lang_code)\n",
    "else:\n",
    "    words = vxcproc.process_words(validated_log, lang_code)\n",
    "\n",
    "# Filter other out unwanted words\n",
    "words = vxcproc.remove_unwanted_words(words, lang_code, if_cjk)\n",
    "\n",
    "# Save the word list as a .txt file\n",
    "if os.path.exists(word_file_path):\n",
    "    os.remove(word_file_path)\n",
    "    \n",
    "with open(word_file_path,'w') as word_file:\n",
    "    for word in words:\n",
    "        word_file.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. G2P grapheme-to-phoneme (Epitran or XPF)\n",
    "There three files you need to proceed if you use XPF.\n",
    "1. A G2P rule file\n",
    "2. A veryfication file\n",
    "3. The translater python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dict_file_path):\n",
    "    os.remove(dict_file_path)\n",
    "   \n",
    "# Get the dictionary using XPF\n",
    "# -l specifies the rule file\n",
    "# -c specifies the verification file\n",
    "# -r specifies the file to be translated\n",
    "if g2p == 'xpf':\n",
    "    g2p_cmd = [\"python\", xpf_translater_path, \"-l\", rule_file_path, \"-c\", verify_file_path, \"-r\", word_file_path] # XPF translating command that will be sent to subprocess.run() to execute.\n",
    "\n",
    "    with open(dict_file_path,'w') as dict_file:\n",
    "        subprocess.run(g2p_cmd, stdout = dict_file) # stdout = ... means to send the output to the file (so you have to open this file first as above)\n",
    "\n",
    "    # This is to get rid of all the '@' in the lexicon (if there is any). @ means that XPF G2P failure\n",
    "    with open(dict_file_path, \"r\") as dict_file:\n",
    "        dict = dict_file.read().split(\"\\n\")\n",
    "\n",
    "    with open(dict_file_path, 'w') as dict_file:\n",
    "        for i in dict:\n",
    "            i = re.sub(\" ː\", \"ː\", i)\n",
    "            # Get rid of words that contain sounds XPF can't figure out\n",
    "            if '@' not in i:\n",
    "                dict_file.write(i + \"\\n\")\n",
    "\n",
    "# Or using Epitran\n",
    "elif g2p == 'epi':\n",
    "    g2p_cmd = [\"python\", epitran_translater_path, word_file_path, dict_file_path, epi_code]\n",
    "    subprocess.run(g2p_cmd)\n",
    "\n",
    "# Or use Charsiu\n",
    "elif g2p == 'chr':\n",
    "    from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained('charsiu/g2p_multilingual_byT5_tiny_16_layers_100')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('google/byt5-small')\n",
    "\n",
    "    chr_words = [f'<{code_chr}>: '+i for i in words]\n",
    "\n",
    "    out = tokenizer(words, padding = True, add_special_tokens = False, return_tensors = 'pt')\n",
    "\n",
    "    preds = model.generate(**out, num_beams = 1, max_length = 50) # We do not find beam search helpful. Greedy decoding is enough. \n",
    "    phones = tokenizer.batch_decode(preds.tolist(), skip_special_tokens = True)\n",
    "\n",
    "    from ipatok import tokenise\n",
    "    phones = [tokenise(phone) for phone in phones]\n",
    "    phones = [' '.join(phone) for phone in phones]\n",
    "\n",
    "    dict = []\n",
    "    for sent, w in zip(sentence, phones):\n",
    "        dict.append(sent + '\\t' + w)\n",
    "\n",
    "# Or use the pretrained MFA G2P model\n",
    "elif g2p == 'mfa':\n",
    "    cmd_mfa_g2p = f'mfa g2p {word_file_path} {mfa_g2p_path} {dict_file_path}'  # If using a word list\n",
    "    print('To g2p, copy and run:\\t', cmd_mfa_g2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for some languages, you probably want to use the lexicon and the model from MFA or something of your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Validate the corpus\n",
    "\n",
    "First, you need to activate the MFA environment in the terminal.\n",
    "1. Press ctrl+` to open Terminal in VS Code.\n",
    "2. Run 'conda activate aligner' until you see '(aligner)' at the beginning of the line in Terminal.\n",
    "3. When you finished using MFA (both training and aligning), run 'conda deactivate' to shut down the MFA environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder of MFA in document\n",
    "# You DON'T need to run this if you already have an MFA folder in your Documents folder (What would this be like on Windows?)\n",
    "# Uncomment the command below to run:\n",
    "#!mfa model download acostic english.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the corpus, run this line in terminal: \n",
    "\n",
    "        mfa validate {wherever your validated recordings are} {wherever your lexicon file is} --ignore_acoustics --clean\n",
    "\n",
    "You can copy the command lines from below.\n",
    "Notebook can't handle ```mfa``` commands. MFA commands can only run in Terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_validate = f'mfa validate {validated_recs_path} {dict_file_path} --ignore_acoustics --clean'\n",
    "print('To validate, copy:\\t' + cmd_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Train the acoustic model and forced align.\n",
    "\n",
    "### Step 6.1. Then to train the acoustic model, run the next line:\n",
    "\n",
    "        mfa train --clean {where your validated recordings are} {where your lexicon file is} {where your model will be saved}\n",
    "\n",
    "You can copy the command lines from below.\n",
    "Notebook can't handle ```mfa``` commands. The mfa commands above can only run in Terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your own model\n",
    "cmd_train = f'mfa train --clean {validated_recs_path} {dict_file_path} {acs_mod_path}'\n",
    "print('To train, copy: \\t' + cmd_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.2. The final step: (split the validated folder and) forced align the recordings\n",
    "\n",
    "        mfa align --clean {where your validated recordings are} {where your lexicon file is} {where your acoustic model is} {where your output will be saved}\n",
    "        \n",
    "When the model is trained, align the corpus.\n",
    "\n",
    "However, since the MFA alignment somehow stops after generating 32609 textgrid files, we will split the corpus into n subfolders with each subfolder containing 32000 files.\n",
    "If the corpus has more than 32000 recordings, move the mp3 and textgrid files into subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the files and create the subfolders\n",
    "all_items = os.listdir(validated_recs_path)\n",
    "all_mp3 = [item for item in all_items if os.path.splitext(item)[1] == '.mp3']\n",
    "n_clips = len(all_mp3)\n",
    "print(f\"There are {n_clips} clips in the validated folder.\")\n",
    "\n",
    "# Create the subfolders\n",
    "if n_clips > 32000:\n",
    "    subfolders = valid['subfolder'].unique()\n",
    "    for subfolder in subfolders:\n",
    "        subfolder_path = os.path.join(validated_recs_path, subfolder)\n",
    "        if not os.path.exists(subfolder_path):\n",
    "            os.makedirs(subfolder_path)\n",
    "\n",
    "    # Create the paths in the subfolders for each recording according to their grouping\n",
    "    splits = valid[valid['path'].isin(all_mp3)]\n",
    "    splits.to_csv(os.path.join(language_dir, 'all_splits.csv'), index = False)\n",
    "\n",
    "del all_items, all_mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the files into subfolders\n",
    "if n_clips > 32000:\n",
    "    n_workers = 10\n",
    "    chunksize = round(len(splits) / n_workers)\n",
    "    with ThreadPoolExecutor(n_workers) as exe:\n",
    "        for i in range(0, len(splits), chunksize):\n",
    "            chunk_data = splits.loc[i:(i+chunksize),]\n",
    "            _ = exe.submit(vxcproc.split_recs, chunk_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all mp3 and textgrid files are moved to subfolders, and check if there are any overlapping file names across the subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all mp3 and textgrid files are moved to the subfolders\n",
    "if n_clips > 32000:\n",
    "    all_items = os.listdir(validated_recs_path)\n",
    "    contains_subdir = any(\n",
    "        os.path.isfile(os.path.join(validated_recs_path, item)) and \n",
    "        (item.lower().endswith('.mp3') or item.lower().endswith('.textgrid')) \n",
    "        for item in all_items\n",
    "        )\n",
    "    if contains_subdir:\n",
    "        print(\"The validated folder still contains mp3 or TextGrid files.\")\n",
    "        print('')\n",
    "    else:\n",
    "        print(\"All mp3 or TextGrid files are moved to subfolders.\")\n",
    "        print('')\n",
    "\n",
    "    # Check if there are overlapping file names across the subfolders\n",
    "    overlap_dict = vxcproc.check_file_overlaps(validated_recs_path)\n",
    "    if len(overlap_dict) == 0:\n",
    "        print(\"There are no overlapping file names across the subfolders.\") \n",
    "    else:\n",
    "        print(overlap_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the MFA commands to align the data in (each subfolder of) the validated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the MFA commands for alignment\n",
    "all_files = os.listdir(validated_recs_path)\n",
    "all_files = [file for file in all_files if '.DS_Store' not in file]\n",
    "all_subfolders.sort()\n",
    "any_folder = any(os.path.isdir(os.path.join(validated_recs_path, subfolder)) for subfolder in all_subfolders)\n",
    "if any_folder:\n",
    "    for index, item in enumerate(all_subfolders):\n",
    "        subfolder_path = os.path.join(validated_recs_path, item)\n",
    "        cmd_validate = f'mfa validate {subfolder_path} {dict_file_path} --ignore_acoustics --clean'\n",
    "        cmd_train = f'mfa align --clean {subfolder_path} {dict_file_path} {acs_mod_path} {output_path}'\n",
    "        #print(f'To validate split {index+1}, copy:\\t', cmd_validate, '\\n')\n",
    "        print(f'To align split {index+1}, copy: \\t', cmd_train)\n",
    "        print('\\n')\n",
    "else:  \n",
    "    cmd_train = f'mfa align --clean {validated_recs_path} {dict_file_path} {acs_mod_path} {output_path}'\n",
    "    print('To align, copy: \\t' + cmd_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.3: (optional) Put back the recordings to the validated folder\n",
    "\n",
    "When the alignment is done, if splits were created for aligning the data, put the recordings back to one single folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clips = len(valid)\n",
    "if n_clips > 32000:\n",
    "    # Put the files back\n",
    "    n_workers = 10\n",
    "    chunksize = round(len(valid) / n_workers)\n",
    "    with ThreadPoolExecutor(n_workers) as exe:\n",
    "        for i in range(0, len(valid), chunksize):\n",
    "            chunk_data = valid.loc[i:(i+chunksize),]\n",
    "            _ = exe.submit(vxcproc.merge_recs, chunk_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all files are put back to the validated folder's root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_clips > 32000:\n",
    "    # Use os.scandir() for better performance\n",
    "    with os.scandir(validated_recs_path) as entries:\n",
    "        subfolders = [entry.name for entry in entries if entry.is_dir()]\n",
    "        subfolders.sort()\n",
    "\n",
    "    # Lists to store undeleted subfolders and files\n",
    "    undeleted_subfolders = []\n",
    "\n",
    "    # Batch deletion of empty subfolders\n",
    "    for subfolder in subfolders:\n",
    "        subfolder_path = os.path.join(validated_recs_path, subfolder)\n",
    "        with os.scandir(subfolder_path) as sub_entries:\n",
    "            if not any(entry.is_file() for entry in sub_entries):\n",
    "                # If the subfolder does not contain any files, delete it\n",
    "                shutil.rmtree(subfolder_path)\n",
    "                print(f\"Subfolder '{subfolder}' deleted because it contains no files.\")\n",
    "            else:\n",
    "                undeleted_subfolders.append(subfolder)\n",
    "\n",
    "    print(\"Subfolders checked and processed.\")\n",
    "\n",
    "    # List undeleted subfolders\n",
    "    if len(undeleted_files) > 0:\n",
    "        print(\"Undeleted subfolders:\")\n",
    "        for subfolder in undeleted_subfolders:\n",
    "            print(subfolder)\n",
    "\n",
    "    # Check if all the subfolders are deleted\n",
    "    with os.scandir(validated_recs_path) as entries:\n",
    "        contains_subdir = any(entry.is_dir() for entry in entries)\n",
    "        if contains_subdir:\n",
    "            print(\"\\nThe validated folder still contains subfolders.\")\n",
    "        else:\n",
    "            print(\"\\nThe validated folder does not contain any subfolders now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the output and input files matches with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool() as pool:\n",
    "    result = pool.apply(vxcproc.compare_inout, args=(output_path, validated_recs_path))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, move the output files (the speaker file, the lexicon, the acoustic model, and the aligned textgrids) to the OSF folder to be ready to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a .tar.gz file of the aligned textgrids\n",
    "# list all files to add to the zip\n",
    "tgfiles = [os.path.join(output_path, filename) for filename in os.listdir(output_path)]\n",
    "# create lock for adding files to the zip\n",
    "lock = Lock()\n",
    "# open the zip file\n",
    "with zipfile.ZipFile(txtgrds_path, 'w', compression=zipfile.ZIP_DEFLATED) as handle:\n",
    "    # create the thread pool\n",
    "    with ThreadPoolExecutor(10) as exe:\n",
    "        # add all files to the zip archive\n",
    "        _ = [exe.submit(vxcproc.add_file, lock, handle, tg) for tg in tgfiles]\n",
    "\n",
    "# Move the acoustic model\n",
    "shutil.copy(acs_mod_path, os.path.join(osf_path, 'acoustic_models'))\n",
    "\n",
    "# Move the lexicon\n",
    "shutil.copy(dict_file_path, os.path.join(osf_path, 'lexicons'))\n",
    "\n",
    "# Move the speaker file\n",
    "shutil.copy(spkr_file_path, os.path.join(osf_path, 'spkr_files'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, upadate the tracking info in `VoxCommunis_Info.csv`. \n",
    "\n",
    "Make sure it is not in the lang_code_processing folder. Once updated, push the updated .csv to the GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have trained the model, set this to 1\n",
    "model_trained = 1\n",
    "aligned = 1\n",
    "\n",
    "# Paste the name of the outputs into the tracking file\n",
    "cv_track = pd.read_csv(cv_tracking_file)\n",
    "cv_track = cv_track.astype('string')\n",
    "if model_trained == 1:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'acoustic_model'] = acs_mod_name\n",
    "else:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'acoustic_model'] = ''\n",
    "if aligned == 1:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'textgrids'] = textgrid_folder_name\n",
    "else:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'textgrids'] = ''\n",
    "cv_track.loc[cv_track['code_cv'] == lang_code, 'spkr_file'] = spkr_file_name\n",
    "cv_track.loc[cv_track['code_cv'] == lang_code, 'lexicon'] = dict_file_name\n",
    "\n",
    "\n",
    "# Update the tracking file\n",
    "cv_track.to_csv(cv_tracking_file, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
