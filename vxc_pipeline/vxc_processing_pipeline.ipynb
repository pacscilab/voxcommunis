{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VoxCommunis data processing pipeline\n",
    "\n",
    "This is a script of running MFA on recordings from Common Voice corpus. \n",
    "\n",
    "To run this pipeline, you need to download:\n",
    "\n",
    "1. Python modules: epitran, praatio, re, pandas, numpy, subprocess, shutil, os\n",
    "2. The data of XPF corpus\n",
    "\n",
    "The pipeline takes these steps to process data:\n",
    "\n",
    "1. [Step 0: Setups](#step-0-setups)\n",
    "2. [Step 1: Remap speakers](#step-1-remap-the-validated-speakers)\n",
    "3. [Step 2: Create TextGrid and .wav files](#step-2-create-textgrid-files-and-wav-files-based-on-the-mp3-recordings-from-common-voice)\n",
    "4. [Step 3: Prepare the lexicon](#step-3-prepare-the-lexicon)\n",
    "5. [Step 4: G2P grapheme-to-phoneme](#step-4-g2p-grapheme-to-phoneme-epitran-or-xpf)\n",
    "6. [Step 5: Validation](#step-5-train-the-acoustic-model)\n",
    "7. [Step 6: Run MFA](#step-6-train-the-acoustic-model-and-forced-align)\n",
    "8. [Finale](#finale)\n",
    "\n",
    "This script was created by Miao Zhang (miao.zhang@uzh.ch), 22.12.2023\n",
    "\n",
    "This script was modified by Miao Zhang, 07.02.2024 (Revalidation added)\n",
    "\n",
    "Modified on 16.02.2024: added automatic log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Setups\n",
    "Import packages and setup file directories (for both the scripts and data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os, subprocess, shutil, re, csv, sys, importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For creating textgrids\n",
    "from praatio import textgrid\n",
    "\n",
    "# For move files concurrently\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Import functions from cv_processing.py\n",
    "import vxc_processing as vxcproc\n",
    "importlib.reload(vxcproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.system('pip install janome')\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "def tokenize_japanese_text(text):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [token.surface for token in tokens]\n",
    "\n",
    "def remove_japanese_punctuation(text):\n",
    "    # Define a regular expression pattern to match Japanese punctuations\n",
    "    japanese_punctuation_pattern = r'[、。！？・…「」『』（）［］【】〈〉《》〔〕／＼・]'\n",
    "    # Replace Japanese punctuations with an empty string\n",
    "    cleaned_text = re.sub(japanese_punctuation_pattern, ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "japanese_text = \"今日はいい天気です。明日は雨が降ります。\"\n",
    "clean_text = remove_japanese_punctuation(japanese_text)\n",
    "tokenized_text = tokenize_japanese_text(clean_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the paths and directories of data and scripts to use.\n",
    "\n",
    "_IMPORTANT_: the folder of the corpus data you downloaded from Common Voice should be named as: {lang_code}_v{version_number}.\n",
    "- For example: the folder for the 16th version of Divhehi should be named: dv_v16.\n",
    "- Another example: the folder for the 15th version of Upper Sorbian should be: hsb_v15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### Directories ################################################\n",
    "\n",
    "# This is the directory where your data downloaded from Common Voice should be saved. This is the root directory where data from each language should be saved in individual folders.\n",
    "# NO (BACK)SLASH at the end!!!\n",
    "commonVoice_dir = '/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice' \n",
    "\n",
    "# To use XPF as the G2P engine to process lexicon, you will need to download the XPF data from: https://github.com/CohenPr-XPF/XPF/tree/master/Data and save them on your computer.\n",
    "# Specify the directory where your XPF data is saved.\n",
    "# NO (BACK)SLASH at the end!!!\n",
    "xpf_dir = '/Users/miaozhang/Research/CorpusPhon/CorpusData/G2P/XPF' \n",
    "\n",
    "######################### Language name/code and Common Voice version ##############################\n",
    "\n",
    "# Language-related variable names\n",
    "# the Common Voice code of the language (unfortunately, Common Voice mixes the use of iso 639-3 and iso 639-1 codes (they use bcp47 code). This code is also used in XPF).\n",
    "# The code should match the code used in the name of the folder you downloaded from Common Voice.\n",
    "lang_code = 'ha' \n",
    "\n",
    "# The version of the data in Common Voice\n",
    "# Only numbers!!!\n",
    "cv_version = '16' \n",
    "\n",
    "######################### G2P ######################################################################\n",
    "\n",
    "# Specify the G2P engine. Only three keywords are acceptable: \n",
    "# 'xpf' for XPF\n",
    "# 'epi' for Epitran\n",
    "# 'chr' for Charsiu\n",
    "g2p = 'epi'\n",
    "\n",
    "######################### The delimiter ############################################################\n",
    "# Set it to 0 if you use a Windows machine.\n",
    "if_mac = 1 \n",
    "\n",
    "if if_mac == 1:\n",
    "    path_sep = '/'\n",
    "    # this is the default directory where Praat is installed on a Mac.\n",
    "    praat_path = '/Applications/Praat.app/Contents/MacOS/Praat' \n",
    "else:\n",
    "    path_sep = '\\\\'\n",
    "    # the directory of Praat installed on Windows.\n",
    "    praat_path = 'C:\\Program Files\\Praat.exe' \n",
    "\n",
    "######################### What writing system is the language using? ###############\n",
    "\n",
    "# Specify if the language is Chinese/Japanese/Korean\n",
    "if_cjk = 0\n",
    "\n",
    "\n",
    "###################### G2P settings (XPF or Epitran) ################################################\n",
    "\n",
    "# This is where VxcCommunis_tracking.csv is (NO (BACK)SLASH at the end!!!):\n",
    "cv_tracking_file = 'VoxCommunis_Info.csv'\n",
    "\n",
    "# Get the G2P processing code for the language\n",
    "if g2p == 'xpf' or 'chr':\n",
    "    with open(cv_tracking_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        lang_row = [row for row in reader if row['code_cv'] == lang_code][0]\n",
    "    # If you are using XPF, get the name of the language in XPF corpus\n",
    "    if g2p == 'xpf':\n",
    "        lang_name = lang_row['name_xpf'].replace(' ', '')\n",
    "    # If you are using Charsiu, get the processing code for the language in Charsiu.\n",
    "    elif g2p == 'chr':\n",
    "        code_chr = lang_row['code_chr']\n",
    "elif g2p == 'epi':\n",
    "    # If you are using epitran, ...\n",
    "    # Please refer to VoxCommunics_info.csv to get the processing code of the language in epitran\n",
    "    # !!!Do this manually, since depending on the type of the orthography, the epitran code can differ!!!\n",
    "    epi_code = 'ces-Latn'\n",
    "\n",
    "# Specify if the subversion of a corpus is used. The default is 0\n",
    "if_subversion = 0 \n",
    "# If if_subversion == 1, what suffix you would use?:\n",
    "# Ignore this part, if you don't have a subversion of the corpus you are using.\n",
    "subversion = '_' + 'sub3'\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "# The folder for the language\n",
    "language_dir = commonVoice_dir + path_sep + lang_code + '_v' + cv_version\n",
    "\n",
    "# The file that contains the duration of each clip:\n",
    "clip_info_path = language_dir + path_sep + 'clip_durations.tsv'\n",
    "\n",
    "# The folder of the OOV word files (NO (BACK)SLASH at the end!!!):\n",
    "mfa_oov_path = '/Users/miaozhang/Documents/MFA/validated'\n",
    "\n",
    "# This is where the acoustic model will be saved after MFA training is done (NO (BACK)SLASH at the end!!!):\n",
    "mfa_mod_path = '/Users/miaozhang/Documents/MFA/pretrained_models/acoustic'\n",
    "\n",
    "# This is where files that will be uploaded to the OSF repo will be saved after the processing is finished (NO (BACK)SLASH at the end!!!):\n",
    "osf_path = '/Users/miaozhang/Research/CorpusPhon/CorpusData/VoxCommunis_OSF'\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "\n",
    "###################################### DO NOT CHANGE ANYTHING IN THIS BLOCK FROM BELOW #############################################\n",
    "\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "\n",
    "# Get the naming schema.\n",
    "naming_schema = pd.read_csv('vxc_naming_schema.csv', usecols = ['Python_code'])['Python_code'].tolist()\n",
    "naming_schema = [eval(name) for name in naming_schema]\n",
    "acs_mod_name = naming_schema[0]\n",
    "textgrid_folder_name = naming_schema[1]\n",
    "word_file_name = naming_schema[2]\n",
    "dict_file_name = naming_schema[3]\n",
    "spkr_file_name = naming_schema[4]\n",
    "textgrid_folder_path = language_dir + path_sep + textgrid_folder_name\n",
    "word_file_path = language_dir + path_sep + word_file_name\n",
    "dict_file_path = language_dir + path_sep + dict_file_name\n",
    "spkr_file_path = language_dir + path_sep + spkr_file_name\n",
    "del naming_schema\n",
    "\n",
    "# For step 3: prepare the lexicon and pronunciation dictionary\n",
    "validated_log = language_dir + path_sep + 'validated.tsv'\n",
    "\n",
    "# For step 4: G2P\n",
    "if if_xpf == 1:\n",
    "    xpf_translater_path = 'xpf_translate04.py'\n",
    "    rule_file_path = xpf_dir + path_sep + lang_code + '_' + lang_name + path_sep + lang_code + '.rules'\n",
    "    verify_file_path = xpf_dir + path_sep + lang_code + '_' + lang_name + path_sep + lang_code + '.verify.csv'\n",
    "else:\n",
    "    epitran_translater_path = 'epi_run.py'\n",
    "\n",
    "# For step 6: running MFA\n",
    "# Validate the corpus\n",
    "validated_recs_path = language_dir + path_sep + 'validated'\n",
    "if if_subversion == 0:\n",
    "    acs_mod_path = mfa_mod_path + path_sep + acs_mod_name\n",
    "else:\n",
    "    acs_mod_name = re.sub('.zip', subversion + '.zip', acs_mod_name)\n",
    "    acs_mod_path = mfa_mod_path + path_sep + acs_mod_name\n",
    "output_path = language_dir + path_sep + 'output'\n",
    "\n",
    "# Finale:\n",
    "txtgrds_path = osf_path + path_sep + 'textgrids' + path_sep + textgrid_folder_name[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Remap the validated speakers\n",
    "Get speaker IDs to put on TextGrids for speaker adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole = vxcproc.remap_spkr(language_dir, path_sep, spkr_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Create TextGrid files for the validated recordings and save the them in a separate folder.\n",
    "\n",
    "All validated clips that are longer than 1s will be moved to a subfolder called 'validated'.\n",
    "\n",
    "The validated clips but are shorter than 1s will be moved to the 'other' folder.\n",
    "\n",
    "The invalidated clips will stay in the 'clips' folder. When the moving is done the 'clips' folder will be renamed to 'invalidated'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the folder for validated clips and other clips:\n",
    "validated_folder = language_dir + path_sep + 'validated'\n",
    "other_folder = language_dir + path_sep + 'other'\n",
    "if os.path.exists(validated_folder) or os.path.exists(other_folder):\n",
    "    shutil.rmtree(validated_folder)\n",
    "    shutil.rmtree(other_folder)\n",
    "os.makedirs(validated_folder)\n",
    "os.makedirs(other_folder)\n",
    "\n",
    "# Setup file chunks to batch processing clip moving and textgrid creating \n",
    "n_clips = len(whole.index)\n",
    "n_workers = 10\n",
    "chunksize = round(n_clips / n_workers)\n",
    "\n",
    "# Move the clips and create textgrid files:\n",
    "with ThreadPoolExecutor(n_workers) as exe:\n",
    "    for i in range(0, len(whole.index), chunksize):\n",
    "        chunk_data = whole.loc[i:(i+chunksize),]\n",
    "        _ = exe.submit(vxcproc.move_and_create_tg, chunk_data)\n",
    "\n",
    "# Rename the clip folder to invalidated\n",
    "os.rename(language_dir + path_sep + 'clips', language_dir + path_sep + 'invalidated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Prepare the lexicon\n",
    "Extract transcripts from validated.tsv and get each word on its own line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "words = vxcproc.process_words(validated_log)\n",
    "\n",
    "# Filter other out unwanted words\n",
    "words = vxcproc.remove_unwanted_words(words, lang_code, if_cjk)\n",
    "\n",
    "# Save the word list as a .txt file\n",
    "if os.path.exists(word_file_path):\n",
    "    os.remove(word_file_path)\n",
    "    \n",
    "with open(word_file_path,'w') as word_file:\n",
    "    for word in words:\n",
    "        word_file.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. G2P grapheme-to-phoneme (Epitran or XPF)\n",
    "There three files you need to proceed if you use XPF.\n",
    "1. A G2P rule file\n",
    "2. A veryfication file\n",
    "3. The translater python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dict_file_path):\n",
    "    os.remove(dict_file_path)\n",
    "   \n",
    "# Get the dictionary using XPF\n",
    "# -l specifies the rule file\n",
    "# -c specifies the verification file\n",
    "# -r specifies the file to be translated\n",
    "if g2p == 'xpf':\n",
    "    g2p_cmd = [\"python\", xpf_translater_path, \"-l\", rule_file_path, \"-c\", verify_file_path, \"-r\", word_file_path] # XPF translating command that will be sent to subprocess.run() to execute.\n",
    "\n",
    "    with open(dict_file_path,'w') as dict_file:\n",
    "        subprocess.run(g2p_cmd, stdout = dict_file) # stdout = ... means to send the output to the file (so you have to open this file first as above)\n",
    "\n",
    "    # This is to get rid of all the '@' in the lexicon (if there is any). @ means that XPF G2P failure\n",
    "    with open(dict_file_path, \"r\") as dict_file:\n",
    "        dict = dict_file.read().split(\"\\n\")\n",
    "\n",
    "    with open(dict_file_path, 'w') as dict_file:\n",
    "        for i in dict:\n",
    "            i = re.sub(\" ː\", \"ː\", i)\n",
    "            # Get rid of words that contain sounds XPF can't figure out\n",
    "            if '@' not in i:\n",
    "                dict_file.write(i + \"\\n\")\n",
    "# Or using Epitran\n",
    "else:\n",
    "    g2p_cmd = [\"python\", epitran_translater_path, word_file_path, dict_file_path, epi_code]\n",
    "    subprocess.run(g2p_cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Validate the corpus\n",
    "\n",
    "First, you need to activate the MFA environment in the terminal.\n",
    "1. Press ctrl+` to open Terminal in VS Code.\n",
    "2. Run 'conda activate aligner' until you see '(aligner)' at the beginning of the line in Terminal.\n",
    "3. When you finished using MFA (both training and aligning), run 'conda deactivate' to shut down the MFA environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder of MFA in document\n",
    "# You DON'T need to run this if you already have an MFA folder in your Documents folder (What would this be like on Windows?)\n",
    "# Uncomment the command below to run:\n",
    "#!mfa model download acostic english.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the corpus, run this line in terminal: \n",
    "\n",
    "        mfa validate {wherever your validated recordings are} {wherever your lexicon file is} --ignore_acoustics --clean\n",
    "\n",
    "You can copy the command lines from below.\n",
    "Notebook can't handle ```mfa``` commands. MFA commands can only run in Terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_validate = f'mfa validate {validated_recs_path} {dict_file_path} --ignore_acoustics --clean'\n",
    "print('To validate, copy:\\t' + cmd_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the oov words back into the word list and rerun G2P.\n",
    "\n",
    "I DO NOT recommend running this step as it puts some weird words that were filtered out back to the lexicon. \n",
    "\n",
    "In the MFA documentation, it's also mentioned that if we want to use this model to align other data in the future, it's better to leave some OOV words in the corpus. \n",
    "\n",
    "But if you want to rule out as many oov words as possible, go ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The oov file:\n",
    "oov_file = 'oovs_found_' + dict_file_name\n",
    "\n",
    "oov_path = mfa_oov_path + path_sep + oov_file\n",
    "with open(oov_path, 'r') as oov_file:\n",
    "    with open(word_file_path, 'a') as wordlist:\n",
    "        shutil.copyfileobj(oov_file, wordlist)\n",
    "\n",
    "# And then rerun Step 4. G2P to process the oov words.\n",
    "if g2p == 'xpf':\n",
    "    g2p_cmd = [\"python\", xpf_translater_path, \"-l\", rule_file_path, \"-c\", verify_file_path, \"-r\", word_file_path]\n",
    "    with open(dict_file_path,'w') as dict_file:\n",
    "        subprocess.run(g2p_cmd, stdout = dict_file) \n",
    "    with open(dict_file_path, \"r\") as dict_file:\n",
    "        dict = dict_file.read().split(\"\\n\")\n",
    "    with open(dict_file_path, 'w') as dict_file:\n",
    "        for i in dict:\n",
    "            i = re.sub(\" ː\", \"ː\", i)\n",
    "            if '@' not in i: \n",
    "                dict_file.write(i + \"\\n\")\n",
    "else:\n",
    "    g2p_cmd = [\"python\", epitran_translater_path, word_file_path, dict_file_path, epi_code]\n",
    "    subprocess.run(g2p_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To revalidate the corpus, copy and paste the command below.\n",
    "print('To validate, copy:\\t' + cmd_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Train the acoustic model and forced align.\n",
    "\n",
    "### Step 6.1. Then to train the acoustic model, run the next line:\n",
    "\n",
    "        mfa train --clean {where your validated recordings are} {where your lexicon file is} {where your model will be saved}\n",
    "\n",
    "### Step 6.2. The final step: forced align the recordings:\n",
    "\n",
    "        mfa train --clean {where your validated recordings are} {where your lexicon file is} {where your output will be saved}\n",
    "\n",
    "You can copy the command lines from below.\n",
    "Notebook can't handle ```mfa``` commands. The mfa commands above can only run in Terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_train = f'mfa train --clean {validated_recs_path} {dict_file_path} {acs_mod_path}'\n",
    "cmd_align = f'mfa align --clean {validated_recs_path} {dict_file_path} {acs_mod_path} {output_path}'\n",
    "\n",
    "print('To train, copy: \\t' + cmd_train)\n",
    "print(\"\\n\")\n",
    "print('To align, copy: \\t' + cmd_align)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finale\n",
    "Move the output files (the speaker file, the lexicon, the acoustic model, and the aligned textgrids) to the OSF folder to be ready to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a zip file of the aligned textgrids\n",
    "shutil.make_archive(txtgrds_path, 'zip', output_path)\n",
    "\n",
    "# Move the acoustic model\n",
    "shutil.copy(acs_mod_path, osf_path + path_sep + 'acoustic_models' + path_sep)\n",
    "\n",
    "# Move the lexicon\n",
    "shutil.copy(dict_file_path, osf_path + path_sep + 'lexicons' + path_sep)\n",
    "\n",
    "# Move the speaker file\n",
    "shutil.copy(spkr_file_path, osf_path + path_sep + 'spkr_files' + path_sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upadate the tracking info in `VoxCommunis_Info.csv`. \n",
    "\n",
    "Make sure it is not in the lang_code_processing folder. Once updated, push the updated .csv to the GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have trained the model, set this to 1\n",
    "model_train = 1\n",
    "aligned = 1\n",
    "\n",
    "# Paste the name of the outputs into the tracking file\n",
    "cv_track = pd.read_csv(cv_tracking_file)\n",
    "cv_track = cv_track.astype('string')\n",
    "if model_train == 1:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'acoustic_model'] = acs_mod_name\n",
    "else:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'acoustic_model'] = ''\n",
    "if aligned == 1:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'textgrids'] = textgrid_folder_name\n",
    "else:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'textgrids'] = ''\n",
    "cv_track.loc[cv_track['code_cv'] == lang_code, 'spkr_file'] = spkr_file_name\n",
    "cv_track.loc[cv_track['code_cv'] == lang_code, 'lexicon'] = dict_file_name\n",
    "\n",
    "\n",
    "# Update the tracking file\n",
    "cv_track.to_csv(cv_tracking_file, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
