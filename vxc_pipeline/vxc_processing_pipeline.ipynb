{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VoxCommunis data processing pipeline\n",
    "\n",
    "This is a script of running MFA on recordings from Common Voice corpus. \n",
    "\n",
    "To run this pipeline, you need to download:\n",
    "\n",
    "1. ffmpeg (a command line tool to convert multimedia files including both audio and video)\n",
    "2. Python modules: epitran, pydub, praatio\n",
    "3. The data of XPF corpus\n",
    "\n",
    "The pipeline takes these steps to process data:\n",
    "\n",
    "1. [Step 0: Setups](#step-0-setups)\n",
    "2. [Step 1: Remap speakers](#step-1-remap-the-validated-speakers)\n",
    "3. [Step 2: Create TextGrid and .wav files](#step-2-create-textgrid-files-and-wav-files-based-on-the-mp3-recordings-from-common-voice)\n",
    "4. [Step 3: Prepare the lexicon](#step-3-prepare-the-lexicon)\n",
    "5. [Step 4: G2P grapheme-to-phoneme](#step-4-g2p-grapheme-to-phoneme-epitran-or-xpf)\n",
    "6. [Step 5: Validation](#step-5-train-the-acoustic-model)\n",
    "7. [Step 6: Run MFA](#step-6-train-the-acoustic-model-and-forced-align)\n",
    "8. [Finale](#finale)\n",
    "\n",
    "This script was created by Miao Zhang (miao.zhang@uzh.ch), 22.12.2023\n",
    "\n",
    "This script was modified by Miao Zhang, 07.02.2024 (Revalidation added)\n",
    "\n",
    "Modified on 16.02.2024: added automatic log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Setups\n",
    "Import packages and setup file directories (for both the scripts and data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os, subprocess, shutil, re\n",
    "from pydub import AudioSegment\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the paths and directories of data and scripts to use.\n",
    "\n",
    "_IMPORTANT_: the folder of the corpus data you downloaded from Common Voice should be named as: {lang_code}_v{version_number}.\n",
    "- For example: the folder for the 16th version of Divhehi should be named: dv_v16.\n",
    "- Another example: the folder for the 15th version of Upper Sorbian should be: hsb_v15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### Directories #########################################\n",
    "\n",
    "# This is the directory where your data downloaded from Common Voice should be saved. This is the root directory where data from each language is saved in individual folders.\n",
    "# NO (BACK)SLASH at the end!!!\n",
    "commonVoice_dir = '/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice' \n",
    "\n",
    "# To use XPF as the G2P engine to process lexicon, you will need to download the XPF data from: https://github.com/CohenPr-XPF/XPF/tree/master/Data and save them on your computer.\n",
    "# Specify the directory where your XPF data is saved.\n",
    "# NO (BACK)SLASH at the end!!!\n",
    "xpf_dir = '/Users/miaozhang/Research/CorpusPhon/CorpusData/G2P/XPF' \n",
    "\n",
    "######################### Language name/code and Common Voice version ##############################\n",
    "\n",
    "# Language-related variable names\n",
    "# the Common Voice code of the language (unfortunately, Common Voice mixes the use of iso 639-3 and iso 639-1 codes (they use bcp47 code). This code is also used in XPF).\n",
    "# The code should match the code used in the name of the folder you downloaded from Common Voice.\n",
    "lang_code = 'hsb' \n",
    "\n",
    "# Set it to 0 if you use a Windows machine.\n",
    "if_mac = 1 \n",
    "\n",
    "# Specify the G2P engine. If 0, then epitran\n",
    "if_xpf = 1\n",
    "\n",
    "if if_xpf == 1:\n",
    "    # If you are using XPF, ...\n",
    "    # Please refer to VoxCommunics_info.csv to get the name of the language in XPF\n",
    "    # You can ignore this if you are not going to use XPF.\n",
    "    lang_name = 'Upper Sorbian'\n",
    "else:\n",
    "    # If you are using epitran, ...\n",
    "    # Please refer to VoxCommunics_info.csv to get the processing code of the language in epitran\n",
    "    epi_code = 'kin-Latin'\n",
    "\n",
    "# The version of the data in Common Voice\n",
    "# Only numbers!!!\n",
    "cv_version = '16' \n",
    "\n",
    "# Specify if the subversion of a corpus is used. The default is 0\n",
    "if_subversion = 0 \n",
    "# If if_subversion == 1, what suffix you would use?:\n",
    "subversion = '_' + 'sub3'\n",
    "\n",
    "##################################################################\n",
    "\n",
    "# The folder of the OOV word files (NO (BACK)SLASH at the end!!!):\n",
    "mfa_oov_path = '/Users/miaozhang/Documents/MFA/validated'\n",
    "\n",
    "# This is where the acoustic model will be saved after MFA training is done (NO (BACK)SLASH at the end!!!):\n",
    "mfa_mod_path = '/Users/miaozhang/Documents/MFA/pretrained_models/acoustic'\n",
    "\n",
    "# This is where files that will be uploaded to the OSF repo will be saved after the processing is finished (NO (BACK)SLASH at the end!!!):\n",
    "osf_path = '/Users/miaozhang/Documents/VoxCommunis_OSF'\n",
    "\n",
    "# This is where VxcCommunis_tracking.csv is (NO (BACK)SLASH at the end!!!):\n",
    "cv_tracking_file = 'VoxCommunis_Info.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the paths and directories (You don't need to chang anything in this script from this point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if if_mac == 1:\n",
    "    path_sep = '/'\n",
    "    # this is the default directory where Praat is installed on a Mac.\n",
    "    praat_path = '/Applications/Praat.app/Contents/MacOS/Praat' \n",
    "else:\n",
    "    path_sep = '\\\\'\n",
    "    # the directory of Praat installed on Windows.\n",
    "    praat_path = 'C:\\Program Files\\Praat.exe' \n",
    "\n",
    "if if_xpf == 1:\n",
    "    g2p = 'xpf'\n",
    "else:\n",
    "    g2p = 'epi'\n",
    "\n",
    "language_dir = lang_code + '_v' + cv_version\n",
    "\n",
    "# Get the naming schema. (Don't change this part)\n",
    "naming_schema = pd.read_csv('vxc_naming_schema.csv')\n",
    "acs_mod_name = naming_schema['Python_code'][0]\n",
    "textgrid_folder_name = naming_schema['Python_code'][1]\n",
    "spkr_file_name = naming_schema['Python_code'][4]\n",
    "word_file_name = naming_schema['Python_code'][2]\n",
    "dict_file_name = naming_schema['Python_code'][3]\n",
    "\n",
    "# For step 1: speaker remapping\n",
    "# Get the full paths\n",
    "remap_spkr_path = []\n",
    "remap_spkr_path.append('vxc_remap_spkrs.py') # where the scipt of speaker remapping is\n",
    "remap_spkr_path.append(commonVoice_dir + path_sep + language_dir + path_sep + 'validated.tsv') # where the validated utterance log of common voice is\n",
    "remap_spkr_path.append(commonVoice_dir + path_sep + language_dir + path_sep + eval(spkr_file_name)) # where the validated speaker log will be saved\n",
    "\n",
    "\n",
    "# For step 2: Create .wav and .TextGrid files\n",
    "# Get the path of the praat script\n",
    "create_txgdwav_script = 'vxc_createTextGridsWav.praat' # this is where the praat script was saved.\n",
    "\n",
    "# Set the arguments for the praat script\n",
    "praat_args = []\n",
    "praat_args.append(commonVoice_dir + path_sep + language_dir) #this is the directory of the language. NO path_sep!!!\n",
    "praat_args.append(commonVoice_dir + path_sep + language_dir + path_sep + 'validated') #this is the folder name of validated files. NO path_sep at the end!!!\n",
    "praat_args.append(commonVoice_dir + path_sep + language_dir + path_sep + eval(spkr_file_name)) #this is remapped speaker file\n",
    "\n",
    "# For step 3: prepare the lexicon and pronunciation dictionary\n",
    "# Remember the file is saved in this variable:\n",
    "validated_log = remap_spkr_path[1]\n",
    "wordlist_path = commonVoice_dir + path_sep + language_dir + path_sep + eval(word_file_name)\n",
    " \n",
    "\n",
    "# For step 4: G2P\n",
    "if if_xpf == 1:\n",
    "    xpf_translater_path = 'xpf_translate04.py'\n",
    "    rule_file_path = xpf_dir + path_sep + lang_code + '_' + lang_name + path_sep + lang_code + '.rules'\n",
    "    verify_file_path = xpf_dir + path_sep + lang_code + '_' + lang_name + path_sep + lang_code + '.verify.csv'\n",
    "else:\n",
    "    epitran_translater_path = 'epi_run.py'\n",
    "\n",
    "dict_file_path = commonVoice_dir + path_sep + language_dir + path_sep + eval(dict_file_name)\n",
    "\n",
    "# For step 6: running MFA\n",
    "# Validate the corpus\n",
    "validated_recs_path = praat_args[1] \n",
    "if if_subversion == 0:\n",
    "    acs_mod_path = mfa_mod_path + path_sep + eval(acs_mod_name)\n",
    "else:\n",
    "    acs_mod_name = re.sub('.zip', subversion + '.zip', acs_mod_name)\n",
    "    acs_mod_path = mfa_mod_path + path_sep + eval(acs_mod_name)\n",
    "output_path = commonVoice_dir + path_sep + language_dir + path_sep + 'output/'\n",
    "\n",
    "\n",
    "# Finale:\n",
    "txtgrds_path = osf_path + path_sep + 'textgrids' + path_sep + eval(textgrid_folder_name)[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the paths and directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\t\n",
      "The name of the language:\t\"Upper Sorbian\" (as in the XPF corpus)\n",
      "The Common Voice code of the language:\t\"hsb\"\n",
      "The version of the Common Voice data:\t\"16\"\n",
      "\n",
      "\n",
      "Step 1:\tRemapping speakers\n",
      "The script of remapping speakers:\tvxc_remap_spkrs.py\n",
      "Validated log:\t/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hsb_v16/validated.tsv\n",
      "Validated log with speakers remapped (to be created):\t/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hsb_v16/hsb_xpf_spkr16.tsv\n",
      "\n",
      "\n",
      "Step 2:\tCreating validated .wav and .TextGrid\n",
      "Praat:\t/Applications/Praat.app/Contents/MacOS/Praat\n",
      "The scipt of creating .wav and .TextGrid files for validated recs:\tvxc_createTextGridsWav.praat\n",
      "\n",
      "\n",
      "Step 3:\tPreparing the lexicon\n",
      "The directory of Common Voice recordings of the language:\t/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hsb_v16\n",
      "The folder where validated .wav/.TextGrid files will be saved:\t/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hsb_v16/validated\n",
      "Validated log with speakers remapped:\t/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hsb_v16/hsb_xpf_spkr16.tsv\n",
      "Wordlist file:\t/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hsb_v16/hsb_xpf_wordlist16.txt\n",
      "\n",
      "\n",
      "Step 4:\tG2P\n",
      "The XPF G2P script:\txpf_translate04.py\n",
      "The XPF rule file:\t/Users/miaozhang/Research/CorpusPhon/CorpusData/G2P/XPF/hsb_Upper Sorbian/hsb.rules\n",
      "The XPF verification file:\t/Users/miaozhang/Research/CorpusPhon/CorpusData/G2P/XPF/hsb_Upper Sorbian/hsb.verify.csv\n",
      "The lexicon file (to be created):\t/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hsb_v16/hsb_xpf_lexicon16.txt\n",
      "\n",
      "\n",
      "Step 5 and 6:\tMFA\n",
      "(Again) The validated recordings:\t/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hsb_v16/validated\n",
      "The pronunciation dictionary:\t/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hsb_v16/hsb_xpf_lexicon16.txt\n",
      "Where the acoustic model will be saved:\t/Users/miaozhang/Documents/MFA/pretrained_models/acoustic/hsb_xpf_acoustic16.zip\n",
      "Where to put the output of forced alignment:\t/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hsb_v16/output/\n"
     ]
    }
   ],
   "source": [
    "# Print all the file and folder paths\n",
    "print('Step 0:\\t')\n",
    "if if_xpf == 1:\n",
    "    print(f'The name of the language:\\t\"{lang_name}\" (as in the XPF corpus)')\n",
    "else:\n",
    "    print(f'The processing code of the language:\\t\"{epi_code}\" (as in Epitran)')\n",
    "print(f'The Common Voice code of the language:\\t\"{lang_code}\"')\n",
    "print(f'The version of the Common Voice data:\\t\"{cv_version}\"')\n",
    "print('\\n')\n",
    "\n",
    "print('Step 1:\\tRemapping speakers')\n",
    "print('The script of remapping speakers:\\t' + remap_spkr_path[0])\n",
    "print('Validated log:\\t' + remap_spkr_path[1])\n",
    "print('Validated log with speakers remapped (to be created):\\t' + remap_spkr_path[2])\n",
    "print('\\n')\n",
    "\n",
    "print('Step 2:\\tCreating validated .wav and .TextGrid')\n",
    "print('Praat:\\t' + praat_path)\n",
    "print('The scipt of creating .wav and .TextGrid files for validated recs:\\t' + create_txgdwav_script)\n",
    "print('\\n')\n",
    "\n",
    "print('Step 3:\\tPreparing the lexicon')\n",
    "print('The directory of Common Voice recordings of the language:\\t' + praat_args[0])\n",
    "print('The folder where validated .wav/.TextGrid files will be saved:\\t' + praat_args[1])\n",
    "print('Validated log with speakers remapped:\\t' + praat_args[2])\n",
    "print('Wordlist file:\\t' + wordlist_path)\n",
    "print('\\n')\n",
    "\n",
    "print('Step 4:\\tG2P')\n",
    "if if_xpf == 1:\n",
    "    print('The XPF G2P script:\\t' + xpf_translater_path)\n",
    "    print('The XPF rule file:\\t' + rule_file_path)\n",
    "    print('The XPF verification file:\\t' + verify_file_path)\n",
    "else:\n",
    "    print('The script to run epitran:\\t' + epitran_translater_path)\n",
    "\n",
    "print('The lexicon file (to be created):\\t' + dict_file_path)\n",
    "print('\\n')\n",
    "\n",
    "print('Step 5 and 6:\\tMFA')\n",
    "print('(Again) The validated recordings:\\t' + validated_recs_path) # the validated recordings\n",
    "print('The pronunciation dictionary:\\t' + dict_file_path) # the lexicon\n",
    "print('Where the acoustic model will be saved:\\t' + acs_mod_path) # where the acoustic model will be saved\n",
    "print('Where to put the output of forced alignment:\\t' + output_path) # where the outputs will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Remap the validated speakers\n",
    "Get speaker IDs to put on TextGrids for speaker adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'vxc_remap_spkrs.py', '/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hsb_v16/validated.tsv', '/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hsb_v16/hsb_xpf_spkr16.tsv'], returncode=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(commonVoice_dir + path_sep + language_dir + path_sep + eval(spkr_file_name)):\n",
    "    os.remove(commonVoice_dir + path_sep + language_dir + path_sep + eval(spkr_file_name))\n",
    "\n",
    "cmd_remap_spkr = ['python', remap_spkr_path[0], remap_spkr_path[1], remap_spkr_path[2]]\n",
    "subprocess.run(cmd_remap_spkr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Create TextGrid files and .wav files based on the .mp3 recordings from Common Voice\n",
    "Now we can create TextGrid files and .wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the clip durations\n",
    "clip_dur = pd.read_csv(commonVoice_dir + path_sep + language_dir + path_sep + 'clip_durations.tsv', sep = '\\t')\n",
    "clip_dur.rename(columns = {'clip':'path', 'duration[ms]':'dur'}, inplace=True)\n",
    "clip_dur.set_index('path', inplace = True)\n",
    "\n",
    "# Load the validated speaker data\n",
    "validSpkr = pd.read_csv(remap_spkr_path[2], sep = '\\t', usecols=['path', 'sentence', 'speaker_id', 'new_utt'])\n",
    "validSpkr.set_index('path', inplace = True)\n",
    "\n",
    "# Append duration info to validated speaker file\n",
    "validSpkr = pd.concat([validSpkr, clip_dur], axis = 1, join = 'inner')\n",
    "validSpkr['dur'] = validSpkr['dur']/1000\n",
    "validSpkr.reset_index(inplace=True)\n",
    "\n",
    "# The file names\n",
    "validSpkr['path'] = commonVoice_dir + path_sep + language_dir + path_sep + 'clips/' + validSpkr['path']\n",
    "validSpkr['new_utt'] = commonVoice_dir + path_sep + language_dir + path_sep + 'validated/' +validSpkr['new_utt'] + '.mp3'\n",
    "validSpkr['speaker_id'] = validSpkr.speaker_id.astype('string')\n",
    "\n",
    "# Get total file number\n",
    "n_clips = validSpkr.shape[0]\n",
    "\n",
    "# The path of the 'validated' folder to contain validated recordings from Common Voice. If there is already a folder with the same name, delete it\n",
    "if os.path.exists(commonVoice_dir + path_sep + language_dir + path_sep + 'validated'):\n",
    "    shutil.rmtree(commonVoice_dir + path_sep + language_dir + path_sep + 'validated')\n",
    "# Make the folder:\n",
    "os.makedirs(commonVoice_dir + path_sep + language_dir + path_sep + 'validated')\n",
    "\n",
    "# Create a function to copy the validated clips into validated folder:\n",
    "def copy_snd(src, output):\n",
    "    cmd_copy = ['cp', src, output]\n",
    "    subprocess.run(cmd_copy)\n",
    "\n",
    "# The function to create the textgrid files\n",
    "from praatio import textgrid\n",
    "def create_textgrid(snd_file, dur, speaker_id, transcript):\n",
    "    # Create the textgrid\n",
    "    tg = textgrid.Textgrid()\n",
    "    \n",
    "    # Add a new tier to the TextGrid\n",
    "    speaker_tier = textgrid.IntervalTier(speaker_id, # tier name\n",
    "                                        [(0.05, dur-0.05, transcript)], # interval starting, ending time, and the transcript\n",
    "                                        0, # start time\n",
    "                                        dur) # end time\n",
    "    tg.addTier(speaker_tier)\n",
    "\n",
    "    # Save the TextGrid to a file\n",
    "    tg_filename = snd_file.replace(\".mp3\", \".TextGrid\")\n",
    "    tg.save(tg_filename, format=\"short_textgrid\", includeBlankSpaces=True)\n",
    "\n",
    "    del tg, tg_filename, speaker_tier\n",
    "\n",
    "for mp3, wav, speaker, dur, transcript in zip(validSpkr.path, validSpkr.new_utt, validSpkr.speaker_id, validSpkr.dur, validSpkr.sentence):\n",
    "    # Crate wav files  \n",
    "    copy_snd(mp3, wav)\n",
    "    create_textgrid(wav, dur, speaker, transcript)\n",
    "\n",
    "del validSpkr, clip_dur, mp3, wav, speaker, dur, transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/Applications/Praat.app/Contents/MacOS/Praat', '--run', 'vxc_createTextGridsWav.praat', '/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/th_v16', '/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/th_v16/validated', '/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/th_v16/th_epi_spkr16.tsv'], returncode=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The path of the 'validated' folder to contain validated recordings from Common Voice. If there is already a folder with the same name, delete it\n",
    "if os.path.exists(commonVoice_dir + path_sep + language_dir + path_sep + 'validated'):\n",
    "    shutil.rmtree(commonVoice_dir + path_sep + language_dir + path_sep + 'validated')\n",
    "# Make the folder:\n",
    "os.makedirs(commonVoice_dir + path_sep + language_dir + path_sep + 'validated')\n",
    "\n",
    "# Run the praat script:\n",
    "subprocess.run([praat_path, '--run', create_txgdwav_script, praat_args[0], praat_args[1], praat_args[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Prepare the lexicon\n",
    "Extract transcripts from validated.tsv and get each word on its own line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/np/mck_44bj6097m3dfm7zhp4mm0000gn/T/ipykernel_47142/1178128765.py:8: FutureWarning: Possible nested set at position 1\n",
      "  sentence = re.sub(\"[[:punct:]]+\", \" \", sentence) # replace any remaining punctuations with spaces\n"
     ]
    }
   ],
   "source": [
    "# Read in the validated.tsv file and get the orthographical transcriptions of the utterances\n",
    "words_col = pd.read_csv(validated_log, sep='\\t')['sentence'] # get the transcribed sentences\n",
    "sentences = words_col.astype('string').tolist() # turn the transcription into a list of sentences\n",
    "\n",
    "sentences_processed = []\n",
    "for sentence in sentences:\n",
    "    sentence = re.sub(\"[,|.|!|?|\\\"|“|„|–|-|-|—|’|‘|«|»|;|”|؟|&| \\' ]+\", \" \", sentence) # remove a lot of non-word symbols\n",
    "    sentence = re.sub(\"[[:punct:]]+\", \" \", sentence) # replace any remaining punctuations with spaces\n",
    "    sentence = re.sub(\"[ ]+\", \" \", sentence) # replace multiple continuous white spaces with a single space\n",
    "    sentence = re.sub(\" \", \"\\n\", sentence) # replace space with new line\n",
    "    sentence = sentence.lower()\n",
    "    sentences_processed.append(sentence)\n",
    "\n",
    "\n",
    "words = \"\".join(sentences_processed).split(\"\\n\") # make a string of word tokens\n",
    "words = sorted(set(words)) # sort and get word types\n",
    "words = list(filter(None, words)) # remove empty strings\n",
    "\n",
    "# Save the word list as a .txt file\n",
    "if os.path.exists(wordlist_path):\n",
    "    os.remove(wordlist_path)\n",
    "    \n",
    "with open(wordlist_path,'w') as word_file:\n",
    "\tfor word in words:\n",
    "          word_file.write(word + \"\\n\")\n",
    "\n",
    "del words_col, words, sentences_processed, sentence, word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. G2P grapheme-to-phoneme (Epitran or XPF)\n",
    "There three files you need to proceed if you use XPF.\n",
    "1. A G2P rule file\n",
    "2. A veryfication file\n",
    "3. The translater python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dict_file_path):\n",
    "    os.remove(dict_file_path)\n",
    "    \n",
    "# Get the dictionary using XPF\n",
    "# -l specifies the rule file\n",
    "# -c specifies the verification file\n",
    "# -r specifies the file to be translated\n",
    "if g2p == 'xpf':\n",
    "    g2p_cmd = [\"python\", xpf_translater_path, \"-l\", rule_file_path, \"-c\", verify_file_path, \"-r\", wordlist_path] # XPF translating command that will be sent to subprocess.run() to execute.\n",
    "\n",
    "    if os.path.exists(dict_file_path):\n",
    "        os.remove(dict_file_path)\n",
    "\n",
    "    with open(dict_file_path,'w') as dict_file:\n",
    "        subprocess.run(g2p_cmd, stdout = dict_file) # stdout = ... means to send the output to the file (so you have to open this file first as above)\n",
    "\n",
    "    # This is to get rid of all the '@' in the lexicon (if there is any). @ means that XPF G2P failure\n",
    "    with open(dict_file_path, \"r\") as dict_file:\n",
    "        dict = dict_file.read().split(\"\\n\")\n",
    "\n",
    "    with open(dict_file_path, 'w') as dict_file:\n",
    "        for i in dict:\n",
    "            i = re.sub(\" ː\", \"ː\", i)\n",
    "            if '@' not in i:\n",
    "                dict_file.write(i + \"\\n\")\n",
    "# Or using Epitran\n",
    "else:\n",
    "    g2p_cmd = [\"python\", epitran_translater_path, wordlist_path, dict_file_path, epi_code]\n",
    "    subprocess.run(g2p_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Validate the corpus\n",
    "\n",
    "First, you need to activate the MFA environment in the terminal.\n",
    "1. Press ctrl+` to open Terminal in VS Code.\n",
    "2. Run 'conda activate aligner' until you see '(aligner)' at the beginning of the line in Terminal.\n",
    "3. When you finished using MFA (both training and aligning), run 'conda deactivate' to shut down the MFA environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder of MFA in document\n",
    "# You DON'T need to run this if you already have an MFA folder in your Documents folder (What would this be like on Windows?)\n",
    "# Uncomment the command below to run:\n",
    "#!mfa model download acostic english.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the corpus, run this line in terminal: \n",
    "\n",
    "        mfa validate {wherever your validated recordings are} {wherever your lexicon file is} --ignore_acoustics --clean\n",
    "\n",
    "You can copy the command lines from below.\n",
    "Notebook can't handle ```mfa``` commands. MFA commands can only run in Terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_validate = f'mfa validate {validated_recs_path} {dict_file_path} --ignore_acoustics --clean'\n",
    "print('To validate, copy:\\t' + cmd_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the oov words back into the word list and rerun G2P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The oov file:\n",
    "oov_file = 'oovs_found_' + eval(dict_file_name)\n",
    "\n",
    "oov_path = mfa_oov_path + path_sep + oov_file\n",
    "with open(oov_path, 'r') as oov_file:\n",
    "        with open(wordlist_path, 'a') as wordlist:\n",
    "            shutil.copyfileobj(oov_file, wordlist)\n",
    "\n",
    "# And then rerun Step 4. G2P to process the oov words.\n",
    "if g2p == 'xpf':\n",
    "    g2p_cmd = [\"python\", xpf_translater_path, \"-l\", rule_file_path, \"-c\", verify_file_path, \"-r\", wordlist_path]\n",
    "\n",
    "    if os.path.exists(dict_file_path):\n",
    "        os.remove(dict_file_path)\n",
    "\n",
    "    with open(dict_file_path,'w') as dict_file:\n",
    "        subprocess.run(g2p_cmd, stdout = dict_file) \n",
    "\n",
    "    with open(dict_file_path, \"r\") as dict_file:\n",
    "        dict = dict_file.read().split(\"\\n\")\n",
    "\n",
    "    with open(dict_file_path, 'w') as dict_file:\n",
    "        for i in dict:\n",
    "            i = re.sub(\" ː\", \"ː\", i)\n",
    "            if '@' not in i: \n",
    "                dict_file.write(i + \"\\n\")\n",
    "\n",
    "else:\n",
    "    g2p_cmd = [\"python\", epitran_translater_path, wordlist_path, dict_file_path, epi_code]\n",
    "    subprocess.run(g2p_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To revalidate the corpus, copy and paste the command below.\n",
    "print('To validate, copy:\\t' + cmd_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Train the acoustic model and forced align.\n",
    "\n",
    "### Step 6.1. Then to train the acoustic model, run the next line:\n",
    "\n",
    "        mfa train --clean {where your validated recordings are} {where your lexicon file is} {where your model will be saved}\n",
    "\n",
    "### Step 6.2. The final step: forced align the recordings:\n",
    "\n",
    "        mfa train --clean {where your validated recordings are} {where your lexicon file is} {where your output will be saved}\n",
    "\n",
    "You can copy the command lines from below.\n",
    "Notebook can't handle ```mfa``` commands. The mfa commands above can only run in Terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_train = f'mfa train --clean {validated_recs_path} {dict_file_path} {acs_mod_path}'\n",
    "cmd_align = f'mfa align --clean {validated_recs_path} {dict_file_path} {acs_mod_path} {output_path}'\n",
    "\n",
    "print('To train, copy: \\t' + cmd_train)\n",
    "print(\"\\n\")\n",
    "print('To align, copy: \\t' + cmd_align)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finale\n",
    "Move the output files (the speaker file, the lexicon, the acoustic model, and the aligned textgrids) to the OSF folder to be ready to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a zip file of the aligned textgrids\n",
    "shutil.make_archive(txtgrds_path, 'zip', output_path)\n",
    "\n",
    "# Move the acoustic model\n",
    "shutil.copy(acs_mod_path, osf_path + path_sep + 'acoustic_models' + path_sep)\n",
    "\n",
    "# Move the lexicon\n",
    "shutil.copy(dict_file_path, osf_path + path_sep + 'lexicons' + path_sep)\n",
    "\n",
    "# Move the speaker file\n",
    "shutil.copy(remap_spkr_path[2], osf_path + path_sep + 'spkr_files' + path_sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upadate the tracking info in `VoxCommunis_tracking.csv`. Make a copy of the tracking csv file in the folder `vxc_pipeline` on your computer before running the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_track = pd.read_csv(cv_tracking_file)\n",
    "cv_track = cv_track.astype('string')\n",
    "cv_track.loc[cv_track['code_cv'] == lang_code, 'spkr_file'] = eval(spkr_file_name)\n",
    "cv_track.loc[cv_track['code_cv'] == lang_code, 'lexicon'] = eval(dict_file_name)\n",
    "cv_track.loc[cv_track['code_cv'] == lang_code, 'acoustic_model'] = eval(acs_mod_name)\n",
    "cv_track.to_csv(cv_tracking_file, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
