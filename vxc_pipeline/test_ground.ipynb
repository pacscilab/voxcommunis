{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycantonese, re, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Korean sentences\n",
    "sentences = ['你好，我叫張淼。', '下午天氣好熱！', '今天係星期五。', '細仔悟性唔錯。', '呢個係一個好有趣嘅對話。']\n",
    "\n",
    "# Tokenize each sentence\n",
    "tokenized_sentences = [' '.join(pycantonese.segment(sentence)) for sentence in sentences]\n",
    "tokenized_sentences = [re.sub('[。|，|！|？]', '', item) for item in tokenized_sentences]\n",
    "words = ''.join(tokenized_sentences).split()\n",
    "\n",
    "# Print the list of tokenized sentences\n",
    "for i, tokens in enumerate(tokenized_sentences):\n",
    "    print(f\"Sentence {i+1} tokens: {tokens}\")\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"場\", \"圖\", \"到\", \"嘴\", \"仔\", \"光\", \"⻣\", \"ㄧ\", \"㗎\", \"㗎妹\", \"㩒\", \"㩒錢\", \"㩿\", \"㪐\", \"㪐㩿\", \"䁪\", \"䁪眼\", \"䒐䒏\", \"䟴腳\", \"䰧\", \"䱽\", \"一\"]\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from lingpy import ipa2tokens\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('charsiu/g2p_multilingual_byT5_tiny_16_layers_100')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/byt5-small')\n",
    "\n",
    "chr_words = ['<yue>: '+i for i in words]\n",
    "out = tokenizer(chr_words,padding=True,add_special_tokens=False,return_tensors='pt')\n",
    "\n",
    "preds = model.generate(**out,num_beams=1,max_length=50) # We do not find beam search helpful. Greedy decoding is enough. \n",
    "phones = tokenizer.batch_decode(preds.tolist(),skip_special_tokens=True)\n",
    "\n",
    "\n",
    "phones = [ipa2tokens(phone) for phone in phones]\n",
    "phones = [' '.join(phone) for phone in phones]\n",
    "phones = [re.sub(':', 'ː', phone) for phone in phones]\n",
    "\n",
    "    \n",
    "# Define the regular expression pattern to capture whitespace between 't' and 's' following IPA tone letters\n",
    "pattern_ts = re.compile(r'(^|[˥˦˧˨˩]\\s)t s')\n",
    "pattern_ng_syll = re.compile(r'(^|[˥˦˧˨˩]+\\s)ŋ(\\s[˥˦˧˨˩]+)')\n",
    "\n",
    "# Replace the whitespace between 't' and 's' with empty string\n",
    "phones = [re.sub(pattern_ts, r'\\1t͡s', phone) for phone in phones]\n",
    "phones = [re.sub(pattern_ng_syll, r'\\1ŋ̩\\2', phone) for phone in phones]\n",
    "\n",
    "pattern_tone_pos = re.compile(r'\\s([mnŋptk])\\s([˥˦˧˨˩]+)')\n",
    "#pattern_tone_strip = re.compile(r'\\s[˥˦˧˨˩]+(^|\\s)')\n",
    "\n",
    "phones_with_tone = [re.sub(pattern_tone_pos, r' \\2 \\1', phone) for phone in phones]\n",
    "phones_with_tone = [re.sub(r'(.)\\s([˥˦˧˨˩]+)', r'\\1\\2', phone) for phone in phones_with_tone]\n",
    "phones_no_tone = [re.sub(r'[˥˦˧˨˩]+', '', phone) for phone in phones]\n",
    "phones_no_tone = [re.sub(r'\\s+', ' ', phone) for phone in phones_no_tone]\n",
    "\n",
    "for word, phone in zip(words, phones_with_tone):\n",
    "    print(word + '\\t' + phone)\n",
    "print(\"\\n\")\n",
    "for word, phone in zip(words, phones_no_tone):\n",
    "    print(word + '\\t' + phone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and combine the content of both files\n",
    "combined_content = \"\"\n",
    "with open('/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/zh-HK_v17/zh-HK_chr_lexicon17.txt', 'r') as file1:\n",
    "    combined_content += file1.read()\n",
    "\n",
    "with open('/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/yue_v17/yue_chr_lexicon17.txt', 'r') as file2:\n",
    "    combined_content += file2.read()\n",
    "\n",
    "# Get unique values and sort them\n",
    "unique_sorted_content = '\\n'.join(sorted(set(combined_content.splitlines())))\n",
    "\n",
    "# Write the unique and sorted content to a new file\n",
    "with open('/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/zh-HK_v17/cantonese_chr_lexicon17.txt', 'w') as output_file:\n",
    "    output_file.write(unique_sorted_content)\n",
    "shutil.copy('/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/zh-HK_v17/cantonese_chr_lexicon17.txt', '/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/yue_v17/cantonese_chr_lexicon17.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
