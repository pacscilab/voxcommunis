{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VoxCommunis data processing pipeline\n",
    "\n",
    "This is a script of running MFA on recordings from Common Voice corpus. \n",
    "\n",
    "To run this pipeline, you need to download:\n",
    "\n",
    "1. Python modules: epitran, praatio, re, pandas, numpy, subprocess, shutil, os\n",
    "2. The data of XPF corpus\n",
    "\n",
    "The pipeline takes these steps to process data:\n",
    "\n",
    "1. [Step 0: Setups](#step-0-setups)\n",
    "2. [Step 1: Remap speakers](#step-1-remap-the-validated-speakers)\n",
    "3. [Step 2: Prepare the lexicon](#step-3-prepare-the-lexicon)\n",
    "4. [Step 3: G2P grapheme-to-phoneme](#step-4-g2p-grapheme-to-phoneme-epitran-or-xpf)\n",
    "5. [Step 4: Create TextGrid and .wav files](#step-2-create-textgrid-files-and-wav-files-based-on-the-mp3-recordings-from-common-voice)\n",
    "6. [Step 5: Validation](#step-5-train-the-acoustic-model)\n",
    "7. [Step 6: Run MFA](#step-6-train-the-acoustic-model-and-forced-align)\n",
    "8. [Finale](#finale)\n",
    "\n",
    "This script was created by Miao Zhang (miao.zhang@uzh.ch), 22.12.2023\n",
    "\n",
    "This script was modified by Miao Zhang, 07.02.2024 (Revalidation added)\n",
    "\n",
    "Modified on 16.02.2024: added automatic log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.1. Modules\n",
    "Import packages and setup file directories (for both the scripts and data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os, subprocess, shutil, re, multiprocessing, zipfile\n",
    "import pandas as pd\n",
    "# Turn Copy-On-Write on\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# For creating textgrids\n",
    "from praatio import textgrid\n",
    "\n",
    "# For move files concurrently\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Import Path\n",
    "from pathlib import Path\n",
    "\n",
    "# Import processing functions\n",
    "import vxc_processing as vxcproc\n",
    "import vxc_setup as vxcstp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to reload modules if changes are made.\n",
    "# Don't run this if you didn't make any changes to vxc_processing.py or vxc_processing_cjk.py.\n",
    "import importlib\n",
    "importlib.reload(vxcproc)\n",
    "importlib.reload(vxcstp)\n",
    "if is_cjk:\n",
    "    importlib.reload(vxccjkproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.2. Paths\n",
    "\n",
    "Set the paths and directories of data and scripts to use.\n",
    "\n",
    "_IMPORTANT_: the folder of the corpus data you downloaded from Common Voice should be named as: {lang_code}_v{version_number}.\n",
    "- For example: the folder for the 16th version of Divhehi should be named: dv_v16.\n",
    "- Another example: the folder for the 15th version of Upper Sorbian should be: hsb_v15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### Directories ################################################\n",
    "\n",
    "# This is the directory where your data downloaded from Common Voice should be saved. This is the root directory where data from each language should be saved in individual folders.\n",
    "common_voice_dir = '/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice' \n",
    "\n",
    "# To use XPF as the G2P, you will need to download the XPF data from: https://github.com/CohenPr-XPF/XPF/tree/master/Data.\n",
    "# Specify the directory where your XPF data is saved.\n",
    "xpf_dir = '/Users/miaozhang/Research/CorpusPhon/Scripts/XPF' \n",
    "\n",
    "######################### Language name/code and Common Voice version ##############################\n",
    "\n",
    "# The language code used in Common Voice:\n",
    "lang_code = 'rw' \n",
    "# If the language is Chinese/Japanese/Korean\n",
    "is_cjk = vxcstp.detect_cjk(lang_code)\n",
    "# Import functions from vxc_processing_cjk.py to process CJK texts.\n",
    "if is_cjk:\n",
    "    import vxc_processing_cjk as vxccjkproc\n",
    "\n",
    "# The version of the data in Common Voice. Use only numbers.\n",
    "cv_mod_version = '17' # which version of common voice corpus that the model is trained on?\n",
    "cv_align_version = '17' # which version of common voice corpus is forced-aligned?\n",
    "\n",
    "######################### G2P ######################################################################\n",
    "\n",
    "# Specify the G2P engine. Only these keywords are acceptable: \n",
    "g2p = 'epi'\n",
    "# 'xpf' for XPF\n",
    "# 'epi' for Epitran\n",
    "# 'chr' for Charsiu\n",
    "# 'mfa' for MFA\n",
    "# 'vxc' for self-difined lexicon\n",
    "\n",
    "# Get processing info\n",
    "lang_row = vxcstp.get_codes(lang_code)\n",
    "lang_cv_name = lang_row['name_cv']\n",
    "\n",
    "# Specify G2P details\n",
    "if g2p == 'epi':\n",
    "    # If you are using epitran, ...\n",
    "    # Refer to VoxCommunics_info.csv to get the processing code of the language in epitran\n",
    "    epi_code = 'kin-Latn'\n",
    "elif g2p == 'mfa':\n",
    "    # Specify the path of the MFA G2P model\n",
    "    mfa_g2p_path = ''\n",
    "elif g2p == 'xpf':\n",
    "    # If you are using XPF, get the name of the language in XPF corpus\n",
    "    lang_xpf_name = lang_row['name_xpf'].replace(' ', '')\n",
    "    code_xpf = lang_row['code_xpf']\n",
    "elif g2p == 'chr':\n",
    "    # If you are using Charsiu, get the processing code for the language in Charsiu.\n",
    "    code_chr = lang_row['code_chr']\n",
    "    chr_g2p_script = 'chr_g2p.py'\n",
    "\n",
    "\n",
    "# MFA paths\n",
    "# This is where the acoustic model will be saved after MFA training is done:\n",
    "mfa_mod_folder = '/Users/miaozhang/Documents/MFA/pretrained_models/acoustic'\n",
    "\n",
    "# This is where files that will be uploaded to the OSF repo will be saved after the processing is finished:\n",
    "osf_path = '/Users/miaozhang/Research/CorpusPhon/CorpusData/VoxCommunis_OSF'\n",
    "\n",
    "######################### What writing system is the language using? ###############################\n",
    "\n",
    "# Specify if the language uses Cyrillic letters\n",
    "if_cyrl = 0\n",
    "\n",
    "######################### Using existing model? ###############################\n",
    "\n",
    "# Are you using a pre-trained model or training your own model?\n",
    "# If training your own model, then set it to 0\n",
    "if_self_mod = 0\n",
    "# If you set it to 1, specify the path of the model in step 6 section below (in this code block)\n",
    "if if_self_mod == 1:\n",
    "    # Specify the path of the model\n",
    "    acs_mod_path = '/Users/miaozhang/Documents/MFA/pretrained_models/acoustic/korean_mfa.zip'\n",
    "\n",
    "######################### Using existing lexicon? ###############################\n",
    "\n",
    "# Do you have your own prepared lexicon?\n",
    "# If no, then set the value to 0\n",
    "if_self_lex = 0\n",
    "# If you set it to 1, specify the path of the lexicon in step 6 section below (in this code block)\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "# Specify if the subversion of a corpus is used. The default is 0\n",
    "if_subversion = 0 \n",
    "# If if_subversion == 1, what suffix you would use?:\n",
    "# Ignore this part, if you don't have a subversion of the corpus you are using.\n",
    "if if_subversion == 0:\n",
    "    subversion = '_' + 'sub3'\n",
    "\n",
    "################################################################################################### \n",
    "\n",
    "# Get paths\n",
    "language_dir, clip_info_path, validated_log, validated_recs_path = vxcstp.find_lang_dir(lang_code, cv_align_version, common_voice_dir)\n",
    "\n",
    "# Get file names.    \n",
    "naming_schema = pd.read_csv('vxc_naming_schema.csv', usecols = ['Python_code'])['Python_code'].tolist()\n",
    "naming_schema = [eval(name) for name in naming_schema]\n",
    "acs_mod_name = naming_schema[0]\n",
    "textgrid_folder_name = naming_schema[1]\n",
    "word_file_name = naming_schema[2]\n",
    "dict_file_name = naming_schema[3]\n",
    "spkr_file_name = naming_schema[4]\n",
    "# Get the paths\n",
    "textgrid_folder_path = os.path.join(language_dir, textgrid_folder_name)\n",
    "word_file_path = os.path.join(language_dir, word_file_name)\n",
    "dict_file_path = os.path.join(language_dir, dict_file_name)\n",
    "spkr_file_path = os.path.join(language_dir, spkr_file_name)\n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "# For step 3: G2P\n",
    "if g2p == 'xpf':\n",
    "    xpf_translater_path = 'xpf_translate04.py'\n",
    "    rule_file_path = os.path.join(xpf_dir, code_xpf + '_' + lang_xpf_name, code_xpf + '.rules')\n",
    "    if lang_code == 'ug':\n",
    "        verify_file_path = os.path.join(xpf_dir, code_xpf + '_' + lang_xpf_name, code_xpf + '-arabic.verify.csv')\n",
    "    else:\n",
    "        verify_file_path = os.path.join(xpf_dir, code_xpf + '_' + lang_xpf_name, code_xpf + '.verify.csv')\n",
    "elif g2p == 'epi':\n",
    "    epitran_translater_path = 'epi_run.py'\n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "# For step 5: running MFA\n",
    "# Validate the corpus\n",
    "if if_self_mod != 1:\n",
    "    if if_subversion == 0:\n",
    "        acs_mod_path = os.path.join(mfa_mod_folder, acs_mod_name)\n",
    "    else:\n",
    "        acs_mod_name = re.sub('.zip', subversion + '.zip', acs_mod_name)\n",
    "        acs_mod_path = os.path.join(mfa_mod_folder, acs_mod_name)\n",
    "output_path = os.path.join(language_dir, 'output')\n",
    "\n",
    "# If you are using your own model or lexicon:\n",
    "if if_self_lex == 1:\n",
    "    # Specify the path of the lexicon\n",
    "    dict_file_path = '/Users/miaozhang/Documents/MFA/pretrained_models/dictionary/french_mfa.dict'  \n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "vxcstp.show_files(language_dir, acs_mod_path, dict_file_path, spkr_file_path, textgrid_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Speaker remapping\n",
    "Get speaker IDs to put on TextGrids for speaker adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remap the speakers and save it to output the validated recordings to the speaker file\n",
    "if_output = True\n",
    "if not is_cjk:\n",
    "    # Process non-CJK\n",
    "    valid = vxcproc.remap_spkr(language_dir, spkr_file_path, lang_code, output=if_output)\n",
    "else:\n",
    "    # Process CJK\n",
    "    valid = vxccjkproc.remap_cjk_spkr(language_dir, spkr_file_path, lang_code, output=if_output)\n",
    "\n",
    "print(f'There are {len(valid)} validated recordings in total for {lang_cv_name}.', '\\n')\n",
    "print('Some sentence examples:\\n')\n",
    "\n",
    "if not is_cjk:\n",
    "    for line in valid['sentence'].sample(20).tolist():\n",
    "        print('\\t', line)\n",
    "else:\n",
    "    for line in valid['sentence_tok'].sample(20).tolist():\n",
    "        print('\\t', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Word list\n",
    "Generate the word list from Common Voice transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations and get the unique word types\n",
    "words = vxcproc.process_words(valid, lang_code)\n",
    "\n",
    "# Remove other unwanted words (E.g., Cyrl in Latin writnig, other alphabets in CJK languages, etc.)\n",
    "words = vxcproc.remove_unwanted_words(words, lang_code, is_cjk, if_cyrl)\n",
    "\n",
    "# Save the word list as a .txt file\n",
    "if os.path.exists(word_file_path):\n",
    "    os.remove(word_file_path)\n",
    "\n",
    "with open(word_file_path,'w') as word_file:\n",
    "    for word in words:\n",
    "        word_file.write(word + \"\\n\")\n",
    "\n",
    "print(\"Word list saved to:\", word_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. G2P\n",
    "There three files you need to proceed if you use XPF.\n",
    "1. A G2P rule file\n",
    "2. A veryfication file\n",
    "3. The translater python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dict_file_path):\n",
    "    os.remove(dict_file_path)\n",
    "   \n",
    "# XPF\n",
    "if g2p == 'xpf':\n",
    "    vxcproc.xpf_g2p(xpf_translater_path, rule_file_path, verify_file_path, word_file_path, dict_file_path)\n",
    "\n",
    "# Epitran\n",
    "elif g2p == 'epi':\n",
    "    if not is_cjk:\n",
    "        vxcproc.epi_g2p(words, epi_code, dict_file_path)\n",
    "    else:\n",
    "        vxccjkproc.epi_cjk_g2p(words, epi_code, dict_file_path)\n",
    "\n",
    "# Charsiu\n",
    "elif g2p == 'chr':\n",
    "    chr_args = [chr_g2p_script, word_file_path, code_chr, dict_file_path]\n",
    "    subprocess.run(['python'] + chr_args)\n",
    "\n",
    "# MFA\n",
    "elif g2p == 'mfa':\n",
    "    cmd_mfa_g2p = f'mfa g2p {word_file_path} {mfa_g2p_path} {dict_file_path}' \n",
    "    print('To g2p, copy and run:\\n\\t', cmd_mfa_g2p, '\\n')\n",
    "\n",
    "# Other source\n",
    "elif g2p == 'vxc':\n",
    "    if lang_code == 'zh-CN':\n",
    "        vxccjkproc.cmn_g2p(words, dict_file_path)\n",
    "    elif lang_code == 'nan-tw':\n",
    "        vxccjkproc.nan_g2p(words, dict_file_path)\n",
    "\n",
    "print(f'Check the lexicon file: {dict_file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. TextGrid files\n",
    "\n",
    "All validated clips that are longer than 1s and the sentence is not empty will be moved to a subfolder called 'validated'.\n",
    "\n",
    "The invalidated clips will stay in the 'clips' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the folder for validated clips\n",
    "if not os.path.exists(validated_recs_path):\n",
    "    os.makedirs(validated_recs_path)\n",
    "\n",
    "# Setup file chunks to batch processing clip moving and textgrid creating \n",
    "n_clips = len(valid)\n",
    "n_workers = 10\n",
    "chunksize = round(n_clips / n_workers)\n",
    "\n",
    "# Move the clips and create textgrid files:\n",
    "with ThreadPoolExecutor(n_workers) as exe:\n",
    "    for i in range(0, len(valid), chunksize):\n",
    "        chunk_data = valid.loc[i:(i+chunksize),]\n",
    "        if not is_cjk:\n",
    "            _ = exe.submit(vxcproc.move_and_create_tg, chunk_data)\n",
    "        else:\n",
    "            _ = exe.submit(vxccjkproc.move_and_create_cjk_tg, chunk_data)\n",
    "\n",
    "# Uncomment the next line if you want to delete the invalidated recordings\n",
    "shutil.rmtree(os.path.join(language_dir, \"clips\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. MFA\n",
    "\n",
    "### Step 5.1. Validating\n",
    "\n",
    "First, you need to activate the MFA environment in the terminal.\n",
    "1. Press ctrl+` to open Terminal in VS Code.\n",
    "2. Run 'conda activate aligner' until you see '(aligner)' at the beginning of the line in Terminal.\n",
    "3. When you finished using MFA (both training and aligning), run 'conda deactivate' to shut down the MFA environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder of MFA in document\n",
    "# You DON'T need to run this if you already have an MFA folder in your Documents folder\n",
    "# Uncomment the command below to run:\n",
    "#!mfa model download acostic english.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the corpus, run this line in terminal: \n",
    "\n",
    "        mfa validate {wherever your validated recordings are} {wherever your lexicon file is} --ignore_acoustics --clean\n",
    "\n",
    "You can copy the command lines from below.\n",
    "Notebook can't handle ```mfa``` commands. MFA commands can only run in Terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_validate = f'mfa validate {validated_recs_path} {dict_file_path} --ignore_acoustics --clean'\n",
    "print('To validate, copy:\\t\\n\\t' + cmd_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.2. Train the model\n",
    "\n",
    "        mfa train --clean {where your validated recordings are} {where your lexicon file is} {where your model will be saved}\n",
    "\n",
    "You can copy the command lines from below.\n",
    "Notebook can't handle ```mfa``` commands. The mfa commands above can only run in Terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your own model\n",
    "cmd_train = f'mfa train --clean {validated_recs_path} {dict_file_path} {acs_mod_path}'\n",
    "print('To train, copy:\\t\\n\\t' + cmd_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.3. Forced-alignment\n",
    "\n",
    "        mfa align --clean {where your validated recordings are} {where your lexicon file is} {where your acoustic model is} {where your output will be saved}\n",
    "        \n",
    "When the model is trained, align the corpus.\n",
    "\n",
    "However, since the MFA alignment somehow stops after generating 32609 textgrid files, we will split the corpus into n subfolders with each subfolder containing 32000 files.\n",
    "If the corpus has more than 32000 recordings, move the mp3 and textgrid files into subfolders.\n",
    "\n",
    "THen print out the MFA commands to align the data in (each subfolder of) the validated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### !!!SKIP THIS PART IF THERE ARE LESS THAN 32000 recordings IN THE FOLDER!!! ####\n",
    "\n",
    "# Get all mp3 files in the validated folder\n",
    "all_mp3 = [item for item in os.listdir(validated_recs_path) if os.path.splitext(item)[1] == '.mp3']\n",
    "n_clips = len(all_mp3)\n",
    "print(f\"There are {n_clips} clips in the validated folder.\")\n",
    "n_valid = len(valid)\n",
    "\n",
    "if n_clips > 32000:\n",
    "    # Create subfolders\n",
    "    subfolders = valid['subfolder'].unique()\n",
    "    for subfolder in subfolders:\n",
    "        subfolder_path = os.path.join(validated_recs_path, subfolder)\n",
    "        if not os.path.exists(subfolder_path):\n",
    "            os.makedirs(subfolder_path)\n",
    "\n",
    "    # Create the paths in the subfolders for each recording according to their grouping\n",
    "    splits = valid[valid['path'].isin(all_mp3)]\n",
    "    splits.to_csv(os.path.join(language_dir, 'all_splits.csv'), index = False)\n",
    "\n",
    "    # Move the files into subfolders using multithreads\n",
    "    n_workers = 10\n",
    "    chunksize = round(len(splits) / n_workers)\n",
    "    with ThreadPoolExecutor(n_workers) as exe:\n",
    "        for i in range(0, len(splits), chunksize):\n",
    "            chunk_data = splits.loc[i:(i+chunksize),]\n",
    "            _ = exe.submit(vxcproc.split_recs, chunk_data)\n",
    "    \n",
    "    # If there are still files left in the root directory, move them into their subfolders\n",
    "    rest_mp3 = [item for item in os.listdir(validated_recs_path) if os.path.splitext(item)[1] == '.mp3']\n",
    "    rest_move = valid[valid['path'].isin(rest_mp3)]\n",
    "    vxcproc.split_recs(rest_move)\n",
    "    del rest_move, rest_mp3\n",
    "    \n",
    "    # Check if there are still mp3 or textgrid files in the root directory\n",
    "    contains_subdir = any(\n",
    "        os.path.isfile(os.path.join(validated_recs_path, item)) and \n",
    "        (item.lower().endswith('.mp3') or item.lower().endswith('.textgrid')) \n",
    "        for item in os.listdir(validated_recs_path)\n",
    "        )\n",
    "    if contains_subdir:\n",
    "        print(\"The validated folder still contains mp3 or TextGrid files.\", \"\\n\")\n",
    "    else:\n",
    "        print(\"\\n\", \"All mp3 or TextGrid files are moved to subfolders.\", \"\\n\")\n",
    "\n",
    "    # Check if there are overlapping file names across the subfolders\n",
    "    overlap_dict = vxcproc.check_file_overlaps(validated_recs_path)\n",
    "    if len(overlap_dict) == 0:\n",
    "        print(\"\\n\", \"There are no overlapping file names across the subfolders.\", \"\\n\") \n",
    "    else:\n",
    "        print(overlap_dict)\n",
    "\n",
    "# Print the MFA commands for alignment\n",
    "all_items = os.listdir(validated_recs_path)\n",
    "all_items = [file for file in all_items if '.DS_Store' not in file]\n",
    "all_items.sort()\n",
    "any_file = any(os.path.isfile(os.path.join(validated_recs_path, item)) for item in all_items)\n",
    "if not any_file:\n",
    "    mfa_align_script_path = '/Users/miaozhang/Research/CorpusPhon/Scripts/vxc_pipeline/mfa_align.sh'\n",
    "    print(f'There are {len([os.path.isdir(os.path.join(validated_recs_path, item)) for item in all_items])} subfolders for {lang_cv_name}.', '\\n')\n",
    "    # Use a bash script to automatically align the data in all subfolders. Remember to activate the MFA virtual environment: conda activate aligner\n",
    "    # print(f'Copy and run this in the terminal to grant execution permission to the script:\\tchmod +x {mfa_align_script_path}', '\\n')\n",
    "    print(f'Copy and run this in the terminal to align the data in all subfolders:\\t')\n",
    "    print(f'\\tbash {mfa_align_script_path} {validated_recs_path} {dict_file_path} {acs_mod_path} {output_path}', '\\n')\n",
    "else:  \n",
    "    cmd_train = f'mfa align --clean {validated_recs_path} {dict_file_path} {acs_mod_path} {output_path}'\n",
    "    print('To align, copy:')\n",
    "    print('\\t' + cmd_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.4: Put back (optional)\n",
    "\n",
    "When the alignment is done, if splits were created for aligning the data, put the recordings back to one single folder.\n",
    "\n",
    "Then check if all files are put back to the validated folder's root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_valid > 32000:\n",
    "    # Put the files back to validated root folder\n",
    "    n_workers = 10\n",
    "    chunksize = round(len(valid) / n_workers)\n",
    "    with ThreadPoolExecutor(n_workers) as exe:\n",
    "        for i in range(0, len(valid), chunksize):\n",
    "            chunk_data = valid.loc[i:(i+chunksize),]\n",
    "            _ = exe.submit(vxcproc.merge_recs, chunk_data)\n",
    "\n",
    "    # Use os.scandir() for better performance\n",
    "    with os.scandir(validated_recs_path) as entries:\n",
    "        subfolders = [entry.name for entry in entries if entry.is_dir()]\n",
    "        subfolders.sort()\n",
    "\n",
    "    # Lists to store undeleted subfolders and files\n",
    "    undeleted_subfolders = []\n",
    "\n",
    "    # Batch deletion of empty subfolders\n",
    "    for subfolder in subfolders:\n",
    "        subfolder_path = os.path.join(validated_recs_path, subfolder)\n",
    "        with os.scandir(subfolder_path) as sub_entries:\n",
    "            if not any(entry.is_file() for entry in sub_entries):\n",
    "                # If the subfolder does not contain any files, delete it\n",
    "                shutil.rmtree(subfolder_path)\n",
    "                print(\"\\n\", f\"Subfolder '{subfolder}' deleted because it contains no files.\", \"\\n\")\n",
    "            else:\n",
    "                undeleted_subfolders.append(subfolder)\n",
    "\n",
    "    print(\"\\n\", \"Subfolders checked and processed.\", \"\\n\")\n",
    "\n",
    "    # List undeleted subfolders\n",
    "    if len(undeleted_subfolders) > 0:\n",
    "        print(\"Undeleted subfolders:\")\n",
    "        for subfolder in undeleted_subfolders:\n",
    "            print(subfolder)\n",
    "            # Move the files in the uncleared subfolder back to the validated folder if there is still any\n",
    "            vxcproc.move_files_to_root(validated_recs_path, os.path.join(validated_recs_path, subfolder))\n",
    "        \n",
    "    # Check if all the subfolders are deleted\n",
    "    with os.scandir(validated_recs_path) as entries:\n",
    "        contains_subdir = any(entry.is_dir() for entry in entries)\n",
    "        if contains_subdir:\n",
    "            print(\"\\nThe validated folder still contains subfolders.\")\n",
    "        else:\n",
    "            print(\"\\nThe validated folder does not contain any subfolders now.\")\n",
    "\n",
    "# Check if the output and input files match.\n",
    "with multiprocessing.Pool() as pool:\n",
    "    result = pool.apply(vxcproc.compare_inout, args=(output_path, validated_recs_path))\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, move the output files (the speaker file, the lexicon, the acoustic model, and the aligned textgrids) to the OSF folder to be ready to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip output textgrids\n",
    "txtgrds_path = os.path.join(osf_path, 'textgrids', textgrid_folder_name)\n",
    "from threading import Lock\n",
    "if os.path.exists(output_path):\n",
    "    # list all files to add to the zip\n",
    "    tgfiles = [os.path.join(output_path, filename) for filename in os.listdir(output_path) if '.DS_Store' not in filename]\n",
    "    # create lock for adding files to the zip\n",
    "    lock = Lock()\n",
    "    # open the zip file\n",
    "    with zipfile.ZipFile(txtgrds_path, 'w', compression=zipfile.ZIP_DEFLATED) as handle:\n",
    "        # create the thread pool\n",
    "        with ThreadPoolExecutor(10) as exe:\n",
    "            # add all files to the zip archive\n",
    "            _ = [exe.submit(vxcproc.add_file, lock, handle, tg, output_path) for tg in tgfiles]\n",
    "\n",
    "# Move the acoustic model\n",
    "shutil.copy(acs_mod_path, os.path.join(osf_path, 'acoustic_models'))\n",
    "\n",
    "# Move the lexicon\n",
    "shutil.copy(dict_file_path, os.path.join(osf_path, 'lexicons'))\n",
    "\n",
    "# Move the speaker file\n",
    "shutil.copy(spkr_file_path, os.path.join(osf_path, 'spkr_files'))\n",
    "\n",
    "# Check if the model was trained\n",
    "model_trained = os.path.exists(os.path.join(osf_path, 'acoustic_models', acs_mod_name))\n",
    "# Check if the textgrid archive was created\n",
    "aligned = os.path.exists(os.path.join(osf_path, 'textgrids', textgrid_folder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, upadate the tracking info in `VoxCommunis_Info.csv`. \n",
    "\n",
    "Make sure it is not in the lang_code_processing folder. Once updated, push the updated .csv to the GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste the name of the outputs into the tracking file\n",
    "cv_track = pd.read_csv('VoxCommunis_Info.csv')\n",
    "cv_track = cv_track.astype('string')\n",
    "if model_trained:\n",
    "    print(f'The acoustic model for {lang_cv_name} has been trained.')\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'acoustic_model'] = acs_mod_name\n",
    "else:\n",
    "    print(f'Couldn\\'t find the acoustic model.')\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'acoustic_model'] = ''\n",
    "\n",
    "if aligned:\n",
    "    print(f'The forced alignment for {lang_cv_name} has been created.')\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'textgrids'] = textgrid_folder_name\n",
    "else:\n",
    "    print(f'Couldn\\'t find the alignment.')\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'textgrids'] = ''\n",
    "\n",
    "cv_track.loc[cv_track['code_cv'] == lang_code, 'spkr_file'] = spkr_file_name\n",
    "cv_track.loc[cv_track['code_cv'] == lang_code, 'lexicon'] = dict_file_name\n",
    "\n",
    "# Update the tracking file\n",
    "cv_track.to_csv('VoxCommunis_Info.csv', index = False)\n",
    "\n",
    "# Sample some files to check the alignment\n",
    "# How many sentences you want to check?\n",
    "n_sentences_to_check = 20\n",
    "\n",
    "\n",
    "post_check = valid['path'].sample(n_sentences_to_check, random_state=42).tolist()\n",
    "snd_path = [os.path.join(language_dir, 'validated', snd) for snd in post_check]\n",
    "tg_path = [os.path.join(language_dir, 'output', Path(snd).with_suffix('.TextGrid')) for snd in post_check]\n",
    "post_check = pd.DataFrame(\n",
    "    {'sound': snd_path,\n",
    "    'textgrid': tg_path}\n",
    ")\n",
    "post_check.to_csv(os.path.join(language_dir, 'post_check.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
