{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VoxCommunis data processing pipeline\n",
    "\n",
    "This is a script of running MFA on recordings from Common Voice corpus.\n",
    "\n",
    "0. [Step 0: Setups](#step-0-setups)\n",
    "1. [Step 1: Remap speakers](#step-1-remap-the-validated-speakers)\n",
    "2. [Step 2: Create TextGrid and .wav files](#step-2-create-textgrid-files-and-wav-files-based-on-the-mp3-recordings-from-common-voice)\n",
    "3. [Step 3: Prepare the lexicon](#step-3-prepare-the-lexicon)\n",
    "4. [Step 4: G2P grapheme-to-phoneme](#step-4-g2p-grapheme-to-phoneme-epitran-or-xpf)\n",
    "5. [Step 5: Validation](#step-5-train-the-acoustic-model)\n",
    "6. [Step 6: Run MFA](#step-6-train-the-acoustic-model-and-forced-align)\n",
    "\n",
    "This script was created by Miao Zhang (miao.zhang@uzh.ch), 22.12.2023\n",
    "\n",
    "This script was modified by Miao Zhang, 07.02.2024 (Revalidation added)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Setups\n",
    "Import packages and setup file directories (for both the scripts and data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "# Import os and subprocess to run terminal commands, pandas and re to process the lexicon, and shutil to delete folders\n",
    "import os, subprocess, shutil, re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the paths and directories of data and scripts to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it to 0 if you use a Windows machine.\n",
    "if_mac = 1 \n",
    "\n",
    "# Specify the G2P engine. If 0, then epitran\n",
    "if_xpf = 1\n",
    "\n",
    "###################################### Directories #########################################\n",
    "\n",
    "# This is the directory where your data downloaded from Common Voice should be saved. This is the root directory where data from each language is saved in individual folders.\n",
    "# NO (BACK)SLASH at the end!!!\n",
    "commonVoice_dir = '/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice' \n",
    "\n",
    "# To use XPF as the G2P engine to process lexicon, you will need to download the XPF data from: https://github.com/CohenPr-XPF/XPF/tree/master/Data and save them on your computer.\n",
    "# Specify the directory where your XPF data is saved.\n",
    "# NO (BACK)SLASH at the end!!!\n",
    "xpf_dir = '/Users/miaozhang/Research/CorpusPhon/CorpusData/G2P/XPF' \n",
    "\n",
    "######################### Language name/code and Common Voice version ##############################\n",
    "\n",
    "# Language-related variable names\n",
    "lang_code = 'ab' # the Common Voice code of the language (unfortunately, Common Voice mixes the use of iso 639-3 and iso 639-1 codes (they use bcp47 code). This code is also used in XPF).\n",
    "\n",
    "# If you are using XPF, ...\n",
    "# Please refer to VoxCommunics_info.csv to get the name of the language in XPF\n",
    "# You can ignore this if you are not going to use XPF.\n",
    "lang_name = 'Abkhaz'\n",
    "\n",
    "# If you are using epitran, ...\n",
    "# Please refer to VoxCommunics_info.csv to get the processing code of the language in epitran\n",
    "epi_code = 'yor-Latn' \n",
    "\n",
    "# The version of the data in Common Voice\n",
    "# Only numbers!!!\n",
    "cv_version = '16' \n",
    "\n",
    "# Specify if the subversion of a corpus is used. The default is 0\n",
    "if_subversion = 0 \n",
    "# If if_subversion == 1, what suffix you would use?:\n",
    "subversion = '_' + 'sub3'\n",
    "\n",
    "##################################################################\n",
    "\n",
    "# This is where the acoustic model will be saved after MFA training is done:\n",
    "mfa_mod_path = '/Users/miaozhang/Documents/MFA/pretrained_models/acoustic'\n",
    "\n",
    "##################################################################\n",
    "\n",
    "# Get the naming schema. (Don't change this part)\n",
    "naming_schema = pd.read_csv('vxc_naming_schema.csv')\n",
    "acs_mod_name = naming_schema['Python_code'][0]\n",
    "spkr_file_name = naming_schema['Python_code'][4]\n",
    "word_file_name = naming_schema['Python_code'][2]\n",
    "dict_file_name = naming_schema['Python_code'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the paths and directories (You don't need to chang anything in this script from this point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if if_mac == 1:\n",
    "    path_sep = '/'\n",
    "    # this is the default directory where Praat is installed on a Mac.\n",
    "    praat_path = '/Applications/Praat.app/Contents/MacOS/Praat' \n",
    "else:\n",
    "    path_sep = '\\\\'\n",
    "    # the directory of Praat installed on Windows.\n",
    "    praat_path = 'C:\\Program Files\\Praat.exe' \n",
    "\n",
    "if if_xpf == 1:\n",
    "    g2p = 'xpf'\n",
    "else:\n",
    "    g2p = 'epi'\n",
    "\n",
    "language_dir = lang_code + '_v' + cv_version\n",
    "\n",
    "# For step 1: speaker remapping\n",
    "# Get the full paths\n",
    "remap_spkr_path = []\n",
    "remap_spkr_path.append('vxc_remap_spkrs.py') # where the scipt of speaker remapping is\n",
    "remap_spkr_path.append(commonVoice_dir + path_sep + language_dir + path_sep + 'validated.tsv') # where the validated utterance log of common voice is\n",
    "remap_spkr_path.append(commonVoice_dir + path_sep + language_dir + path_sep + eval(spkr_file_name)) # where the validated speaker log will be saved\n",
    "\n",
    "\n",
    "# For step 2: Create .wav and .TextGrid files\n",
    "# Get the path of the praat script\n",
    "create_txgdwav_script = 'vxc_createTextGridsWav.praat' # this is where the praat script was saved.\n",
    "\n",
    "# Set the arguments for the praat script\n",
    "praat_args = []\n",
    "praat_args.append(commonVoice_dir + path_sep + language_dir) #this is the directory of the language. NO path_sep!!!\n",
    "praat_args.append(commonVoice_dir + path_sep + language_dir + path_sep + 'validated') #this is the folder name of validated files. NO path_sep at the end!!!\n",
    "praat_args.append(commonVoice_dir + path_sep + language_dir + path_sep + eval(spkr_file_name)) #this is remapped speaker file\n",
    "\n",
    "# For step 3: prepare the lexicon and pronunciation dictionary\n",
    "# Remember the file is saved in this variable:\n",
    "validated_log = remap_spkr_path[1]\n",
    "wordlist_path = commonVoice_dir + path_sep + language_dir + path_sep + eval(word_file_name)\n",
    "\n",
    "\n",
    "# For step 4: G2P\n",
    "if if_xpf == 1:\n",
    "    xpf_translater_path = 'xpf_translate04.py'\n",
    "    rule_file_path = xpf_dir + path_sep + lang_code + '_' + lang_name + path_sep + lang_code + '.rules'\n",
    "    verify_file_path = xpf_dir + path_sep + lang_code + '_' + lang_name + path_sep + lang_code + '.verify.csv'\n",
    "else:\n",
    "    epitran_translater_path = 'epi_run.py'\n",
    "\n",
    "dict_file_path = commonVoice_dir + path_sep + language_dir + path_sep + eval(dict_file_name)\n",
    "\n",
    "\n",
    "# For step 5: running MFA\n",
    "# Validate the corpus\n",
    "validated_recs_path = praat_args[1] \n",
    "if if_subversion == 0:\n",
    "    acs_mod_path = mfa_mod_path + path_sep + eval(acs_mod_name)\n",
    "else:\n",
    "    acs_mod_name = re.sub('.zip', subversion + '.zip', acs_mod_name)\n",
    "    acs_mod_path = mfa_mod_path + path_sep + eval(acs_mod_name)\n",
    "output_path = commonVoice_dir + path_sep + language_dir + path_sep + 'output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the paths and directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the file and folder paths\n",
    "print('Step 0:\\t')\n",
    "print(f'The name of the language:\\t\"{lang_name}\" (as in the XPF corpus)')\n",
    "print(f'The Common Voice code of the language:\\t\"{lang_code}\"')\n",
    "print(f'The version of the Common Voice data:\\t\"{cv_version}\"')\n",
    "print('\\n')\n",
    "\n",
    "print('Step 1:\\tRemapping speakers')\n",
    "print('The script of remapping speakers:\\t' + remap_spkr_path[0])\n",
    "print('Validated log:\\t' + remap_spkr_path[1])\n",
    "print('Validated log with speakers remapped (to be created):\\t' + remap_spkr_path[2])\n",
    "print('\\n')\n",
    "\n",
    "print('Step 2:\\tCreating validated .wav and .TextGrid')\n",
    "print('Praat:\\t' + praat_path)\n",
    "print('The scipt of creating .wav and .TextGrid files for validated recs:\\t' + create_txgdwav_script)\n",
    "print('\\n')\n",
    "\n",
    "print('Step 3:\\tPreparing the lexicon')\n",
    "print('The directory of Common Voice recordings of the language:\\t' + praat_args[0])\n",
    "print('The folder where validated .wav/.TextGrid files will be saved:\\t' + praat_args[1])\n",
    "print('Validated log with speakers remapped:\\t' + praat_args[2])\n",
    "print('Wordlist file:\\t' + wordlist_path)\n",
    "print('\\n')\n",
    "\n",
    "print('Step 4:\\tG2P')\n",
    "if if_xpf == 1:\n",
    "    print('The XPF G2P script:\\t' + xpf_translater_path)\n",
    "    print('The XPF rule file:\\t' + rule_file_path)\n",
    "    print('The XPF verification file:\\t' + verify_file_path)\n",
    "else:\n",
    "    print('The script to run epitran:\\t' + epitran_translater_path)\n",
    "\n",
    "print('The lexicon file (to be created):\\t' + dict_file_path)\n",
    "print('\\n')\n",
    "\n",
    "print('Step 5:\\tMFA')\n",
    "print('(Again) The validated recordings:\\t' + validated_recs_path) # the validated recordings\n",
    "print('The pronunciation dictionary:\\t' + dict_file_path) # the lexicon\n",
    "print('Where the acoustic model will be saved:\\t' + acs_mod_path) # where the acoustic model will be saved\n",
    "print('Where to put the output of forced alignment:\\t' + output_path) # where the outputs will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Remap the validated speakers\n",
    "Get speaker IDs to put on TextGrids for speaker adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the next line to remap speakers\n",
    "if os.path.exists(commonVoice_dir + path_sep + language_dir + path_sep + 'validated_spkr.tsv'):\n",
    "    os.remove(commonVoice_dir + path_sep + language_dir + path_sep + 'validated_spkr.tsv')\n",
    "os.system(f'python {remap_spkr_path[0]} {remap_spkr_path[1]} {remap_spkr_path[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Create TextGrid files and .wav files based on the .mp3 recordings from Common Voice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create TextGrid files and .wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path of the 'validated' folder to contain validated recordings from Common Voice. If there is already a folder with the same name, delete it\n",
    "if os.path.exists(commonVoice_dir + path_sep + language_dir + path_sep + 'validated'):\n",
    "    shutil.rmtree(commonVoice_dir + path_sep + language_dir + path_sep + 'validated')\n",
    "# Make the folder:\n",
    "os.makedirs(commonVoice_dir + path_sep + language_dir + path_sep + 'validated')\n",
    "# Run the praat script:\n",
    "subprocess.run([praat_path, '--run', create_txgdwav_script, praat_args[0], praat_args[1], praat_args[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Prepare the lexicon\n",
    "Extract transcripts from validated.tsv and get each word on its own line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the validated.tsv file and get the orthographical transcriptions of the utterances\n",
    "words_col = pd.read_csv(validated_log, sep='\\t')['sentence'] # get the transcribed sentences\n",
    "sentences = words_col.astype('string').tolist() # turn the transcription into a list of sentences\n",
    "\n",
    "sentences_processed = []\n",
    "for sentence in sentences:\n",
    "    sentence = re.sub(\"[,|.|!|?|\\\"|“|„|–|-|’|‘|-]+\", \" \", sentence) # remove a lot of non-word symbols\n",
    "    sentence = re.sub(\"[[:punct:]]\", \" \", sentence) # replace any remaining punctuations with spaces\n",
    "    sentence = re.sub(\"[ ]+\", \" \", sentence) # replace multiple continuous white spaces with a single space\n",
    "    sentence = re.sub(\" \", \"\\n\", sentence) # replace space with new line\n",
    "    sentence = sentence.lower()\n",
    "    sentences_processed.append(sentence)\n",
    "\n",
    "\n",
    "words = \"\".join(sentences_processed).split(\"\\n\") # make a string of word tokens\n",
    "words = sorted(set(words)) # sort and get word types\n",
    "words = list(filter(None, words)) # remove empty strings\n",
    "print(words)\n",
    "\n",
    "# Save the word list as a .txt file\n",
    "if os.path.exists(wordlist_path):\n",
    "    os.remove(wordlist_path)\n",
    "    \n",
    "with open(wordlist_path,'w') as word_file:\n",
    "\tfor word in words:\n",
    "\t\tword_file.write(word + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. G2P grapheme-to-phoneme (epitran or XPF)\n",
    "There three files you need to proceed.\n",
    "1. A G2P rule file\n",
    "2. A veryfication file\n",
    "3. The translater python script\n",
    "Make sure you have downloaded the G2P rule files and the translate.py file from XPF corpus and know where they are saved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dictionary using XPF\n",
    "# -l specifies the rule file\n",
    "# -c specifies the verification file\n",
    "# -r specifies the file to be translated\n",
    "if g2p == 'xpf':\n",
    "    g2p_cmd = [\"python\", xpf_translater_path, \"-l\", rule_file_path, \"-c\", verify_file_path, \"-r\", wordlist_path] # XPF translating command that will be sent to subprocess.run() to execute.\n",
    "\n",
    "    if os.path.exists(dict_file_path):\n",
    "        os.remove(dict_file_path)\n",
    "\n",
    "    with open(dict_file_path,'w') as dict_file:\n",
    "        subprocess.run(g2p_cmd, stdout = dict_file) # stdout = ... means to send the output to the file (so you have to open this file first as above)\n",
    "\n",
    "    # This is to get rid of all the '@' in the lexicon (if there is any). @ means that XPF G2P failure\n",
    "    with open(dict_file_path, \"r\") as dict_file:\n",
    "        dict = dict_file.read().split(\"\\n\")\n",
    "\n",
    "    with open(dict_file_path, 'w') as dict_file:\n",
    "        for i in dict:\n",
    "            if '@' not in i:\n",
    "                dict_file.write(i + \"\\n\")\n",
    "# Or using Epitran\n",
    "else:\n",
    "    g2p_cmd = [\"python\", epitran_translater_path, wordlist_path, dict_file_path, epi_code]\n",
    "    subprocess.run(g2p_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Validate the corpus\n",
    "\n",
    "First, you need to activate the MFA environment in the terminal.\n",
    "1. Press ctrl+` to open Terminal in VS Code.\n",
    "2. Run 'conda activate aligner' until you see '(aligner)' at the beginning of the line in Terminal.\n",
    "3. When you finished using MFA (both training and aligning), run 'conda deactivate' to shut down the MFA environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder of MFA in document\n",
    "# You DON'T need to run this if you already have an MFA folder in your Documents folder (What would this be like on Windows?)\n",
    "!mfa model download acostic english.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Validate\n",
    "\n",
    "To validate the corpus, run this line in terminal: \n",
    "\n",
    "        mfa validate {wherever your validated recordings are} {wherever your lexicon file is} --ignore_acoustics --clean\n",
    "\n",
    "You can copy the command lines from below.\n",
    "Notebook can't handle ```mfa``` commands. The mfa commands above can only run in Terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_validate = f'mfa validate {validated_recs_path} {dict_file_path} --ignore_acoustics --clean'\n",
    "cmd_train = f'mfa train --clean {validated_recs_path} {dict_file_path} {acs_mod_path}'\n",
    "cmd_align = f'mfa align --clean {validated_recs_path} {dict_file_path} {acs_mod_path} {output_path}'\n",
    "print('To validate, copy:\\t' + cmd_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the oov words back into the word list and rerun G2P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The folder of the OOV word files:\n",
    "mfa_oov_path = '/Users/miaozhang/Documents/MFA/validated'\n",
    "# The oov file:\n",
    "oov_file = 'oovs_found_' + eval(dict_file_name)\n",
    "\n",
    "oov_path = mfa_oov_path + path_sep + oov_file\n",
    "with open(oov_path, 'r') as oov_file:\n",
    "        with open(wordlist_path, 'a') as wordlist:\n",
    "            shutil.copyfileobj(oov_file, wordlist)\n",
    "\n",
    "# And then rerun Step 4. G2P to process the oov words.\n",
    "if g2p == 'xpf':\n",
    "    g2p_cmd = [\"python\", xpf_translater_path, \"-l\", rule_file_path, \"-c\", verify_file_path, \"-r\", wordlist_path]\n",
    "\n",
    "    if os.path.exists(dict_file_path):\n",
    "        os.remove(dict_file_path)\n",
    "\n",
    "    with open(dict_file_path,'w') as dict_file:\n",
    "        subprocess.run(g2p_cmd, stdout = dict_file) \n",
    "\n",
    "    with open(dict_file_path, \"r\") as dict_file:\n",
    "        dict = dict_file.read().split(\"\\n\")\n",
    "\n",
    "    with open(dict_file_path, 'w') as dict_file:\n",
    "        for i in dict:\n",
    "            if '@' not in i: \n",
    "                dict_file.write(i + \"\\n\")\n",
    "\n",
    "else:\n",
    "    g2p_cmd = [\"python\", epitran_translater_path, wordlist_path, dict_file_path, epi_code]\n",
    "    subprocess.run(g2p_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To revalidate the corpus, copy and paste the command below.\n",
    "print('To validate, copy:\\t' + cmd_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Train the acoustic model and forced align.\n",
    "\n",
    "### Step 6.1. Then to train the acoustic model, run the next line:\n",
    "\n",
    "        mfa train --clean {where your validated recordings are} {where your lexicon file is} {where your model will be saved}\n",
    "\n",
    "### Step 6.2. The final step: forced align the recordings:\n",
    "\n",
    "        mfa train --clean {where your validated recordings are} {where your lexicon file is} {where your output will be saved}\n",
    "\n",
    "You can copy the command lines from below.\n",
    "Notebook can't handle ```mfa``` commands. The mfa commands above can only run in Terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('To train, copy: \\t' + cmd_train)\n",
    "print(\"\\n\")\n",
    "print('To align, copy: \\t' + cmd_align)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
