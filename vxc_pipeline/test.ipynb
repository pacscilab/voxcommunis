{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os, subprocess, shutil, re, csv, sys, importlib\n",
    "import pandas as pd\n",
    "# Turn Copy-On-Write on\n",
    "pd.options.mode.copy_on_write = True\n",
    "import numpy as np\n",
    "\n",
    "# For creating textgrids\n",
    "from praatio import textgrid\n",
    "\n",
    "# For move files concurrently\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Import Path\n",
    "from pathlib import Path\n",
    "\n",
    "# Import functions from cv_processing.py\n",
    "import vxc_processing as vxcproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the folder:\t /Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/ca_v17\n",
      "The acoustic model to be trained/used:\t /Users/miaozhang/Documents/MFA/pretrained_models/acoustic/ca_vxc_acoustic16.zip\n",
      "The lexicon to be generated/used:\t /Users/miaozhang/Documents/MFA/pretrained_models/acoustic/ca_lexicon-IPA.dict\n"
     ]
    }
   ],
   "source": [
    "###################################### Directories ################################################\n",
    "\n",
    "# This is the directory where your data downloaded from Common Voice should be saved. This is the root directory where data from each language should be saved in individual folders.\n",
    "# NO (BACK)SLASH at the end!!!\n",
    "commonVoice_dir = '/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice' \n",
    "\n",
    "# To use XPF as the G2P engine to process lexicon, you will need to download the XPF data from: https://github.com/CohenPr-XPF/XPF/tree/master/Data and save them on your computer.\n",
    "# Specify the directory where your XPF data is saved.\n",
    "# NO (BACK)SLASH at the end!!!\n",
    "xpf_dir = '/Users/miaozhang/Research/CorpusPhon/CorpusData/G2P/XPF' \n",
    "\n",
    "######################### Language name/code and Common Voice version ##############################\n",
    "\n",
    "# Language-related variable names\n",
    "# the Common Voice code of the language (unfortunately, Common Voice mixes the use of iso 639-3 and iso 639-1 codes (they use bcp47 code). This code is also used in XPF).\n",
    "# The code should match the code used in the name of the folder you downloaded from Common Voice.\n",
    "lang_code = 'ca' \n",
    "\n",
    "# The version of the data in Common Voice\n",
    "# Only numbers!!!\n",
    "cv_version = '17' \n",
    "\n",
    "######################### G2P ######################################################################\n",
    "\n",
    "# Specify the G2P engine. Only these keywords are acceptable: \n",
    "# 'xpf' for XPF\n",
    "# 'epi' for Epitran\n",
    "# 'chr' for Charsiu\n",
    "# 'mfa' for MFA\n",
    "# 'vxc' for self-difined lexicon\n",
    "g2p = 'vxc'\n",
    "\n",
    "######################### The delimiter ############################################################\n",
    "\n",
    "# Set it to 0 if you use a Windows machine.\n",
    "if_mac = 1 \n",
    "\n",
    "######################### What writing system is the language using? ###############################\n",
    "\n",
    "# Specify if the language is Chinese/Japanese/Korean\n",
    "if_cjk = 0\n",
    "\n",
    "######################### Using existing model? ###############################\n",
    "\n",
    "if_self_mod = 1\n",
    "\n",
    "######################### Using existing lexicon? ###############################\n",
    "\n",
    "if_self_lex = 1\n",
    "\n",
    "###################### G2P settings (XPF or Epitran) ################################################\n",
    "\n",
    "# This is where VxcCommunis_tracking.csv is (NO (BACK)SLASH at the end!!!):\n",
    "cv_tracking_file = 'VoxCommunis_Info.csv'\n",
    "\n",
    "# Get the G2P processing code for the language\n",
    "if g2p == 'xpf' or 'chr':\n",
    "    with open(cv_tracking_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        lang_row = [row for row in reader if row['code_cv'] == lang_code][0]\n",
    "    # If you are using XPF, get the name of the language in XPF corpus\n",
    "    if g2p == 'xpf':\n",
    "        lang_name = lang_row['name_xpf'].replace(' ', '')\n",
    "    # If you are using Charsiu, get the processing code for the language in Charsiu.\n",
    "    elif g2p == 'chr':\n",
    "        code_chr = lang_row['code_chr']\n",
    "\n",
    "if g2p == 'epi':\n",
    "    # If you are using epitran, ...\n",
    "    # Please refer to VoxCommunics_info.csv to get the processing code of the language in epitran\n",
    "    # !!!Do this manually, since depending on the type of the orthography, the epitran code can differ!!!\n",
    "    epi_code = 'ukr-Cyrl'\n",
    "\n",
    "\n",
    "# Specify if the subversion of a corpus is used. The default is 0\n",
    "if_subversion = 0 \n",
    "# If if_subversion == 1, what suffix you would use?:\n",
    "# Ignore this part, if you don't have a subversion of the corpus you are using.\n",
    "subversion = '_' + 'sub3'\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "if if_mac == 1:\n",
    "    path_sep = '/'\n",
    "    # this is the default directory where Praat is installed on a Mac.\n",
    "    #praat_path = '/Applications/Praat.app/Contents/MacOS/Praat' \n",
    "else:\n",
    "    path_sep = '\\\\'\n",
    "    # the directory of Praat installed on Windows.\n",
    "    #praat_path = 'C:\\Program Files\\Praat.exe' \n",
    "\n",
    "# The folder for the language\n",
    "language_dir = commonVoice_dir + path_sep + lang_code + '_v' + cv_version\n",
    "\n",
    "# The file that contains the duration of each clip:\n",
    "clip_info_path = language_dir + path_sep + 'clip_durations.tsv'\n",
    "\n",
    "# MFA paths\n",
    "# The folder of the OOV word files (NO (BACK)SLASH at the end!!!):\n",
    "mfa_oov_path = '/Users/miaozhang/Documents/MFA/validated'\n",
    "# This is where the acoustic model will be saved after MFA training is done (NO (BACK)SLASH at the end!!!):\n",
    "mfa_mod_folder = '/Users/miaozhang/Documents/MFA/pretrained_models/acoustic'\n",
    "\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "\n",
    "# This is where files that will be uploaded to the OSF repo will be saved after the processing is finished (NO (BACK)SLASH at the end!!!):\n",
    "osf_path = '/Users/miaozhang/Research/CorpusPhon/CorpusData/VoxCommunis_OSF'\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "\n",
    "# Get the naming schema.\n",
    "naming_schema = pd.read_csv('vxc_naming_schema.csv', usecols = ['Python_code'])['Python_code'].tolist()\n",
    "naming_schema = [eval(name) for name in naming_schema]\n",
    "acs_mod_name = naming_schema[0]\n",
    "textgrid_folder_name = naming_schema[1]\n",
    "word_file_name = naming_schema[2]\n",
    "dict_file_name = naming_schema[3]\n",
    "spkr_file_name = naming_schema[4]\n",
    "textgrid_folder_path = language_dir + path_sep + textgrid_folder_name\n",
    "word_file_path = language_dir + path_sep + word_file_name\n",
    "dict_file_path = language_dir + path_sep + dict_file_name\n",
    "spkr_file_path = language_dir + path_sep + spkr_file_name\n",
    "del naming_schema\n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "# For step 3: prepare the lexicon and pronunciation dictionary\n",
    "validated_log = language_dir + path_sep + 'validated.tsv'\n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "# For step 4: G2P\n",
    "if g2p == 'xpf':\n",
    "    xpf_translater_path = 'xpf_translate04.py'\n",
    "    rule_file_path = xpf_dir + path_sep + lang_code + '_' + lang_name + path_sep + lang_code + '.rules'\n",
    "    verify_file_path = xpf_dir + path_sep + lang_code + '_' + lang_name + path_sep + lang_code + '.verify.csv'\n",
    "elif g2p == 'epi':\n",
    "    epitran_translater_path = 'epi_run.py'\n",
    "elif g2p == 'chr':\n",
    "    from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "    chr_model = T5ForConditionalGeneration.from_pretrained('charsiu/g2p_multilingual_byT5_tiny_16_layers_100')\n",
    "    chr_tok = AutoTokenizer.from_pretrained('google/byt5-small')\n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "# For step 6: running MFA\n",
    "# Validate the corpus\n",
    "validated_recs_path = language_dir + path_sep + 'validated'\n",
    "if if_subversion == 0:\n",
    "    acs_mod_path = mfa_mod_folder + path_sep + acs_mod_name\n",
    "else:\n",
    "    acs_mod_name = re.sub('.zip', subversion + '.zip', acs_mod_name)\n",
    "    acs_mod_path = mfa_mod_folder + path_sep + acs_mod_name\n",
    "output_path = language_dir + path_sep + 'output'\n",
    "\n",
    "if if_self_mod == 1:\n",
    "    # Specify the path of the model\n",
    "    acs_mod_path = '/Users/miaozhang/Documents/MFA/pretrained_models/acoustic/ca_vxc_acoustic16.zip'\n",
    "if if_self_lex == 1:\n",
    "    # Specify the path of the lexicon\n",
    "    dict_file_path = '/Users/miaozhang/Documents/MFA/pretrained_models/acoustic/ca_lexicon-IPA.dict'   \n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "# Finale:\n",
    "txtgrds_path = osf_path + path_sep + 'textgrids' + path_sep + textgrid_folder_name[:-4]\n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "print(\"Processing the folder:\\t\", language_dir)\n",
    "print(\"The acoustic model to be trained/used:\\t\", acs_mod_path)\n",
    "print(\"The lexicon to be generated/used:\\t\", dict_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole = vxcproc.remap_spkr(language_dir, path_sep, spkr_file_path, lang_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the files and create the subfolders\n",
    "valid_clips = whole[whole['validation'] == 'validated']\n",
    "all_items = os.listdir(validated_recs_path)\n",
    "all_mp3 = [item for item in all_items if os.path.splitext(item)[1] == '.mp3']\n",
    "n_clips = len(all_mp3)\n",
    "if n_clips > 32000:\n",
    "    # Create the paths in the subfolders for each recording according to their grouping\n",
    "    splits = vxcproc.make_groups(validated_recs_path, valid_clips, 32000)\n",
    "    splits = splits[splits['path'].isin(all_mp3)]\n",
    "\n",
    "splits.to_csv(os.path.join(language_dir, 'all_splits.csv'), index = False)\n",
    "del all_items, all_mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_recs(df):\n",
    "    for src_snd, sub_snd in zip(df.new_path, df.sub_path):\n",
    "        src_tg = Path(src_snd).with_suffix('.TextGrid')\n",
    "        sub_tg = Path(sub_snd).with_suffix('.TextGrid')\n",
    "        try:\n",
    "            shutil.move(src_snd, sub_snd)\n",
    "            shutil.move(src_tg, sub_tg)\n",
    "        except Exception as e:\n",
    "            print(f\"File moving error: {e}\")\n",
    "\n",
    "def merge_recs(df):\n",
    "    for src_snd, sub_snd in zip(df.sub_path, df.new_path):\n",
    "        src_tg = Path(src_snd).with_suffix('.TextGrid')\n",
    "        sub_tg = Path(sub_snd).with_suffix('.TextGrid')\n",
    "        try:\n",
    "            shutil.move(src_snd, sub_snd)\n",
    "            shutil.move(src_tg, sub_tg)\n",
    "        except Exception as e:\n",
    "            print(f\"File moving error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
