{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycantonese, re, shutil, pkuseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file names\n",
    "file1 = \"/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/zh-HK_v17/zh-HK_epi_lexicon17.txt\"\n",
    "file2 = \"/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/yue_v17/yue_epi_lexicon17.txt\"\n",
    "output_file = \"/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/yue_v17/cantonese_epi_lexicon17.txt\"\n",
    "\n",
    "# Read and store lines from file1 and file2 without duplicates\n",
    "lines_seen = set()\n",
    "all_lines = []\n",
    "\n",
    "for infile in [file1, file2]:\n",
    "    with open(infile, \"r\") as file:\n",
    "        for line in file:\n",
    "            if line.strip() not in lines_seen:\n",
    "                lines_seen.add(line.strip())\n",
    "                all_lines.append(line)\n",
    "\n",
    "# Write unique and sorted lines to the output file\n",
    "with open(output_file, \"w\") as outfile:\n",
    "    sorted_lines = sorted(set(all_lines))\n",
    "    outfile.writelines(sorted_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Korean sentences\n",
    "sentences = ['你好，我叫張淼。', '下午天氣好熱！', '今天係星期五。', '細仔悟性唔錯。', '呢個係一個好有趣嘅對話。']\n",
    "\n",
    "# Tokenize each sentence\n",
    "tokenized_sentences = [' '.join(pycantonese.segment(sentence)) for sentence in sentences]\n",
    "tokenized_sentences = [re.sub('[。|，|！|？]', '', item) for item in tokenized_sentences]\n",
    "words = ''.join(tokenized_sentences).split()\n",
    "\n",
    "# Print the list of tokenized sentences\n",
    "for i, tokens in enumerate(tokenized_sentences):\n",
    "    print(f\"Sentence {i+1} tokens: {tokens}\")\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"場\", \"圖\", \"到\", \"嘴\", \"仔\", \"光\", \"⻣\", \"ㄧ\", \"㗎\", \"㗎妹\", \"㩒\", \"㩒錢\", \"㩿\", \"㪐\", \"㪐㩿\", \"䁪\", \"䁪眼\", \"䒐䒏\", \"䟴腳\", \"䰧\", \"䱽\", \"一\"]\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from lingpy import ipa2tokens\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('charsiu/g2p_multilingual_byT5_tiny_16_layers_100')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/byt5-small')\n",
    "\n",
    "chr_words = ['<yue>: '+i for i in words]\n",
    "out = tokenizer(chr_words,padding=True,add_special_tokens=False,return_tensors='pt')\n",
    "\n",
    "preds = model.generate(**out,num_beams=1,max_length=50) # We do not find beam search helpful. Greedy decoding is enough. \n",
    "phones = tokenizer.batch_decode(preds.tolist(),skip_special_tokens=True)\n",
    "\n",
    "\n",
    "phones = [ipa2tokens(phone) for phone in phones]\n",
    "phones = [' '.join(phone) for phone in phones]\n",
    "phones = [re.sub(':', 'ː', phone) for phone in phones]\n",
    "\n",
    "    \n",
    "# Define the regular expipasion pattern to capture whitespace between 't' and 's' following IPA tone letters\n",
    "pattern_ts = re.compile(r'(^|[˥˦˧˨˩]\\s)t s')\n",
    "pattern_ng_syll = re.compile(r'(^|[˥˦˧˨˩]+\\s)ŋ(\\s[˥˦˧˨˩]+)')\n",
    "\n",
    "# Replace the whitespace between 't' and 's' with empty string\n",
    "phones = [re.sub(pattern_ts, r'\\1t͡s', phone) for phone in phones]\n",
    "phones = [re.sub(pattern_ng_syll, r'\\1ŋ̩\\2', phone) for phone in phones]\n",
    "\n",
    "pattern_tone_pos = re.compile(r'\\s([mnŋptk])\\s([˥˦˧˨˩]+)')\n",
    "#pattern_tone_strip = re.compile(r'\\s[˥˦˧˨˩]+(^|\\s)')\n",
    "\n",
    "phones_with_tone = [re.sub(pattern_tone_pos, r' \\2 \\1', phone) for phone in phones]\n",
    "phones_with_tone = [re.sub(r'(.)\\s([˥˦˧˨˩]+)', r'\\1\\2', phone) for phone in phones_with_tone]\n",
    "phones_no_tone = [re.sub(r'[˥˦˧˨˩]+', '', phone) for phone in phones]\n",
    "phones_no_tone = [re.sub(r'\\s+', ' ', phone) for phone in phones_no_tone]\n",
    "\n",
    "for word, phone in zip(words, phones_with_tone):\n",
    "    print(word + '\\t' + phone)\n",
    "print(\"\\n\")\n",
    "for word, phone in zip(words, phones_no_tone):\n",
    "    print(word + '\\t' + phone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dict = '/Users/miaozhang/Documents/MFA/pretrained_models/dictionary/mandarin_china_mfa.dict'\n",
    "with open(dict, 'r') as file:\n",
    "    data = [re.sub('\\n', '', line) for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypinyin import pinyin, Style\n",
    "from pinyin_to_ipa import pinyin_to_ipa\n",
    "import jieba, re\n",
    "\n",
    "# Example\n",
    "chinese_text = [\"若男\", \"丹阳\", \"大娘\", \"看我\", \"卡诺\", \"南阳\", \"那年\", \"瓜分\", \"挖掘\", \"募捐\", \"坏账\", \"连绵\", \"梳妆\", \"床照\", \"银行\", \"打扮\", \"牵过\", \"长江\", \"全国\", \"队长\", \"葵花\", \"大二\", \"八亿\", \"蛇口\", \"是非\", \"明显\", \"勉强\"]\n",
    "#tokenized = ' '.join(jieba.cut(chinese_text))\n",
    "#tokenized = tokenized.split(' ')\n",
    "#print(tokenized)\n",
    "\n",
    "g2p = []\n",
    "# Convert to Pinyin with tone numbers\n",
    "def cmn_g2p(chinese_text):\n",
    "    py = pinyin(chinese_text, style=Style.TONE3)\n",
    "    # Flatten the list and join words with spaces\n",
    "    py = [item[0] for item in py]\n",
    "    # Get each syllable\n",
    "    ipa = [pinyin_to_ipa(item)[0] for item in py]\n",
    "    ipa = [' '.join(sound) for sound in ipa]\n",
    "    ipa = [re.sub('[˥˦˧˨˩]', '', sound) for sound in ipa]\n",
    "    # Make onglides superscript and attach them to the following vowel\n",
    "    ipa = [re.sub(r'(p|m|f|t|n|l|k|x|s|ʂ|ɻ|ʰ) w ', r'\\1 ʷ', syll) for syll in ipa]\n",
    "    ipa = [re.sub(r'(p|t|m|n|l|ɕ|ʰ) j ', r'\\1 ʲ', syll) for syll in ipa]\n",
    "    ipa = [re.sub(r'(n|l|ɕ|ʰ) ɥ ', r'\\1 ᶣ', syll) for syll in ipa]\n",
    "    ipa = [re.sub('a ŋ', 'ɑ ŋ', syll) for syll in ipa]\n",
    "    ipa = ' '.join(ipa)\n",
    "    \n",
    "    transcript = chinese_text + '\\t' + ipa\n",
    "    \n",
    "    return transcript\n",
    "\n",
    "for word in chinese_text:\n",
    "    transcript = cmn_g2p(word)\n",
    "    g2p.append(transcript)\n",
    "\n",
    "for item in g2p:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jyutping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jyutping import Jyutping\n",
    "from pycantonese import YALE_TO_IPA\n",
    "\n",
    "def convert_to_jyutping(text):\n",
    "    converter = Jyutping()\n",
    "    jyutping_output = converter.get(text)\n",
    "    return jyutping_output\n",
    "\n",
    "def convert_jyutping_to_ipa(jyutping_text):\n",
    "    yale_to_ipa = YALE_TO_IPA\n",
    "    ipa_symbols = []\n",
    "\n",
    "    for syllable in jyutping_text.split():\n",
    "        ipa_symbols.extend(yale_to_ipa[syllable] if syllable in yale_to_ipa else [syllable])\n",
    "\n",
    "    ipa_output = \" \".join(ipa_symbols)\n",
    "    return ipa_output\n",
    "\n",
    "# Input text in Cantonese\n",
    "input_text = \"你好\"\n",
    "\n",
    "# Convert characters to Jyutping\n",
    "jyutping_result = convert_to_jyutping(input_text)\n",
    "\n",
    "# Convert Jyutping to IPA\n",
    "ipa_result = convert_jyutping_to_ipa(jyutping_result)\n",
    "\n",
    "print(f\"Input Text: {input_text}\")\n",
    "print(f\"Jyutping: {jyutping_result}\")\n",
    "print(f\"IPA: {ipa_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice'\n",
    "script = '/Users/miaozhang/Research/CorpusPhon/Scripts/vxc_pipeline/delete_invalid_clips.sh'\n",
    "print(f'bash {script} {folder}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
