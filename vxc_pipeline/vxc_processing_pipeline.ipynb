{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VoxCommunis data processing pipeline\n",
    "\n",
    "This is a script of running MFA on recordings from Common Voice corpus. \n",
    "\n",
    "To run this pipeline, you need to download:\n",
    "\n",
    "1. Python modules: epitran, praatio, re, pandas, numpy, subprocess, shutil, os\n",
    "2. The data of XPF corpus\n",
    "\n",
    "The pipeline takes these steps to process data:\n",
    "\n",
    "1. [Step 0: Setups](#step-0-setups)\n",
    "2. [Step 1: Remap speakers](#step-1-remap-the-validated-speakers)\n",
    "3. [Step 2: Create TextGrid and .wav files](#step-2-create-textgrid-files-and-wav-files-based-on-the-mp3-recordings-from-common-voice)\n",
    "4. [Step 3: Prepare the lexicon](#step-3-prepare-the-lexicon)\n",
    "5. [Step 4: G2P grapheme-to-phoneme](#step-4-g2p-grapheme-to-phoneme-epitran-or-xpf)\n",
    "6. [Step 5: Validation](#step-5-train-the-acoustic-model)\n",
    "7. [Step 6: Run MFA](#step-6-train-the-acoustic-model-and-forced-align)\n",
    "8. [Finale](#finale)\n",
    "\n",
    "This script was created by Miao Zhang (miao.zhang@uzh.ch), 22.12.2023\n",
    "\n",
    "This script was modified by Miao Zhang, 07.02.2024 (Revalidation added)\n",
    "\n",
    "Modified on 16.02.2024: added automatic log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Setups\n",
    "Import packages and setup file directories (for both the scripts and data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os, subprocess, shutil, re, csv, sys, importlib, multiprocessing, zipfile\n",
    "import pandas as pd\n",
    "# Turn Copy-On-Write on\n",
    "pd.options.mode.copy_on_write = True\n",
    "import numpy as np\n",
    "\n",
    "# For creating textgrids\n",
    "from praatio import textgrid\n",
    "\n",
    "# For move files concurrently\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Import Path\n",
    "from pathlib import Path\n",
    "\n",
    "# Import Lock to zip output textgrids\n",
    "from threading import Lock\n",
    "\n",
    "# Import functions from cv_processing.py\n",
    "import vxc_processing as vxcproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'vxc_processing' from '/Users/miaozhang/Research/CorpusPhon/Scripts/vxc_pipeline/vxc_processing.py'>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload vxcproc in case if there are any changes to the code\n",
    "importlib.reload(vxcproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.1 Path setup\n",
    "\n",
    "Set the paths and directories of data and scripts to use.\n",
    "\n",
    "_IMPORTANT_: the folder of the corpus data you downloaded from Common Voice should be named as: {lang_code}_v{version_number}.\n",
    "- For example: the folder for the 16th version of Divhehi should be named: dv_v16.\n",
    "- Another example: the folder for the 15th version of Upper Sorbian should be: hsb_v15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the folder:\t /Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hu_v17\n",
      "The acoustic model to be trained/used:\t /Users/miaozhang/Documents/MFA/pretrained_models/acoustic/hu_xpf_acoustic17.zip\n",
      "The lexicon to be generated/used:\t /Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hu_v17/hu_xpf_lexicon17.txt\n",
      "The speaker file to be generated:\t /Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hu_v17/hu_xpf_spkr17.tsv\n",
      "The textgrid files to be generated:\t /Users/miaozhang/Research/CorpusPhon/CorpusData/VoxCommunis_OSF/textgrids/hu_xpf_textgrids17_acoustic17.zip\n"
     ]
    }
   ],
   "source": [
    "###################################### Directories ################################################\n",
    "\n",
    "# This is the directory where your data downloaded from Common Voice should be saved. This is the root directory where data from each language should be saved in individual folders.\n",
    "# NO (BACK)SLASH at the end!!!\n",
    "commonVoice_dir = '/Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice' \n",
    "# For Eleanor\n",
    "#commonVoice_dir = '/Users/eleanor/Documents/CommonVoice' \n",
    "\n",
    "# To use XPF as the G2P engine to process lexicon, you will need to download the XPF data from: https://github.com/CohenPr-XPF/XPF/tree/master/Data and save them on your computer.\n",
    "# Specify the directory where your XPF data is saved.\n",
    "# NO (BACK)SLASH at the end!!!\n",
    "xpf_dir = '/Users/miaozhang/Research/CorpusPhon/CorpusData/G2P/XPF' \n",
    "# For Eleanor\n",
    "#xpf_dir = '/Users/eleanorchodroff/Documents/CorpusData/G2P/XPF'\n",
    "\n",
    "######################### Language name/code and Common Voice version ##############################\n",
    "\n",
    "# Language-related variable names\n",
    "# the Common Voice code of the language (unfortunately, Common Voice mixes the use of iso 639-3 and iso 639-1 codes (they use bcp47 code). This code is also used in XPF).\n",
    "# The code should match the code used in the name of the folder you downloaded from Common Voice.\n",
    "lang_code = 'hu' \n",
    "\n",
    "# The version of the data in Common Voice\n",
    "# Only numbers!!!\n",
    "cv_mod_version = '17' # which version of common voice corpus that the model is trained on?\n",
    "cv_align_version = '17' # which version of common voice corpus is forced-aligned?\n",
    "\n",
    "######################### G2P ######################################################################\n",
    "\n",
    "# Specify the G2P engine. Only these keywords are acceptable: \n",
    "# 'xpf' for XPF\n",
    "# 'epi' for Epitran\n",
    "# 'chr' for Charsiu\n",
    "# 'mfa' for MFA\n",
    "# 'vxc' for self-difined lexicon\n",
    "g2p = 'xpf'\n",
    "\n",
    "######################### What writing system is the language using? ###############################\n",
    "\n",
    "# Specify if the language is Chinese/Japanese/Korean. If not, 1\n",
    "if_cjk = 0\n",
    "\n",
    "######################### Using existing model? ###############################\n",
    "\n",
    "# Are you using a pre-trained model or training your own model?\n",
    "# If training your own model, then set it to 0\n",
    "if_self_mod = 0\n",
    "\n",
    "######################### Using existing lexicon? ###############################\n",
    "\n",
    "# Do you have your own prepared lexicon?\n",
    "# If no, then set the value to 0\n",
    "if_self_lex = 0\n",
    "\n",
    "######################### G2P settings ################################################\n",
    "\n",
    "# This is where VxcCommunis_tracking.csv is (NO (BACK)SLASH at the end!!!):\n",
    "cv_tracking_file = 'VoxCommunis_Info.csv'\n",
    "with open(cv_tracking_file, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    lang_row = [row for row in reader if row['code_cv'] == lang_code][0]\n",
    "    lang_name = lang_row['name_xpf'].replace(' ', '')\n",
    "\n",
    "# Get the G2P processing code for the language\n",
    "if g2p == 'xpf' or 'chr':\n",
    "    # If you are using XPF, get the name of the language in XPF corpus\n",
    "    if g2p == 'xpf':\n",
    "        code_xpf = lang_row['code_xpf']\n",
    "    # If you are using Charsiu, get the processing code for the language in Charsiu.\n",
    "    elif g2p == 'chr':\n",
    "        code_chr = lang_row['code_chr']\n",
    "\n",
    "if g2p == 'epi':\n",
    "    # If you are using epitran, ...\n",
    "    # Please refer to VoxCommunics_info.csv to get the processing code of the language in epitran\n",
    "    # !!!Do this manually, since depending on the type of the orthography, the epitran code can differ!!!\n",
    "    epi_code = 'ron-Latn'\n",
    "\n",
    "\n",
    "# Specify if the subversion of a corpus is used. The default is 0\n",
    "if_subversion = 0 \n",
    "# If if_subversion == 1, what suffix you would use?:\n",
    "# Ignore this part, if you don't have a subversion of the corpus you are using.\n",
    "subversion = '_' + 'sub3'\n",
    "\n",
    "################################################################################################### \n",
    "\n",
    "# The folder for the language\n",
    "language_dir = os.path.join(commonVoice_dir, lang_code + '_v' + cv_align_version)\n",
    "\n",
    "# The file that contains the duration of each clip:\n",
    "clip_info_path = os.path.join(language_dir, 'clip_durations.tsv')\n",
    "\n",
    "# MFA paths\n",
    "# This is where the acoustic model will be saved after MFA training is done (NO (BACK)SLASH at the end!!!):\n",
    "mfa_mod_folder = '/Users/miaozhang/Documents/MFA/pretrained_models/acoustic'\n",
    "\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "\n",
    "# This is where files that will be uploaded to the OSF repo will be saved after the processing is finished (NO (BACK)SLASH at the end!!!):\n",
    "osf_path = '/Users/miaozhang/Research/CorpusPhon/CorpusData/VoxCommunis_OSF'\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "\n",
    "# Get the naming schema.\n",
    "naming_schema = pd.read_csv('vxc_naming_schema.csv', usecols = ['Python_code'])['Python_code'].tolist()\n",
    "naming_schema = [eval(name) for name in naming_schema]\n",
    "\n",
    "# Get the names\n",
    "acs_mod_name = naming_schema[0]\n",
    "textgrid_folder_name = naming_schema[1]\n",
    "word_file_name = naming_schema[2]\n",
    "dict_file_name = naming_schema[3]\n",
    "spkr_file_name = naming_schema[4]\n",
    "\n",
    "# Get the paths\n",
    "textgrid_folder_path = os.path.join(language_dir, textgrid_folder_name)\n",
    "word_file_path = os.path.join(language_dir, word_file_name)\n",
    "dict_file_path = os.path.join(language_dir, dict_file_name)\n",
    "spkr_file_path = os.path.join(language_dir, spkr_file_name)\n",
    "del naming_schema\n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "# For step 3: prepare the lexicon and pronunciation dictionary\n",
    "validated_log = os.path.join(language_dir, 'validated.tsv')\n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "# For step 4: G2P\n",
    "if g2p == 'xpf':\n",
    "    xpf_translater_path = 'xpf_translate04.py'\n",
    "    rule_file_path = os.path.join(xpf_dir, code_xpf + '_' + lang_name, code_xpf + '.rules')\n",
    "    verify_file_path = os.path.join(xpf_dir, code_xpf + '_' + lang_name, code_xpf + '.verify.csv')\n",
    "elif g2p == 'epi':\n",
    "    epitran_translater_path = 'epi_run.py'\n",
    "elif g2p == 'chr':\n",
    "    from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "    chr_model = T5ForConditionalGeneration.from_pretrained('charsiu/g2p_multilingual_byT5_tiny_16_layers_100')\n",
    "    chr_tok = AutoTokenizer.from_pretrained('google/byt5-small')\n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "# For step 6: running MFA\n",
    "# Validate the corpus\n",
    "validated_recs_path = os.path.join(language_dir, 'validated')\n",
    "if if_subversion == 0:\n",
    "    acs_mod_path = os.path.join(mfa_mod_folder, acs_mod_name)\n",
    "else:\n",
    "    acs_mod_name = re.sub('.zip', subversion + '.zip', acs_mod_name)\n",
    "    acs_mod_path = os.path.join(mfa_mod_folder, acs_mod_name)\n",
    "output_path = os.path.join(language_dir, 'output')\n",
    "\n",
    "if if_self_mod == 1:\n",
    "    # Specify the path of the model\n",
    "    acs_mod_path = '/Users/miaozhang/Documents/MFA/pretrained_models/acoustic/ca_vxc_acoustic16.zip'\n",
    "if if_self_lex == 1:\n",
    "    # Specify the path of the lexicon\n",
    "    dict_file_path = os.path.join(language_dir, 'ca_lexicon-IPA.txt')   \n",
    "\n",
    "mfa_align_script_path = '/Users/miaozhang/Research/CorpusPhon/Scripts/vxc_pipeline/mfa_align.sh'\n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "# Finale:\n",
    "txtgrds_path = os.path.join(osf_path, 'textgrids', textgrid_folder_name)\n",
    "\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "print(\"Processing the folder:\\t\", language_dir)\n",
    "print(\"The acoustic model to be trained/used:\\t\", acs_mod_path)\n",
    "print(\"The lexicon to be generated/used:\\t\", dict_file_path)\n",
    "print(\"The speaker file to be generated:\\t\", spkr_file_path)\n",
    "print(\"The textgrid files to be generated:\\t\", txtgrds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Speaker remapping\n",
    "Get speaker IDs to put on TextGrids for speaker adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1524 validated recordings in total for Albanian.\n"
     ]
    }
   ],
   "source": [
    "# Remap the speakers and save it to output the validated recordings to the speaker file\n",
    "valid = vxcproc.remap_spkr(language_dir, spkr_file_path, lang_code, output=True)\n",
    "print(f'There are {len(valid)} validated recordings in total for {lang_name}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. TextGrid files\n",
    "\n",
    "All validated clips that are longer than 1s will be moved to a subfolder called 'validated'.\n",
    "\n",
    "The validated clips but are shorter than 1s will be moved to the 'other' folder.\n",
    "\n",
    "The invalidated clips will stay in the 'clips' folder. When the moving is done the 'clips' folder will be renamed to 'invalidated'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the folder for validated clips\n",
    "os.makedirs(validated_recs_path, exist_ok=True)\n",
    "\n",
    "# Setup file chunks to batch processing clip moving and textgrid creating \n",
    "n_clips = len(valid)\n",
    "n_workers = 10\n",
    "chunksize = round(n_clips / n_workers)\n",
    "\n",
    "# Move the clips and create textgrid files:\n",
    "with ThreadPoolExecutor(n_workers) as exe:\n",
    "    for i in range(0, len(valid), chunksize):\n",
    "        chunk_data = valid.loc[i:(i+chunksize),]\n",
    "        _ = exe.submit(vxcproc.move_and_create_tg, chunk_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Word list\n",
    "Generate the word list from Common Voice transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "if lang_code == 'ja':\n",
    "    words = vxcproc.process_words(spkr_file_path, lang_code)\n",
    "else:\n",
    "    words = vxcproc.process_words(validated_log, lang_code)\n",
    "\n",
    "# Filter other out unwanted words\n",
    "words = vxcproc.remove_unwanted_words(words, lang_code, if_cjk)\n",
    "\n",
    "# Save the word list as a .txt file\n",
    "if os.path.exists(word_file_path):\n",
    "    os.remove(word_file_path)\n",
    "    \n",
    "with open(word_file_path,'w') as word_file:\n",
    "    for word in words:\n",
    "        word_file.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. G2P\n",
    "There three files you need to proceed if you use XPF.\n",
    "1. A G2P rule file\n",
    "2. A veryfication file\n",
    "3. The translater python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dict_file_path):\n",
    "    os.remove(dict_file_path)\n",
    "   \n",
    "# Get the dictionary using XPF\n",
    "# -l specifies the rule file\n",
    "# -c specifies the verification file\n",
    "# -r specifies the file to be translated\n",
    "if g2p == 'xpf':\n",
    "    g2p_cmd = [\"python\", xpf_translater_path, \"-l\", rule_file_path, \"-c\", verify_file_path, \"-r\", word_file_path] # XPF translating command that will be sent to subprocess.run() to execute.\n",
    "\n",
    "    with open(dict_file_path,'w') as dict_file:\n",
    "        subprocess.run(g2p_cmd, stdout = dict_file) # stdout = ... means to send the output to the file (so you have to open this file first as above)\n",
    "\n",
    "    # This is to get rid of all the '@' in the lexicon (if there is any). @ means that XPF G2P failure\n",
    "    with open(dict_file_path, \"r\") as dict_file:\n",
    "        dict = dict_file.read().split(\"\\n\")\n",
    "\n",
    "    with open(dict_file_path, 'w') as dict_file:\n",
    "        for i in dict:\n",
    "            i = re.sub(\" ː\", \"ː\", i)\n",
    "            # Get rid of words that contain sounds XPF can't figure out\n",
    "            if '@' not in i:\n",
    "                dict_file.write(i + \"\\n\")\n",
    "\n",
    "# Or using Epitran\n",
    "elif g2p == 'epi':\n",
    "    g2p_cmd = [\"python\", epitran_translater_path, word_file_path, dict_file_path, epi_code]\n",
    "    subprocess.run(g2p_cmd)\n",
    "\n",
    "# Or use Charsiu\n",
    "elif g2p == 'chr':\n",
    "    # Generate the pronunciation\n",
    "    chr_words = [f'<{code_chr}>: '+i for i in words]\n",
    "\n",
    "    out = chr_tok(words, padding = True, add_special_tokens = False, return_tensors = 'pt')\n",
    "\n",
    "    preds = chr_model.generate(**out, num_beams = 1, max_length = 50) # We do not find beam search helpful. Greedy decoding is enough. \n",
    "    phones = chr_tok.batch_decode(preds.tolist(), skip_special_tokens = True)\n",
    "\n",
    "    # Separate the IPA symbols with white space\n",
    "    from ipatok import tokenise\n",
    "    phones = [tokenise(phone) for phone in phones]\n",
    "    phones = [' '.join(phone) for phone in phones]\n",
    "\n",
    "    # Save the output\n",
    "    dict = []\n",
    "    for word, w in zip(words, phones):\n",
    "        dict.append(word + '\\t' + w)\n",
    "\n",
    "    with open(dict_file_path, 'w') as dict_file:\n",
    "        for i in dict:\n",
    "            dict_file.write(i + \"\\n\")\n",
    "\n",
    "# Or use the pretrained MFA G2P model\n",
    "elif g2p == 'mfa':\n",
    "    cmd_mfa_g2p = f'mfa g2p {word_file_path} {mfa_g2p_path} {dict_file_path}'  # If using a word list\n",
    "    print('To g2p, copy and run:\\t', cmd_mfa_g2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for some languages, you probably want to use the lexicon and the model from MFA or something of your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Validate\n",
    "\n",
    "First, you need to activate the MFA environment in the terminal.\n",
    "1. Press ctrl+` to open Terminal in VS Code.\n",
    "2. Run 'conda activate aligner' until you see '(aligner)' at the beginning of the line in Terminal.\n",
    "3. When you finished using MFA (both training and aligning), run 'conda deactivate' to shut down the MFA environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder of MFA in document\n",
    "# You DON'T need to run this if you already have an MFA folder in your Documents folder (What would this be like on Windows?)\n",
    "# Uncomment the command below to run:\n",
    "#!mfa model download acostic english.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the corpus, run this line in terminal: \n",
    "\n",
    "        mfa validate {wherever your validated recordings are} {wherever your lexicon file is} --ignore_acoustics --clean\n",
    "\n",
    "You can copy the command lines from below.\n",
    "Notebook can't handle ```mfa``` commands. MFA commands can only run in Terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To validate, copy:\tmfa validate /Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hu_v17/validated /Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hu_v17/hu_xpf_lexicon17.txt --ignore_acoustics --clean\n"
     ]
    }
   ],
   "source": [
    "cmd_validate = f'mfa validate {validated_recs_path} {dict_file_path} --ignore_acoustics --clean'\n",
    "print('To validate, copy:\\t' + cmd_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. MFA.\n",
    "\n",
    "### Step 6.1. Train the model\n",
    "\n",
    "        mfa train --clean {where your validated recordings are} {where your lexicon file is} {where your model will be saved}\n",
    "\n",
    "You can copy the command lines from below.\n",
    "Notebook can't handle ```mfa``` commands. The mfa commands above can only run in Terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To train, copy: \tmfa train --clean /Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hu_v17/validated /Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/hu_v17/hu_xpf_lexicon17.txt /Users/miaozhang/Documents/MFA/pretrained_models/acoustic/hu_xpf_acoustic17.zip\n"
     ]
    }
   ],
   "source": [
    "# Train your own model\n",
    "cmd_train = f'mfa train --clean {validated_recs_path} {dict_file_path} {acs_mod_path}'\n",
    "print('To train, copy: \\t' + cmd_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.2. Forced-alignment\n",
    "\n",
    "        mfa align --clean {where your validated recordings are} {where your lexicon file is} {where your acoustic model is} {where your output will be saved}\n",
    "        \n",
    "When the model is trained, align the corpus.\n",
    "\n",
    "However, since the MFA alignment somehow stops after generating 32609 textgrid files, we will split the corpus into n subfolders with each subfolder containing 32000 files.\n",
    "If the corpus has more than 32000 recordings, move the mp3 and textgrid files into subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all mp3 files in the validated folder\n",
    "all_mp3 = [item for item in os.listdir(validated_recs_path) if os.path.splitext(item)[1] == '.mp3']\n",
    "n_clips = len(all_mp3)\n",
    "print(f\"There are {n_clips} clips in the validated folder.\")\n",
    "n_valid = len(valid)\n",
    "\n",
    "if n_clips > 32000:\n",
    "    # Create subfolders\n",
    "    subfolders = valid['subfolder'].unique()\n",
    "    for subfolder in subfolders:\n",
    "        subfolder_path = os.path.join(validated_recs_path, subfolder)\n",
    "        if not os.path.exists(subfolder_path):\n",
    "            os.makedirs(subfolder_path)\n",
    "\n",
    "    # Create the paths in the subfolders for each recording according to their grouping\n",
    "    splits = valid[valid['path'].isin(all_mp3)]\n",
    "    splits.to_csv(os.path.join(language_dir, 'all_splits.csv'), index = False)\n",
    "\n",
    "    # Move the files into subfolders using multithreads\n",
    "    n_workers = 10\n",
    "    chunksize = round(len(splits) / n_workers)\n",
    "    with ThreadPoolExecutor(n_workers) as exe:\n",
    "        for i in range(0, len(splits), chunksize):\n",
    "            chunk_data = splits.loc[i:(i+chunksize),]\n",
    "            _ = exe.submit(vxcproc.split_recs, chunk_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all mp3 and textgrid files are moved to subfolders, and check if there are any overlapping file names across the subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all mp3 and textgrid files are moved to the subfolders\n",
    "if n_valid > 32000:\n",
    "    # If there are still files left in the root directory, move them into their subfolders\n",
    "    rest_mp3 = [item for item in os.listdir(validated_recs_path) if os.path.splitext(item)[1] == '.mp3']\n",
    "    rest_move = valid[valid['path'].isin(rest_mp3)]\n",
    "    vxcproc.split_recs(rest_move)\n",
    "    del rest_move, rest_mp3\n",
    "    \n",
    "    # Check if there are still mp3 or textgrid files in the root directory\n",
    "    contains_subdir = any(\n",
    "        os.path.isfile(os.path.join(validated_recs_path, item)) and \n",
    "        (item.lower().endswith('.mp3') or item.lower().endswith('.textgrid')) \n",
    "        for item in os.listdir(validated_recs_path)\n",
    "        )\n",
    "    if contains_subdir:\n",
    "        print(\"The validated folder still contains mp3 or TextGrid files.\")\n",
    "        print('')\n",
    "        \n",
    "    else:\n",
    "        print(\"All mp3 or TextGrid files are moved to subfolders.\")\n",
    "        print('')\n",
    "\n",
    "    # Check if there are overlapping file names across the subfolders\n",
    "    overlap_dict = vxcproc.check_file_overlaps(validated_recs_path)\n",
    "    if len(overlap_dict) == 0:\n",
    "        print(\"There are no overlapping file names across the subfolders.\") \n",
    "    else:\n",
    "        print(overlap_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the MFA commands to align the data in (each subfolder of) the validated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To align, copy: \tmfa align --clean /Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/ro_v17/validated /Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/ro_v17/ro_xpf_lexicon17.txt /Users/miaozhang/Documents/MFA/pretrained_models/acoustic/ro_xpf_acoustic17.zip /Users/miaozhang/Research/CorpusPhon/CorpusData/CommonVoice/ro_v17/output\n"
     ]
    }
   ],
   "source": [
    "# Print the MFA commands for alignment\n",
    "all_items = os.listdir(validated_recs_path)\n",
    "all_items = [file for file in all_items if '.DS_Store' not in file]\n",
    "all_items.sort()\n",
    "any_file = any(os.path.isfile(os.path.join(validated_recs_path, item)) for item in all_items)\n",
    "if not any_file:\n",
    "    # Use a bash script to automatically align the data in all subfolders. Remember to activate the MFA virtual environment: conda activate aligner\n",
    "    print(f'Copy and run this in the terminal to grant execution permission to the script:\\tchmod +x {mfa_align_script_path}', '\\n')\n",
    "    print(f'Copy and run this in the terminal to align the data in all subfolders:\\tbash {mfa_align_script_path} {validated_recs_path} {dict_file_path} {acs_mod_path} {output_path}', '\\n')\n",
    "else:  \n",
    "    cmd_train = f'mfa align --clean {validated_recs_path} {dict_file_path} {acs_mod_path} {output_path}'\n",
    "    print('To align, copy: \\t' + cmd_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.3: (optional) Put back the recordings to the validated folder\n",
    "\n",
    "When the alignment is done, if splits were created for aligning the data, put the recordings back to one single folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_valid > 32000:\n",
    "    n_workers = 10\n",
    "    chunksize = round(len(valid) / n_workers)\n",
    "    with ThreadPoolExecutor(n_workers) as exe:\n",
    "        for i in range(0, len(valid), chunksize):\n",
    "            chunk_data = valid.loc[i:(i+chunksize),]\n",
    "            _ = exe.submit(vxcproc.merge_recs, chunk_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all files are put back to the validated folder's root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!If it reports there are still subfolders undeleted, run this block for a second time! It should move any left files from any subfolder back to the validated folder.\n",
    "\n",
    "if n_valid > 32000:\n",
    "    # Use os.scandir() for better performance\n",
    "    with os.scandir(validated_recs_path) as entries:\n",
    "        subfolders = [entry.name for entry in entries if entry.is_dir()]\n",
    "        subfolders.sort()\n",
    "\n",
    "    # Lists to store undeleted subfolders and files\n",
    "    undeleted_subfolders = []\n",
    "\n",
    "    # Batch deletion of empty subfolders\n",
    "    for subfolder in subfolders:\n",
    "        subfolder_path = os.path.join(validated_recs_path, subfolder)\n",
    "        with os.scandir(subfolder_path) as sub_entries:\n",
    "            if not any(entry.is_file() for entry in sub_entries):\n",
    "                # If the subfolder does not contain any files, delete it\n",
    "                shutil.rmtree(subfolder_path)\n",
    "                print(f\"Subfolder '{subfolder}' deleted because it contains no files.\")\n",
    "            else:\n",
    "                undeleted_subfolders.append(subfolder)\n",
    "\n",
    "    print(\"Subfolders checked and processed.\")\n",
    "\n",
    "    # List undeleted subfolders\n",
    "    if len(undeleted_subfolders) > 0:\n",
    "        print(\"Undeleted subfolders:\")\n",
    "        for subfolder in undeleted_subfolders:\n",
    "            print(subfolder)\n",
    "            # Move the files in the uncleared subfolder back to the validated folder if there is any\n",
    "            vxcproc.move_files_to_root(validated_recs_path, os.path.join(validated_recs_path, subfolder))\n",
    "        \n",
    "    # Check if all the subfolders are deleted\n",
    "    with os.scandir(validated_recs_path) as entries:\n",
    "        contains_subdir = any(entry.is_dir() for entry in entries)\n",
    "        if contains_subdir:\n",
    "            print(\"\\nThe validated folder still contains subfolders.\")\n",
    "        else:\n",
    "            print(\"\\nThe validated folder does not contain any subfolders now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the output and input files match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17737 mp3 files in the validated folder.\n",
      "There are 17737 textgrid files in the output folder.\n",
      "(True, 'The recordings in the validated folder and the textgrids in the output folder match.')\n"
     ]
    }
   ],
   "source": [
    "with multiprocessing.Pool() as pool:\n",
    "    result = pool.apply(vxcproc.compare_inout, args=(output_path, validated_recs_path))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, move the output files (the speaker file, the lexicon, the acoustic model, and the aligned textgrids) to the OSF folder to be ready to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/miaozhang/Research/CorpusPhon/CorpusData/VoxCommunis_OSF/spkr_files/ro_xpf_spkr17.tsv'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a .tar.gz file of the aligned textgrids\n",
    "# list all files to add to the zip\n",
    "tgfiles = [os.path.join(output_path, filename) for filename in os.listdir(output_path)]\n",
    "# create lock for adding files to the zip\n",
    "lock = Lock()\n",
    "# open the zip file\n",
    "with zipfile.ZipFile(txtgrds_path, 'w', compression=zipfile.ZIP_DEFLATED) as handle:\n",
    "    # create the thread pool\n",
    "    with ThreadPoolExecutor(10) as exe:\n",
    "        # add all files to the zip archive\n",
    "        _ = [exe.submit(vxcproc.add_file, lock, handle, tg, output_path) for tg in tgfiles]\n",
    "\n",
    "# Move the acoustic model\n",
    "shutil.copy(acs_mod_path, os.path.join(osf_path, 'acoustic_models'))\n",
    "\n",
    "# Move the lexicon\n",
    "shutil.copy(dict_file_path, os.path.join(osf_path, 'lexicons'))\n",
    "\n",
    "# Move the speaker file\n",
    "shutil.copy(spkr_file_path, os.path.join(osf_path, 'spkr_files'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, upadate the tracking info in `VoxCommunis_Info.csv`. \n",
    "\n",
    "Make sure it is not in the lang_code_processing folder. Once updated, push the updated .csv to the GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have trained the model, set this to 1\n",
    "model_trained = 1\n",
    "aligned = 1\n",
    "\n",
    "# Paste the name of the outputs into the tracking file\n",
    "cv_track = pd.read_csv(cv_tracking_file)\n",
    "cv_track = cv_track.astype('string')\n",
    "if model_trained == 1:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'acoustic_model'] = acs_mod_name\n",
    "else:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'acoustic_model'] = ''\n",
    "if aligned == 1:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'textgrids'] = textgrid_folder_name\n",
    "else:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'textgrids'] = ''\n",
    "cv_track.loc[cv_track['code_cv'] == lang_code, 'spkr_file'] = spkr_file_name\n",
    "cv_track.loc[cv_track['code_cv'] == lang_code, 'lexicon'] = dict_file_name\n",
    "\n",
    "\n",
    "# Update the tracking file\n",
    "cv_track.to_csv(cv_tracking_file, index = False)\n",
    "\n",
    "# Sample ten files to check the alignment afterwards\n",
    "post_check = valid.sample(10, random_state=42)\n",
    "post_check.to_csv(os.path.join(language_dir, \"post_check.csv\"), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
