{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VoxCommunis data processing pipeline\n",
    "\n",
    "This is a script of running MFA on recordings from Common Voice corpus. \n",
    "\n",
    "To run this pipeline, you need to download:\n",
    "\n",
    "1. Python modules: epitran, praatio, re, pandas, numpy, subprocess, shutil, os\n",
    "2. The data of XPF corpus\n",
    "\n",
    "The pipeline takes these steps to process data:\n",
    "\n",
    "1. [Step 0: Setups](#step-0-setups)\n",
    "2. [Step 1: Remap speakers](#step-1-remap-the-validated-speakers)\n",
    "3. [Step 2: Create TextGrid and .wav files](#step-2-create-textgrid-files-and-wav-files-based-on-the-mp3-recordings-from-common-voice)\n",
    "4. [Step 3: Prepare the lexicon](#step-3-prepare-the-lexicon)\n",
    "5. [Step 4: G2P grapheme-to-phoneme](#step-4-g2p-grapheme-to-phoneme-epitran-or-xpf)\n",
    "6. [Step 5: Validation](#step-5-train-the-acoustic-model)\n",
    "7. [Step 6: Run MFA](#step-6-train-the-acoustic-model-and-forced-align)\n",
    "8. [Finale](#finale)\n",
    "\n",
    "This script was created by Miao Zhang (miao.zhang@uzh.ch), 22.12.2023\n",
    "\n",
    "This script was modified by Miao Zhang, 07.02.2024 (Revalidation added)\n",
    "\n",
    "Modified on 16.02.2024: added automatic log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Setups\n",
    "Import packages and setup file directories (for both the scripts and data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lingpy in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (2.6.13)\n",
      "Requirement already satisfied: numpy in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from lingpy) (1.22.0)\n",
      "Requirement already satisfied: appdirs in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from lingpy) (1.4.4)\n",
      "Requirement already satisfied: networkx>=2.3 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from lingpy) (2.8.8)\n",
      "Requirement already satisfied: tqdm in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from lingpy) (4.66.1)\n",
      "Requirement already satisfied: csvw>=1.5.6 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from lingpy) (3.5.1)\n",
      "Requirement already satisfied: clldutils>=2.8.0 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from lingpy) (3.23.1)\n",
      "Requirement already satisfied: pycldf>=1.7.0 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from lingpy) (1.39.0)\n",
      "Requirement already satisfied: lxml in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from lingpy) (5.3.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from clldutils>=2.8.0->lingpy) (2.8.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from clldutils>=2.8.0->lingpy) (0.9.0)\n",
      "Requirement already satisfied: colorlog in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from clldutils>=2.8.0->lingpy) (6.8.2)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from clldutils>=2.8.0->lingpy) (23.1.0)\n",
      "Requirement already satisfied: bibtexparser>=2.0.0b4 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from clldutils>=2.8.0->lingpy) (2.0.0b7)\n",
      "Requirement already satisfied: pylatexenc in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from clldutils>=2.8.0->lingpy) (2.10)\n",
      "Requirement already satisfied: markdown in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from clldutils>=2.8.0->lingpy) (3.5.1)\n",
      "Requirement already satisfied: markupsafe in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from clldutils>=2.8.0->lingpy) (2.1.3)\n",
      "Requirement already satisfied: isodate in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from csvw>=1.5.6->lingpy) (0.7.2)\n",
      "Requirement already satisfied: rfc3986<2 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from csvw>=1.5.6->lingpy) (1.5.0)\n",
      "Requirement already satisfied: uritemplate>=3.0.0 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from csvw>=1.5.6->lingpy) (4.1.1)\n",
      "Requirement already satisfied: babel in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from csvw>=1.5.6->lingpy) (2.13.1)\n",
      "Requirement already satisfied: requests in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from csvw>=1.5.6->lingpy) (2.31.0)\n",
      "Requirement already satisfied: language-tags in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from csvw>=1.5.6->lingpy) (1.2.0)\n",
      "Requirement already satisfied: rdflib in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from csvw>=1.5.6->lingpy) (7.1.0)\n",
      "Requirement already satisfied: colorama in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from csvw>=1.5.6->lingpy) (0.4.6)\n",
      "Requirement already satisfied: jsonschema in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from csvw>=1.5.6->lingpy) (4.23.0)\n",
      "Requirement already satisfied: PyYAML in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from pycldf>=1.7.0->lingpy) (6.0.1)\n",
      "Requirement already satisfied: commonnexus>=1.2.0 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from pycldf>=1.7.0->lingpy) (1.9.2)\n",
      "Requirement already satisfied: jmespath in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from pycldf>=1.7.0->lingpy) (1.0.1)\n",
      "Requirement already satisfied: newick in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from pycldf>=1.7.0->lingpy) (1.9.0)\n",
      "Requirement already satisfied: pybtex in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from pycldf>=1.7.0->lingpy) (0.24.0)\n",
      "Requirement already satisfied: python-frontmatter in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from pycldf>=1.7.0->lingpy) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from pycldf>=1.7.0->lingpy) (58.0.4)\n",
      "Requirement already satisfied: termcolor in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from pycldf>=1.7.0->lingpy) (2.5.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from jsonschema->csvw>=1.5.6->lingpy) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from jsonschema->csvw>=1.5.6->lingpy) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from jsonschema->csvw>=1.5.6->lingpy) (0.20.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from markdown->clldutils>=2.8.0->lingpy) (7.0.0)\n",
      "Requirement already satisfied: latexcodec>=1.0.4 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from pybtex->pycldf>=1.7.0->lingpy) (3.0.0)\n",
      "Requirement already satisfied: six in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from pybtex->pycldf>=1.7.0->lingpy) (1.15.0)\n",
      "Requirement already satisfied: html5lib-modern<2.0,>=1.2 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from rdflib->csvw>=1.5.6->lingpy) (1.2)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from rdflib->csvw>=1.5.6->lingpy) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from requests->csvw>=1.5.6->lingpy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from requests->csvw>=1.5.6->lingpy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from requests->csvw>=1.5.6->lingpy) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from requests->csvw>=1.5.6->lingpy) (2023.11.17)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/eleanorchodroff/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown->clldutils>=2.8.0->lingpy) (3.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lingpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os, subprocess, shutil, re, csv, sys, importlib\n",
    "import pandas as pd\n",
    "# Turn Copy-On-Write on\n",
    "pd.options.mode.copy_on_write = True\n",
    "import numpy as np\n",
    "\n",
    "# For creating textgrids\n",
    "from praatio import textgrid\n",
    "\n",
    "# For move files concurrently\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Import functions from cv_processing.py\n",
    "import vxc_processing as vxcproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'vxc_processing' from '/Users/eleanorchodroff/Documents/GitHub/voxcommunis/vxc_pipeline/vxc_processing.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(vxcproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the paths and directories of data and scripts to use.\n",
    "\n",
    "_IMPORTANT_: the folder of the corpus data you downloaded from Common Voice should be named as: {lang_code}_v{version_number}.\n",
    "- For example: the folder for the 16th version of Divhehi should be named: dv_v16.\n",
    "- Another example: the folder for the 15th version of Upper Sorbian should be: hsb_v15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### Directories ################################################\n",
    "\n",
    "# This is the directory where your data downloaded from Common Voice should be saved. This is the root directory where data from each language should be saved in individual folders.\n",
    "# NO (BACK)SLASH at the end!!!\n",
    "commonVoice_dir = '/Users/eleanorchodroff/Documents/CommonVoice_processing' \n",
    "\n",
    "# To use XPF as the G2P engine to process lexicon, you will need to download the XPF data from: https://github.com/CohenPr-XPF/XPF/tree/master/Data and save them on your computer.\n",
    "# Specify the directory where your XPF data is saved.\n",
    "# NO (BACK)SLASH at the end!!!\n",
    "xpf_dir = '/Users/eleanorchodroff/Documents/GitHub/XPF/Data/mk_Macedonian'\n",
    "\n",
    "######################### Language name/code and Common Voice version ##############################\n",
    "\n",
    "# Language-related variable names\n",
    "# the Common Voice code of the language (unfortunately, Common Voice mixes the use of iso 639-3 and iso 639-1 codes (they use bcp47 code). This code is also used in XPF).\n",
    "# The code should match the code used in the name of the folder you downloaded from Common Voice.\n",
    "lang_code = 'bas' \n",
    "\n",
    "# The version of the data in Common Voice\n",
    "# Only numbers!!!\n",
    "cv_version = '20' \n",
    "\n",
    "######################### G2P ######################################################################\n",
    "\n",
    "# Specify the G2P engine. Only these keywords are acceptable: \n",
    "# 'xpf' for XPF\n",
    "# 'epi' for Epitran\n",
    "# 'chr' for Charsiu\n",
    "# 'mfa' for MFA\n",
    "# 'vxc' for self-difined lexicon\n",
    "g2p = 'vxc'\n",
    "\n",
    "######################### The delimiter ############################################################\n",
    "\n",
    "# Set it to 0 if you use a Windows machine.\n",
    "if_mac = 1 \n",
    "\n",
    "######################### What writing system is the language using? ###############################\n",
    "\n",
    "# Specify if the language is Chinese/Japanese/Korean\n",
    "if_cjk = 0\n",
    "\n",
    "######################### Using existing model? ###############################\n",
    "\n",
    "if_self_mod = 0\n",
    "\n",
    "######################### Using existing lexicon? ###############################\n",
    "\n",
    "if_self_lex = 0\n",
    "\n",
    "###################### G2P settings (XPF or Epitran) ################################################\n",
    "\n",
    "# This is where VxcCommunis_tracking.csv is (NO (BACK)SLASH at the end!!!):\n",
    "cv_tracking_file = 'VoxCommunis_Info.csv'\n",
    "\n",
    "# Get the G2P processing code for the language\n",
    "if g2p == 'xpf' or 'chr':\n",
    "    with open(cv_tracking_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        lang_row = [row for row in reader if row['code_cv'] == lang_code][0]\n",
    "    # If you are using XPF, get the name of the language in XPF corpus\n",
    "    if g2p == 'xpf':\n",
    "        lang_name = lang_row['name_xpf'].replace(' ', '')\n",
    "    # If you are using Charsiu, get the processing code for the language in Charsiu.\n",
    "    elif g2p == 'chr':\n",
    "        code_chr = lang_row['code_chr']\n",
    "\n",
    "if g2p == 'epi':\n",
    "    # If you are using epitran, ...\n",
    "    # Please refer to VoxCommunics_info.csv to get the processing code of the language in epitran\n",
    "    # !!!Do this manually, since depending on the type of the orthography, the epitran code can differ!!!\n",
    "    epi_code = 'rus-Cyrl'\n",
    "\n",
    "\n",
    "# Specify if the subversion of a corpus is used. The default is 0\n",
    "if_subversion = 0 \n",
    "# If if_subversion == 1, what suffix you would use?:\n",
    "# Ignore this part, if you don't have a subversion of the corpus you are using.\n",
    "subversion = '_' + 'sub3'\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "if if_mac == 1:\n",
    "    path_sep = '/'\n",
    "    # this is the default directory where Praat is installed on a Mac.\n",
    "    #praat_path = '/Applications/Praat.app/Contents/MacOS/Praat' \n",
    "else:\n",
    "    path_sep = '\\\\'\n",
    "    # the directory of Praat installed on Windows.\n",
    "    #praat_path = 'C:\\Program Files\\Praat.exe' \n",
    "\n",
    "# The folder for the language\n",
    "language_dir = commonVoice_dir + path_sep + lang_code + '_v' + cv_version\n",
    "\n",
    "# The file that contains the duration of each clip:\n",
    "clip_info_path = language_dir + path_sep + 'clip_durations.tsv'\n",
    "\n",
    "# MFA paths\n",
    "# The folder of the OOV word files (NO (BACK)SLASH at the end!!!):\n",
    "mfa_oov_path = '/Users/eleanorchodroff/Documents/MFA/validated'\n",
    "# This is where the acoustic model will be saved after MFA training is done (NO (BACK)SLASH at the end!!!):\n",
    "mfa_mod_folder = '/Users/eleanorchodroff/Documents/MFA/pretrained_models/acoustic'\n",
    "\n",
    "\n",
    "############################################################################################################################\n",
    "\n",
    "\n",
    "# This is where files that will be uploaded to the OSF repo will be saved after the processing is finished (NO (BACK)SLASH at the end!!!):\n",
    "osf_path = '/Users/eleanorchodroff/Documents/CommonVoice/VoxCommunis_OSF'\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "####################################################################################################################################\n",
    "cv_mod_version = \"20\"\n",
    "cv_align_version = \"20\"\n",
    "# Get the naming schema.\n",
    "naming_schema = pd.read_csv('vxc_naming_schema.csv', usecols = ['Python_code'])['Python_code'].tolist()\n",
    "naming_schema = [eval(name) for name in naming_schema]\n",
    "acs_mod_name = naming_schema[0]\n",
    "textgrid_folder_name = naming_schema[1]\n",
    "word_file_name = naming_schema[2]\n",
    "dict_file_name = naming_schema[3]\n",
    "spkr_file_name = naming_schema[4]\n",
    "textgrid_folder_path = language_dir + path_sep + textgrid_folder_name\n",
    "word_file_path = language_dir + path_sep + word_file_name\n",
    "dict_file_path = language_dir + path_sep + dict_file_name\n",
    "spkr_file_path = language_dir + path_sep + spkr_file_name\n",
    "del naming_schema\n",
    "\n",
    "# For step 3: prepare the lexicon and pronunciation dictionary\n",
    "validated_log = language_dir + path_sep + 'validated.tsv'\n",
    "\n",
    "# For step 4: G2P\n",
    "if g2p == 'xpf':\n",
    "    xpf_translater_path = 'xpf_translate04.py'\n",
    "    rule_file_path = xpf_dir + path_sep + lang_code + '_' + lang_name + path_sep + lang_code + '.rules'\n",
    "    verify_file_path = xpf_dir + path_sep + lang_code + '_' + lang_name + path_sep + lang_code + '.verify.csv'\n",
    "elif g2p == 'epi':\n",
    "    epitran_translater_path = 'epi_run.py'\n",
    "elif g2p == 'chr':\n",
    "    from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "    chr_model = T5ForConditionalGeneration.from_pretrained('charsiu/g2p_multilingual_byT5_tiny_16_layers_100')\n",
    "    chr_tok = AutoTokenizer.from_pretrained('google/byt5-small')\n",
    "\n",
    "\n",
    "# For step 6: running MFA\n",
    "if if_self_mod == 1:\n",
    "    # Specify the path of the model\n",
    "    acs_mod_path = '/Users/eleanorchodroff/Documents/MFA/pretrained_models/acoustic/bas20_cvu.zip'\n",
    "if if_self_lex == 1:\n",
    "    # Specify the path of the lexicon\n",
    "    dict_file_path = language_dir + path_sep + 'ca_lexicon-IPA.txt'\n",
    "\n",
    "# Validate the corpus\n",
    "validated_recs_path = language_dir + path_sep + 'validated'\n",
    "if if_subversion == 0:\n",
    "    acs_mod_path = mfa_mod_folder + path_sep + acs_mod_name\n",
    "else:\n",
    "    acs_mod_name = re.sub('.zip', subversion + '.zip', acs_mod_name)\n",
    "    acs_mod_path = mfa_mod_folder + path_sep + acs_mod_name\n",
    "output_path = language_dir + path_sep + 'output'\n",
    "\n",
    "# Finale:\n",
    "txtgrds_path = osf_path + path_sep + 'textgrids' + path_sep + textgrid_folder_name[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Remap the validated speakers\n",
    "Get speaker IDs to put on TextGrids for speaker adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eleanorchodroff/Documents/CommonVoice_processing/bas_v20 /\n",
      "/Users/eleanorchodroff/Documents/CommonVoice_processing/bas_v20/bas_vxc_spkr20.tsv bas\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(language_dir, path_sep)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(spkr_file_path, lang_code)\n\u001b[0;32m----> 3\u001b[0m whole \u001b[38;5;241m=\u001b[39m \u001b[43mvxcproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremap_spkr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_sep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspkr_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/voxcommunis/vxc_pipeline/vxc_processing.py:91\u001b[0m, in \u001b[0;36mremap_spkr\u001b[0;34m(lang_dir, spkr_file_path, lang_code, output)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(spkr_file_path):\n\u001b[0;32m---> 91\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspkr_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     validated\u001b[38;5;241m.\u001b[39mto_csv(spkr_file_path, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# The file paths\u001b[39;00m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/'"
     ]
    }
   ],
   "source": [
    "print(language_dir, path_sep)\n",
    "print(spkr_file_path, lang_code)\n",
    "whole = vxcproc.remap_spkr(language_dir, path_sep, spkr_file_path, lang_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Create TextGrid files for the validated recordings and save the them in a separate folder.\n",
    "\n",
    "All validated clips that are longer than 1s will be moved to a subfolder called 'validated'.\n",
    "\n",
    "The validated clips but are shorter than 1s will be moved to the 'other' folder.\n",
    "\n",
    "The invalidated clips will stay in the 'clips' folder. When the moving is done the 'clips' folder will be renamed to 'invalidated'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the folder for validated clips and other clips:\n",
    "other_folder = language_dir + path_sep + 'other'\n",
    "if os.path.exists(validated_recs_path) or os.path.exists(other_folder):\n",
    "    shutil.rmtree(validated_recs_path)\n",
    "    shutil.rmtree(other_folder)\n",
    "os.makedirs(validated_recs_path)\n",
    "os.makedirs(other_folder)\n",
    "\n",
    "# Setup file chunks to batch processing clip moving and textgrid creating \n",
    "n_clips = len(whole.index)\n",
    "n_workers = 10\n",
    "chunksize = round(n_clips / n_workers)\n",
    "\n",
    "# Move the clips and create textgrid files:\n",
    "with ThreadPoolExecutor(n_workers) as exe:\n",
    "    for i in range(0, len(whole), chunksize):\n",
    "        chunk_data = whole.loc[i:(i+chunksize),]\n",
    "        _ = exe.submit(vxcproc.move_and_create_tg, chunk_data)\n",
    "\n",
    "# Rename the clip folder to invalidated\n",
    "os.rename(language_dir + path_sep + 'clips', language_dir + path_sep + 'invalidated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Prepare the lexicon\n",
    "Generate the wordlist from Common Voice transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "if lang_code == 'ja':\n",
    "    words = vxcproc.process_words(spkr_file_path, lang_code)\n",
    "else:\n",
    "    words = vxcproc.process_words(validated_log, lang_code)\n",
    "\n",
    "# Filter other out unwanted words\n",
    "words = vxcproc.remove_unwanted_words(words, lang_code, if_cjk)\n",
    "\n",
    "# Save the word list as a .txt file\n",
    "if os.path.exists(word_file_path):\n",
    "    os.remove(word_file_path)\n",
    "    \n",
    "with open(word_file_path,'w') as word_file:\n",
    "    for word in words:\n",
    "        word_file.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. G2P grapheme-to-phoneme (Epitran or XPF)\n",
    "There three files you need to proceed if you use XPF.\n",
    "1. A G2P rule file\n",
    "2. A veryfication file\n",
    "3. The translater python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dict_file_path):\n",
    "    os.remove(dict_file_path)\n",
    "   \n",
    "# Get the dictionary using XPF\n",
    "# -l specifies the rule file\n",
    "# -c specifies the verification file\n",
    "# -r specifies the file to be translated\n",
    "if g2p == 'xpf':\n",
    "    g2p_cmd = [\"python\", xpf_translater_path, \"-l\", rule_file_path, \"-c\", verify_file_path, \"-r\", word_file_path] # XPF translating command that will be sent to subprocess.run() to execute.\n",
    "\n",
    "    with open(dict_file_path,'w') as dict_file:\n",
    "        subprocess.run(g2p_cmd, stdout = dict_file) # stdout = ... means to send the output to the file (so you have to open this file first as above)\n",
    "\n",
    "    # This is to get rid of all the '@' in the lexicon (if there is any). @ means that XPF G2P failure\n",
    "    with open(dict_file_path, \"r\") as dict_file:\n",
    "        dict = dict_file.read().split(\"\\n\")\n",
    "\n",
    "    with open(dict_file_path, 'w') as dict_file:\n",
    "        for i in dict:\n",
    "            i = re.sub(\" ː\", \"ː\", i)\n",
    "            # Get rid of words that contain sounds XPF can't figure out\n",
    "            if '@' not in i:\n",
    "                dict_file.write(i + \"\\n\")\n",
    "\n",
    "# Or using Epitran\n",
    "elif g2p == 'epi':\n",
    "    g2p_cmd = [\"python\", epitran_translater_path, word_file_path, dict_file_path, epi_code]\n",
    "    subprocess.run(g2p_cmd)\n",
    "\n",
    "elif g2p == 'chr':\n",
    "    from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained('charsiu/g2p_multilingual_byT5_tiny_16_layers_100')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('google/byt5-small')\n",
    "\n",
    "    chr_words = [f'<{code_chr}>: '+i for i in words]\n",
    "\n",
    "    out = tokenizer(words, padding = True, add_special_tokens = False, return_tensors = 'pt')\n",
    "\n",
    "    preds = model.generate(**out, num_beams = 1, max_length = 50) # We do not find beam search helpful. Greedy decoding is enough. \n",
    "    phones = tokenizer.batch_decode(preds.tolist(), skip_special_tokens = True)\n",
    "\n",
    "    from ipatok import tokenise\n",
    "    phones = [tokenise(phone) for phone in phones]\n",
    "    phones = [' '.join(phone) for phone in phones]\n",
    "\n",
    "    dict = []\n",
    "    for sent, w in zip(sentence, phones):\n",
    "        dict.append(sent + '\\t' + w)\n",
    "\n",
    "elif g2p == 'mfa':\n",
    "    cmd_mfa_g2p = f'mfa g2p {word_file_path} {mfa_g2p_path} {dict_file_path}'  # If using a word list\n",
    "    print('To g2p, copy and run:\\t', cmd_mfa_g2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for some languages, you probably want to use the lexicon and the model from MFA or something of your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Validate the corpus\n",
    "\n",
    "First, you need to activate the MFA environment in the terminal.\n",
    "1. Press ctrl+` to open Terminal in VS Code.\n",
    "2. Run 'conda activate aligner' until you see '(aligner)' at the beginning of the line in Terminal.\n",
    "3. When you finished using MFA (both training and aligning), run 'conda deactivate' to shut down the MFA environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder of MFA in document\n",
    "# You DON'T need to run this if you already have an MFA folder in your Documents folder (What would this be like on Windows?)\n",
    "# Uncomment the command below to run:\n",
    "#!mfa model download acostic english.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the corpus, run this line in terminal: \n",
    "\n",
    "        mfa validate {wherever your validated recordings are} {wherever your lexicon file is} --ignore_acoustics --clean\n",
    "\n",
    "You can copy the command lines from below.\n",
    "Notebook can't handle ```mfa``` commands. MFA commands can only run in Terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_validate = f'mfa validate {validated_recs_path} {dict_file_path} --ignore_acoustics --clean'\n",
    "print('To validate, copy:\\t' + cmd_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Train the acoustic model and forced align.\n",
    "\n",
    "### Step 6.1. Then to train the acoustic model, run the next line:\n",
    "\n",
    "        mfa train --clean {where your validated recordings are} {where your lexicon file is} {where your model will be saved}\n",
    "\n",
    "You can copy the command lines from below.\n",
    "Notebook can't handle ```mfa``` commands. The mfa commands above can only run in Terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your own model\n",
    "cmd_train = f'mfa train --clean {validated_recs_path} {dict_file_path} {acs_mod_path}'\n",
    "print('To train, copy: \\t' + cmd_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.2. The final step: forced align the recordings:\n",
    "\n",
    "        mfa align --clean {where your validated recordings are} {where your lexicon file is} {where your acoustic model is} {where your output will be saved}\n",
    "        \n",
    "When the model is trained, align the corpus.\n",
    "\n",
    "However, since the MFA alignment somehow stops after generating 32609 textgrid files, we will split the corpus into n subfolders with each subfolder containing 32000 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the mp3 files in the validated folder\n",
    "all_file = os.listdir(validated_recs_path)\n",
    "all_mp3 = [file for file in all_file if file.endswith('.mp3')]\n",
    "\n",
    "# If there are more than 32000 mp3s in the validated folder, split them into several subfolders with each one contains no more than 32000 clips\n",
    "n_mp3 = len(all_mp3)\n",
    "if n_mp3 > 32000:\n",
    "    # Get the source path\n",
    "    all_root = [os.path.join(validated_recs_path, rec) for rec in all_mp3]\n",
    "\n",
    "    # Group the files into n groups with each group\n",
    "    all_grouped = [(i, all_mp3[i:i+32000]) for i, _ in enumerate(range(0, len(all_mp3), 32000))]\n",
    "    # Get the destination path\n",
    "    all_recs_sub = [f\"{validated_recs_path}{path_sep}subfolder_{index}{path_sep}{i}\" for index, group in all_grouped for i in group]\n",
    "\n",
    "    # Create subfolders\n",
    "    for index, sublist in enumerate(all_grouped):\n",
    "        subfolder_path = os.path.join(validated_recs_path, f'subfolder_{index}')\n",
    "        if not os.path.exists(subfolder_path):\n",
    "            os.makedirs(subfolder_path)\n",
    "        \n",
    "    # Move the files to the subfolders with multithreading\n",
    "    n_workers = 10\n",
    "    chunksize = round(n_mp3 / n_workers)\n",
    "    with ThreadPoolExecutor(n_workers) as exe:\n",
    "        for i in range(0, len(all_root), chunksize):\n",
    "            src_names = all_root[i:(i+chunksize)]\n",
    "            dest_names = all_recs_sub[i:(i+chunksize)]\n",
    "            _ = exe.submit(vxcproc.move_recs, src_names, dest_names)\n",
    "\n",
    "    # Check if all mp3 and TextGrid files are moved into subfolders\n",
    "    all_items = os.listdir(validated_recs_path)\n",
    "    contains_subdir = any(\n",
    "        os.path.isfile(os.path.join(validated_recs_path, item)) and \n",
    "        (item.lower().endswith('.mp3') or item.lower().endswith('.textgrid')) \n",
    "        for item in all_items\n",
    "        )\n",
    "    if contains_subdir:\n",
    "        print(\"The validated folder still contains mp3 or TextGrid files.\")\n",
    "        print('')\n",
    "    else:\n",
    "        print(\"All mp3 or TextGrid files are moved to subfolders.\")\n",
    "        print('')\n",
    "\n",
    "    # Print the MFA aligning codes\n",
    "    for index, sublist in enumerate(all_grouped):\n",
    "        subfolder_path = os.path.join(validated_recs_path, f'subfolder_{index}')\n",
    "        cmd_train = f'mfa align --clean {subfolder_path} {dict_file_path} {acs_mod_path} {output_path}'\n",
    "        print(f'To align split {index}, copy: \\t' + cmd_train)\n",
    "        print('')\n",
    "else:\n",
    "    cmd_train = f'mfa align --clean {validated_recs_path} {dict_file_path} {acs_mod_path} {output_path}'\n",
    "    print('To align, copy: \\t' + cmd_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finale\n",
    "\n",
    "First, if splits were created for aligning the data, put the recordings back to one folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After finishing the forced-alignment, move the files in the subfolders out into validated folder\n",
    "if n_mp3 > 32000:\n",
    "    n_workers = 10\n",
    "    chunksize = round(n_mp3 / n_workers)\n",
    "    with ThreadPoolExecutor(n_workers) as exe:\n",
    "        for i in range(0, len(all_root), chunksize):\n",
    "            src_names = all_recs_sub[i:(i+chunksize)]\n",
    "            dest_names = all_root[i:(i+chunksize)]\n",
    "            _ = exe.submit(vxcproc.move_recs, src_names, dest_names)\n",
    "    \n",
    "    # Delete the empty subfolders\n",
    "    for index, sublist in enumerate(all_grouped):\n",
    "        subfolder_path = os.path.join(validated_recs_path, f'subfolder_{index}')\n",
    "        if os.path.exists(subfolder_path):\n",
    "            shutil.rmtree(subfolder_path)\n",
    "\n",
    "    all_items = os.listdir(validated_recs_path)\n",
    "    contains_subdir = any(os.path.isdir(os.path.join(validated_recs_path, item)) for item in all_items)\n",
    "    if contains_subdir:\n",
    "        print(\"The validated folder still contains subfolders.\")\n",
    "    else:\n",
    "        print(\"The validated folder does not contain any subfolders now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, move the output files (the speaker file, the lexicon, the acoustic model, and the aligned textgrids) to the OSF folder to be ready to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a zip file of the aligned textgrids\n",
    "shutil.make_archive(txtgrds_path, 'zip', output_path)\n",
    "\n",
    "# Move the acoustic model\n",
    "shutil.copy(acs_mod_path, osf_path + path_sep + 'acoustic_models' + path_sep)\n",
    "\n",
    "# Move the lexicon\n",
    "shutil.copy(dict_file_path, osf_path + path_sep + 'lexicons' + path_sep)\n",
    "\n",
    "# Move the speaker file\n",
    "shutil.copy(spkr_file_path, osf_path + path_sep + 'spkr_files' + path_sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, upadate the tracking info in `VoxCommunis_Info.csv`. \n",
    "\n",
    "Make sure it is not in the lang_code_processing folder. Once updated, push the updated .csv to the GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have trained the model, set this to 1\n",
    "model_trained = 0\n",
    "aligned = 0\n",
    "\n",
    "# Paste the name of the outputs into the tracking file\n",
    "cv_track = pd.read_csv(cv_tracking_file)\n",
    "cv_track = cv_track.astype('string')\n",
    "if model_trained == 1:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'acoustic_model'] = acs_mod_name\n",
    "else:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'acoustic_model'] = ''\n",
    "if aligned == 1:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'textgrids'] = textgrid_folder_name\n",
    "else:\n",
    "    cv_track.loc[cv_track['code_cv'] == lang_code, 'textgrids'] = ''\n",
    "cv_track.loc[cv_track['code_cv'] == lang_code, 'spkr_file'] = spkr_file_name\n",
    "cv_track.loc[cv_track['code_cv'] == lang_code, 'lexicon'] = dict_file_name\n",
    "\n",
    "\n",
    "# Update the tracking file\n",
    "cv_track.to_csv(cv_tracking_file, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
